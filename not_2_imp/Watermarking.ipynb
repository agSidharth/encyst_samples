{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Watermarking.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "83306d7605474eb2a04986fd955900ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ef34e342f880477586802461c97ee47b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_2809a81556554f87a8df19dbcd505084",
              "IPY_MODEL_5ece87e9f4a24216a8f81d9c4e8258eb"
            ]
          }
        },
        "ef34e342f880477586802461c97ee47b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2809a81556554f87a8df19dbcd505084": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_923ee03f29144b5bb304f878e6d8dbba",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 9912422,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 9912422,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6e139f79ecad4c379683e91151c964ec"
          }
        },
        "5ece87e9f4a24216a8f81d9c4e8258eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_24628d8d17e44e10b753bd00f49f5e41",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 9913344/? [00:18&lt;00:00, 536970.68it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_88e8fa952b99486b8bf0d0b934d749b1"
          }
        },
        "923ee03f29144b5bb304f878e6d8dbba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6e139f79ecad4c379683e91151c964ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "24628d8d17e44e10b753bd00f49f5e41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "88e8fa952b99486b8bf0d0b934d749b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d070d523f9814d1d93f9d62eb5571e5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d8e8360d2c5e4763909c4adfa141915f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_5a8ce85690f6487db9e8c5f28ff382ce",
              "IPY_MODEL_f53a424055494ae58ec45302040fb214"
            ]
          }
        },
        "d8e8360d2c5e4763909c4adfa141915f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5a8ce85690f6487db9e8c5f28ff382ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_5313f9ec25434d8b9d3497cf0b49a708",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 28881,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 28881,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_adbe090575254a918ec5c39d8723b4a3"
          }
        },
        "f53a424055494ae58ec45302040fb214": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0e617e199a40408987aaa241bc973487",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 29696/? [00:00&lt;00:00, 191250.00it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7f2ddd4edfd44992b7ebee56d93335a3"
          }
        },
        "5313f9ec25434d8b9d3497cf0b49a708": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "adbe090575254a918ec5c39d8723b4a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0e617e199a40408987aaa241bc973487": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7f2ddd4edfd44992b7ebee56d93335a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fc93532b6c6b4754b60a71cf2ba35d69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_561daaa71d9943778ac6b77e22cd696d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c187c4a6993b4e608aec67cd0670c2e4",
              "IPY_MODEL_a00149bda2664540bdd476a4ffe630c4"
            ]
          }
        },
        "561daaa71d9943778ac6b77e22cd696d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c187c4a6993b4e608aec67cd0670c2e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e271cff0568d40cd9f1189f67075e8bd",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1648877,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1648877,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3aedd02c3fe44e739f61fb3dd68577ef"
          }
        },
        "a00149bda2664540bdd476a4ffe630c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_aa1fe845f97a419486ffa9585213a757",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1649664/? [00:17&lt;00:00, 93187.34it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_29f292cb61c34e47a6efce4479a8ad57"
          }
        },
        "e271cff0568d40cd9f1189f67075e8bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3aedd02c3fe44e739f61fb3dd68577ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "aa1fe845f97a419486ffa9585213a757": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "29f292cb61c34e47a6efce4479a8ad57": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bb0f2b9c0f7948dc8943fbf92ff55119": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_49fcc2b5b41345528caf08c8e33dcf2e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_6d8cc6d12ce8452aa18be64e70b27355",
              "IPY_MODEL_4ec07971a49240d89c5cfb85d3efa33b"
            ]
          }
        },
        "49fcc2b5b41345528caf08c8e33dcf2e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6d8cc6d12ce8452aa18be64e70b27355": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_05c3e60d2ae64109b640ada839120e6f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 4542,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 4542,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3da9f595f24b4bc8835a9803b4de02f4"
          }
        },
        "4ec07971a49240d89c5cfb85d3efa33b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c5e2fa5b2c3a45aba7ef040292d2ca26",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 5120/? [00:02&lt;00:00, 1752.24it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b742e14eb91e423cafa49de999163b2c"
          }
        },
        "05c3e60d2ae64109b640ada839120e6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3da9f595f24b4bc8835a9803b4de02f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c5e2fa5b2c3a45aba7ef040292d2ca26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b742e14eb91e423cafa49de999163b2c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGRiuu7kQadO"
      },
      "source": [
        "# **Black-box Watermarking**\n",
        "\n",
        "Most of the existing watermarking specially to protect the integrity involves manipulating the CNN model hidden layers to insert the signature. While they provide reasonable integrity protection, but nevertheless they may negatively impact the accuracy of the model. Therefore, our aim in this project is to develop a watermarking mechanism to ensure CNN integrity without manipulating the model layers. In addition, the aim is to produce a watermark that can work on CNN in a Blackbox manner during inference/query time to check the CNN models in their running time. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ip1gmhnX_e-6",
        "outputId": "2e5db3e8-0752-4c19-b6f1-a3e1d98e5b92"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IyhXiX6btCB0",
        "outputId": "669618b7-323c-402d-f022-1182775dd029"
      },
      "source": [
        "%cd gdrive/MyDrive/disentangling-vae/"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/disentangling-vae/classifers\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJl7eP-YQWeu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57b16313-dbfb-4df4-8de9-a1fbbf5c3d2d"
      },
      "source": [
        "import torch\n",
        "import time\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda:0\")\n",
        "  print(\"GPU\")\n",
        "else: \n",
        "  print(\"CPU\")\n",
        "\n",
        "#print(models.resnet18())"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvZeKffkdprk"
      },
      "source": [
        "**Load** the datasets here in the cell below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrvL_lIfV6AO"
      },
      "source": [
        "mnist_classes = 10\n",
        "Fmnist_classes = 10\n",
        "\n",
        "mnist_batch = 128\n",
        "Fmnist_batch = 128\n",
        "\n",
        "mnist_epoch = 6\n",
        "Fmnist_epoch = 8\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 842,
          "referenced_widgets": [
            "83306d7605474eb2a04986fd955900ab",
            "ef34e342f880477586802461c97ee47b",
            "2809a81556554f87a8df19dbcd505084",
            "5ece87e9f4a24216a8f81d9c4e8258eb",
            "923ee03f29144b5bb304f878e6d8dbba",
            "6e139f79ecad4c379683e91151c964ec",
            "24628d8d17e44e10b753bd00f49f5e41",
            "88e8fa952b99486b8bf0d0b934d749b1",
            "d070d523f9814d1d93f9d62eb5571e5a",
            "d8e8360d2c5e4763909c4adfa141915f",
            "5a8ce85690f6487db9e8c5f28ff382ce",
            "f53a424055494ae58ec45302040fb214",
            "5313f9ec25434d8b9d3497cf0b49a708",
            "adbe090575254a918ec5c39d8723b4a3",
            "0e617e199a40408987aaa241bc973487",
            "7f2ddd4edfd44992b7ebee56d93335a3",
            "fc93532b6c6b4754b60a71cf2ba35d69",
            "561daaa71d9943778ac6b77e22cd696d",
            "c187c4a6993b4e608aec67cd0670c2e4",
            "a00149bda2664540bdd476a4ffe630c4",
            "e271cff0568d40cd9f1189f67075e8bd",
            "3aedd02c3fe44e739f61fb3dd68577ef",
            "aa1fe845f97a419486ffa9585213a757",
            "29f292cb61c34e47a6efce4479a8ad57",
            "bb0f2b9c0f7948dc8943fbf92ff55119",
            "49fcc2b5b41345528caf08c8e33dcf2e",
            "6d8cc6d12ce8452aa18be64e70b27355",
            "4ec07971a49240d89c5cfb85d3efa33b",
            "05c3e60d2ae64109b640ada839120e6f",
            "3da9f595f24b4bc8835a9803b4de02f4",
            "c5e2fa5b2c3a45aba7ef040292d2ca26",
            "b742e14eb91e423cafa49de999163b2c"
          ]
        },
        "id": "7wNQtnbdXIpO",
        "outputId": "fb740ee8-058e-4d22-cfd7-88783c47ff93"
      },
      "source": [
        "#input_size = 224        #for  ResNet and VGG\n",
        "transforms_1 = transforms.Compose([transforms.Resize(32),transforms.ToTensor()])\n",
        "\n",
        "\n",
        "mnist_trainset_i = datasets.MNIST(root='./data', train=True, download=True, transform=transforms_1)\n",
        "mnist_trainset = DataLoader(mnist_trainset_i,batch_size = mnist_batch,shuffle=True)\n",
        "\n",
        "mnist_testset_i = datasets.MNIST(root='./data', train=False, download=True, transform=transforms_1)\n",
        "mnist_testset = DataLoader(mnist_testset_i,batch_size = mnist_batch,shuffle=True)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "Fmnist_trainset_i = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transforms_1)\n",
        "Fmnist_trainset = DataLoader(Fmnist_trainset_i,batch_size = Fmnist_batch,shuffle=True)\n",
        "\n",
        "Fmnist_testset_i = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transforms_1)\n",
        "Fmnist_testset = DataLoader(Fmnist_testset_i,batch_size = Fmnist_batch,shuffle=True)\n",
        "\"\"\"\n",
        "print(torch.cuda.memory_allocated(device))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 503: Service Unavailable\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "83306d7605474eb2a04986fd955900ab",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=9912422.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 503: Service Unavailable\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d070d523f9814d1d93f9d62eb5571e5a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=28881.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 503: Service Unavailable\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fc93532b6c6b4754b60a71cf2ba35d69",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1648877.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 503: Service Unavailable\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bb0f2b9c0f7948dc8943fbf92ff55119",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=4542.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
            "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FkWEkWK3aDwP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f7e0a59-0746-46de-bf68-cf5aee11ff6d"
      },
      "source": [
        "def train_model(model,train_set,test_set,optimizer,num_epochs):\n",
        "  since = time.time()\n",
        "\n",
        "  batch_train_loss = []\n",
        "  acc_epoch_train = []\n",
        "  acc_epoch_val = []\n",
        "\n",
        "  best_model_wts = copy.deepcopy(model.state_dict())            #copy the model here\n",
        "  best_acc = 0.0\n",
        "  \n",
        "  for epoch in tqdm(range(num_epochs)):\n",
        "    print('\\nEpoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "    print('-' * 10)\n",
        "    batch = 0\n",
        "    prev_loss = 0\n",
        "    #train step starts here.\n",
        "    model.train()                                               \n",
        "    train_loss = 0.0\n",
        "    train_corrects = 0\n",
        "\n",
        "    for inputs,labels in train_set:\n",
        "      \n",
        "      inputs = inputs.to(device)\n",
        "      labels = labels.to(device)\n",
        "\n",
        "      #print(torch.cuda.memory_allocated(device))\n",
        "      optimizer.zero_grad()\n",
        "      torch.set_grad_enabled(True)\n",
        "\n",
        "      outputs = model(inputs)\n",
        "      loss = criterion(outputs, labels)         #cross entropy loss\n",
        "      _, preds = torch.max(outputs, 1) \n",
        "\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      \n",
        "      train_loss += loss.item() * inputs.size(0)\n",
        "      train_corrects += torch.sum(preds == labels.data)\n",
        "      batch_train_loss.append(train_loss - prev_loss)\n",
        "      print('Training batch number: {} : loss --> {}'.format(batch,train_loss-prev_loss))\n",
        "      prev_loss = train_loss\n",
        "      batch = batch+1\n",
        "\n",
        "    epoch_train_loss = train_loss / len(train_set.dataset)\n",
        "    epoch_train_acc = train_corrects.double() / len(train_set.dataset)\n",
        "    acc_epoch_train.append(epoch_train_acc)\n",
        "\n",
        "    print('\\n\\n{} Loss: {:.4f} Acc: {:.4f}'.format(\"Training..\", epoch_train_loss, epoch_train_acc))\n",
        "\n",
        "    #evaluation starts here...\n",
        "    batch = 0\n",
        "    prev_loss = 0\n",
        "    model.eval()                \n",
        "    testing_loss = 0.0\n",
        "    testing_corrects = 0\n",
        "\n",
        "    for inputs,labels in test_set:\n",
        "      inputs = inputs.to(device)\n",
        "      labels = labels.to(device)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      torch.set_grad_enabled(False)\n",
        "      \n",
        "      outputs = model(inputs)\n",
        "      loss = criterion(outputs, labels)\n",
        "      _, preds = torch.max(outputs, 1)\n",
        "\n",
        "      testing_loss += loss.item() * inputs.size(0)\n",
        "      testing_corrects += torch.sum(preds == labels.data)\n",
        "      print('Validating batch number: {} : loss --> {}'.format(batch,testing_loss - prev_loss))\n",
        "      prev_loss = testing_loss\n",
        "      batch = batch+1\n",
        "    #evaluation is over here...\n",
        "\n",
        "    epoch_testing_loss = testing_loss / len(test_set.dataset)\n",
        "    epoch_testing_acc = testing_corrects.double() / len(test_set.dataset)\n",
        "    acc_epoch_val.append(epoch_testing_acc)\n",
        "    print('{} Loss: {:.4f} Acc: {:.4f}'.format(\"Testing..\", epoch_testing_loss, epoch_testing_acc))\n",
        "\n",
        "    if epoch_testing_acc > best_acc:\n",
        "      best_acc = epoch_testing_acc\n",
        "      best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "  print('\\n\\n Training complete in {:.4f}seconds',time.time() - since)\n",
        "\n",
        "  model.load_state_dict(best_model_wts)\n",
        "  return model,batch_train_loss,acc_epoch_train,acc_epoch_val\n",
        "\n",
        "print(torch.cuda.memory_allocated(device))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDq0jHkcd0Jb"
      },
      "source": [
        "**Load** the pretrained models here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "id": "xT-LrUqld0aB",
        "outputId": "506775f8-bd33-432b-972a-742913794119"
      },
      "source": [
        "PATH = 'classifers/net_architecture.pth'\n",
        "clean_model = torch.load(PATH,map_location=torch.device('cpu'))\n",
        "attacked_model = torch.load(PATH,map_location=torch.device('cpu'))\n",
        "\n",
        "PATH = 'classifers/net.pth'\n",
        "clean_model.load_state_dict(torch.load(PATH,map_location=torch.device('cpu')))\n",
        "\n",
        "PATH = \"classifers/square_white_tar0_alpha0.00_mark(3,3).pth\"\n",
        "attacked_model.load_state_dict(torch.load(PATH,map_location=torch.device('cpu')))\n",
        "\n",
        "Mresnet18 = clean_model\n",
        "\n",
        "\"\"\"\n",
        "Fresnet18 = models.resnet18(pretrained=True)          #for FashionMNIST\n",
        "Fresnet18.fc = nn.Linear(512,Fmnist_classes)\n",
        "Fresnet18.conv1 = nn.Conv2d(1, 64,kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
        "\n",
        "\n",
        "Mvgg13 = models.vgg13(pretrained=True)\n",
        "Mvgg13.features[0] = nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "Mvgg13.classifier[6] = nn.Linear(4096,mnist_classes)\n",
        "\n",
        "\n",
        "Fvgg13 = models.vgg13(pretrained = True)\n",
        "Fvgg13.features[0] = nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "Fvgg13.classifier[6] = nn.Linear(4096,Fmnist_classes)\n",
        "\n",
        "\"\"\"\n",
        "print(torch.cuda.memory_allocated(device))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-0506cb14f304>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mPATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'classifers/net_architecture.pth'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mclean_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mattacked_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mPATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'classifers/net.pth'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    605\u001b[0m                     \u001b[0mopened_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_position\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 607\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    608\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m    880\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnpicklerWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 882\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_loaded_sparse_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mfind_class\u001b[0;34m(self, mod_name, name)\u001b[0m\n\u001b[1;32m    873\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfind_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmod_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m             \u001b[0mmod_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_module_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmod_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 875\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    876\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m     \u001b[0;31m# Load the data (which may in turn use `persistent_load` to load tensors)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'trojanvision'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "9SsuSfhgVQRn",
        "outputId": "f1fbb9bf-abae-4de7-b6c7-b1cd6ba16f17"
      },
      "source": [
        "img_xy = np.random.randint(len(mnist_testset_i));\n",
        "img = mnist_testset_i[img_xy][0][0,:,:]\n",
        "plt.imshow(img, cmap='gray')\n",
        "plt.show()\n",
        "\n",
        "\"\"\"\n",
        "img_xy = np.random.randint(len(Fmnist_testset_i));\n",
        "img = Fmnist_testset_i[img_xy][0][0,:,:]\n",
        "plt.imshow(img, cmap='gray')\n",
        "plt.show()\n",
        "\"\"\"\n",
        "print(torch.cuda.memory_allocated(device))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "318335488\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4Zsz4Ay0AJV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "40a23ca0-0364-456c-c7c4-604b61ad3c02"
      },
      "source": [
        "#title\n",
        "Mresnet18.to(device)\n",
        "optimizer_ft = optim.Adadelta(Mresnet18.parameters())\n",
        "trained_MResnet,MRbatch_train_loss,MRacc_epoch_train,MRacc_epoch_val = train_model(Mresnet18,mnist_trainset,mnist_testset,optimizer_ft,mnist_epoch)\n",
        "\n",
        "plt.title(\"Loss per batch vs. Number of Batches\")\n",
        "plt.xlabel(\"Loss in Training batches\")\n",
        "plt.ylabel(\"Loss in Validation batches\")\n",
        "plt.plot(MRbatch_train_loss,label = \"Training\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.title(\"Accuracy per epoch vs Number of epochs\")\n",
        "plt.xlabel(\"Training Accuracy per epoch\")\n",
        "plt.ylabel(\"Testing Accuracy per epoch\")\n",
        "plt.plot(MRacc_epoch_train,label = \"Training\")\n",
        "plt.plot(MRacc_epoch_val,label = \"Testing\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/7 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 0/6\n",
            "----------\n",
            "Training batch number: 0 : loss --> 322.0579833984375\n",
            "Training batch number: 1 : loss --> 368.2408447265625\n",
            "Training batch number: 2 : loss --> 260.0546875\n",
            "Training batch number: 3 : loss --> 222.90234375\n",
            "Training batch number: 4 : loss --> 190.40892028808594\n",
            "Training batch number: 5 : loss --> 121.91690063476562\n",
            "Training batch number: 6 : loss --> 180.73611450195312\n",
            "Training batch number: 7 : loss --> 464.77801513671875\n",
            "Training batch number: 8 : loss --> 249.01324462890625\n",
            "Training batch number: 9 : loss --> 193.13531494140625\n",
            "Training batch number: 10 : loss --> 160.20748901367188\n",
            "Training batch number: 11 : loss --> 123.98138427734375\n",
            "Training batch number: 12 : loss --> 117.88411712646484\n",
            "Training batch number: 13 : loss --> 114.49777221679688\n",
            "Training batch number: 14 : loss --> 98.19363403320312\n",
            "Training batch number: 15 : loss --> 55.56158447265625\n",
            "Training batch number: 16 : loss --> 46.55664825439453\n",
            "Training batch number: 17 : loss --> 53.53532409667969\n",
            "Training batch number: 18 : loss --> 39.7332649230957\n",
            "Training batch number: 19 : loss --> 50.088531494140625\n",
            "Training batch number: 20 : loss --> 36.24636459350586\n",
            "Training batch number: 21 : loss --> 85.20374298095703\n",
            "Training batch number: 22 : loss --> 88.08495330810547\n",
            "Training batch number: 23 : loss --> 66.59660339355469\n",
            "Training batch number: 24 : loss --> 63.40337371826172\n",
            "Training batch number: 25 : loss --> 106.68827056884766\n",
            "Training batch number: 26 : loss --> 55.841243743896484\n",
            "Training batch number: 27 : loss --> 53.564430236816406\n",
            "Training batch number: 28 : loss --> 89.03050994873047\n",
            "Training batch number: 29 : loss --> 38.32366943359375\n",
            "Training batch number: 30 : loss --> 37.81594467163086\n",
            "Training batch number: 31 : loss --> 32.416343688964844\n",
            "Training batch number: 32 : loss --> 57.74304962158203\n",
            "Training batch number: 33 : loss --> 40.65660858154297\n",
            "Training batch number: 34 : loss --> 40.281097412109375\n",
            "Training batch number: 35 : loss --> 52.1663703918457\n",
            "Training batch number: 36 : loss --> 19.006013870239258\n",
            "Training batch number: 37 : loss --> 22.32878303527832\n",
            "Training batch number: 38 : loss --> 71.04917907714844\n",
            "Training batch number: 39 : loss --> 68.00714874267578\n",
            "Training batch number: 40 : loss --> 64.89835357666016\n",
            "Training batch number: 41 : loss --> 30.382736206054688\n",
            "Training batch number: 42 : loss --> 24.057886123657227\n",
            "Training batch number: 43 : loss --> 43.12529373168945\n",
            "Training batch number: 44 : loss --> 64.2752685546875\n",
            "Training batch number: 45 : loss --> 52.212196350097656\n",
            "Training batch number: 46 : loss --> 60.441383361816406\n",
            "Training batch number: 47 : loss --> 22.607336044311523\n",
            "Training batch number: 48 : loss --> 34.956966400146484\n",
            "Training batch number: 49 : loss --> 32.76325607299805\n",
            "Training batch number: 50 : loss --> 28.633760452270508\n",
            "Training batch number: 51 : loss --> 79.66707611083984\n",
            "Training batch number: 52 : loss --> 50.8426628112793\n",
            "Training batch number: 53 : loss --> 23.543012619018555\n",
            "Training batch number: 54 : loss --> 28.189695358276367\n",
            "Training batch number: 55 : loss --> 29.472387313842773\n",
            "Training batch number: 56 : loss --> 23.834667205810547\n",
            "Training batch number: 57 : loss --> 26.344383239746094\n",
            "Training batch number: 58 : loss --> 25.069889068603516\n",
            "Training batch number: 59 : loss --> 24.19974708557129\n",
            "Training batch number: 60 : loss --> 29.13905906677246\n",
            "Training batch number: 61 : loss --> 16.059890747070312\n",
            "Training batch number: 62 : loss --> 32.72916030883789\n",
            "Training batch number: 63 : loss --> 15.523343086242676\n",
            "Training batch number: 64 : loss --> 35.61079025268555\n",
            "Training batch number: 65 : loss --> 24.46024513244629\n",
            "Training batch number: 66 : loss --> 22.800003051757812\n",
            "Training batch number: 67 : loss --> 21.67985725402832\n",
            "Training batch number: 68 : loss --> 15.982014656066895\n",
            "Training batch number: 69 : loss --> 41.63365173339844\n",
            "Training batch number: 70 : loss --> 36.71592330932617\n",
            "Training batch number: 71 : loss --> 28.559738159179688\n",
            "Training batch number: 72 : loss --> 21.788358688354492\n",
            "Training batch number: 73 : loss --> 21.004398345947266\n",
            "Training batch number: 74 : loss --> 60.73287582397461\n",
            "Training batch number: 75 : loss --> 49.368385314941406\n",
            "Training batch number: 76 : loss --> 48.94034957885742\n",
            "Training batch number: 77 : loss --> 34.795005798339844\n",
            "Training batch number: 78 : loss --> 27.850339889526367\n",
            "Training batch number: 79 : loss --> 12.309496879577637\n",
            "Training batch number: 80 : loss --> 17.817541122436523\n",
            "Training batch number: 81 : loss --> 22.902454376220703\n",
            "Training batch number: 82 : loss --> 28.57405662536621\n",
            "Training batch number: 83 : loss --> 43.16419219970703\n",
            "Training batch number: 84 : loss --> 44.971004486083984\n",
            "Training batch number: 85 : loss --> 81.0291519165039\n",
            "Training batch number: 86 : loss --> 49.60341262817383\n",
            "Training batch number: 87 : loss --> 30.37057876586914\n",
            "Training batch number: 88 : loss --> 30.09699821472168\n",
            "Training batch number: 89 : loss --> 38.882286071777344\n",
            "Training batch number: 90 : loss --> 10.997293472290039\n",
            "Training batch number: 91 : loss --> 22.90576934814453\n",
            "Training batch number: 92 : loss --> 53.840145111083984\n",
            "Training batch number: 93 : loss --> 24.120912551879883\n",
            "Training batch number: 94 : loss --> 13.14969253540039\n",
            "Training batch number: 95 : loss --> 32.298805236816406\n",
            "Training batch number: 96 : loss --> 35.25942611694336\n",
            "Training batch number: 97 : loss --> 17.336116790771484\n",
            "Training batch number: 98 : loss --> 35.37569046020508\n",
            "Training batch number: 99 : loss --> 25.262075424194336\n",
            "Training batch number: 100 : loss --> 18.867267608642578\n",
            "Training batch number: 101 : loss --> 29.061826705932617\n",
            "Training batch number: 102 : loss --> 17.539562225341797\n",
            "Training batch number: 103 : loss --> 7.129044055938721\n",
            "Training batch number: 104 : loss --> 17.320213317871094\n",
            "Training batch number: 105 : loss --> 5.535630702972412\n",
            "Training batch number: 106 : loss --> 17.181657791137695\n",
            "Training batch number: 107 : loss --> 18.916933059692383\n",
            "Training batch number: 108 : loss --> 14.126077651977539\n",
            "Training batch number: 109 : loss --> 20.990217208862305\n",
            "Training batch number: 110 : loss --> 12.810090065002441\n",
            "Training batch number: 111 : loss --> 30.259483337402344\n",
            "Training batch number: 112 : loss --> 26.560876846313477\n",
            "Training batch number: 113 : loss --> 29.165508270263672\n",
            "Training batch number: 114 : loss --> 6.62001371383667\n",
            "Training batch number: 115 : loss --> 20.522737503051758\n",
            "Training batch number: 116 : loss --> 17.990554809570312\n",
            "Training batch number: 117 : loss --> 9.718716621398926\n",
            "Training batch number: 118 : loss --> 8.895756721496582\n",
            "Training batch number: 119 : loss --> 34.874114990234375\n",
            "Training batch number: 120 : loss --> 45.8166618347168\n",
            "Training batch number: 121 : loss --> 28.817441940307617\n",
            "Training batch number: 122 : loss --> 10.18546199798584\n",
            "Training batch number: 123 : loss --> 46.73980712890625\n",
            "Training batch number: 124 : loss --> 25.4251766204834\n",
            "Training batch number: 125 : loss --> 14.409321784973145\n",
            "Training batch number: 126 : loss --> 12.199661254882812\n",
            "Training batch number: 127 : loss --> 9.825063705444336\n",
            "Training batch number: 128 : loss --> 13.245113372802734\n",
            "Training batch number: 129 : loss --> 17.995098114013672\n",
            "Training batch number: 130 : loss --> 16.56134605407715\n",
            "Training batch number: 131 : loss --> 8.178255081176758\n",
            "Training batch number: 132 : loss --> 17.048688888549805\n",
            "Training batch number: 133 : loss --> 15.408734321594238\n",
            "Training batch number: 134 : loss --> 18.85227394104004\n",
            "Training batch number: 135 : loss --> 13.47523021697998\n",
            "Training batch number: 136 : loss --> 22.587923049926758\n",
            "Training batch number: 137 : loss --> 14.983502388000488\n",
            "Training batch number: 138 : loss --> 19.242380142211914\n",
            "Training batch number: 139 : loss --> 52.23841094970703\n",
            "Training batch number: 140 : loss --> 34.93309020996094\n",
            "Training batch number: 141 : loss --> 45.158905029296875\n",
            "Training batch number: 142 : loss --> 19.38763999938965\n",
            "Training batch number: 143 : loss --> 26.679718017578125\n",
            "Training batch number: 144 : loss --> 24.41162109375\n",
            "Training batch number: 145 : loss --> 18.783851623535156\n",
            "Training batch number: 146 : loss --> 20.137739181518555\n",
            "Training batch number: 147 : loss --> 22.81098747253418\n",
            "Training batch number: 148 : loss --> 14.444479942321777\n",
            "Training batch number: 149 : loss --> 9.506706237792969\n",
            "Training batch number: 150 : loss --> 13.1741304397583\n",
            "Training batch number: 151 : loss --> 19.47026824951172\n",
            "Training batch number: 152 : loss --> 22.228363037109375\n",
            "Training batch number: 153 : loss --> 25.29273796081543\n",
            "Training batch number: 154 : loss --> 21.361303329467773\n",
            "Training batch number: 155 : loss --> 12.511316299438477\n",
            "Training batch number: 156 : loss --> 8.847702026367188\n",
            "Training batch number: 157 : loss --> 29.0631046295166\n",
            "Training batch number: 158 : loss --> 15.364316940307617\n",
            "Training batch number: 159 : loss --> 4.609744548797607\n",
            "Training batch number: 160 : loss --> 20.475542068481445\n",
            "Training batch number: 161 : loss --> 30.342357635498047\n",
            "Training batch number: 162 : loss --> 23.168439865112305\n",
            "Training batch number: 163 : loss --> 14.648113250732422\n",
            "Training batch number: 164 : loss --> 20.460241317749023\n",
            "Training batch number: 165 : loss --> 6.207814693450928\n",
            "Training batch number: 166 : loss --> 20.875499725341797\n",
            "Training batch number: 167 : loss --> 9.35638427734375\n",
            "Training batch number: 168 : loss --> 29.899991989135742\n",
            "Training batch number: 169 : loss --> 23.6730899810791\n",
            "Training batch number: 170 : loss --> 2.4345438480377197\n",
            "Training batch number: 171 : loss --> 11.668216705322266\n",
            "Training batch number: 172 : loss --> 4.515815258026123\n",
            "Training batch number: 173 : loss --> 7.213770866394043\n",
            "Training batch number: 174 : loss --> 26.76604461669922\n",
            "Training batch number: 175 : loss --> 20.618793487548828\n",
            "Training batch number: 176 : loss --> 19.542295455932617\n",
            "Training batch number: 177 : loss --> 24.897998809814453\n",
            "Training batch number: 178 : loss --> 24.77869987487793\n",
            "Training batch number: 179 : loss --> 31.469642639160156\n",
            "Training batch number: 180 : loss --> 17.399648666381836\n",
            "Training batch number: 181 : loss --> 9.362082481384277\n",
            "Training batch number: 182 : loss --> 20.987762451171875\n",
            "Training batch number: 183 : loss --> 9.30521297454834\n",
            "Training batch number: 184 : loss --> 13.613727569580078\n",
            "Training batch number: 185 : loss --> 13.422174453735352\n",
            "Training batch number: 186 : loss --> 29.100143432617188\n",
            "Training batch number: 187 : loss --> 12.555633544921875\n",
            "Training batch number: 188 : loss --> 20.218549728393555\n",
            "Training batch number: 189 : loss --> 18.987592697143555\n",
            "Training batch number: 190 : loss --> 8.817916870117188\n",
            "Training batch number: 191 : loss --> 18.711143493652344\n",
            "Training batch number: 192 : loss --> 6.616167068481445\n",
            "Training batch number: 193 : loss --> 16.30915641784668\n",
            "Training batch number: 194 : loss --> 16.431682586669922\n",
            "Training batch number: 195 : loss --> 16.14212417602539\n",
            "Training batch number: 196 : loss --> 12.867966651916504\n",
            "Training batch number: 197 : loss --> 5.41957426071167\n",
            "Training batch number: 198 : loss --> 34.061737060546875\n",
            "Training batch number: 199 : loss --> 19.610477447509766\n",
            "Training batch number: 200 : loss --> 23.022499084472656\n",
            "Training batch number: 201 : loss --> 17.543519973754883\n",
            "Training batch number: 202 : loss --> 7.757538795471191\n",
            "Training batch number: 203 : loss --> 21.035858154296875\n",
            "Training batch number: 204 : loss --> 10.2453031539917\n",
            "Training batch number: 205 : loss --> 8.929468154907227\n",
            "Training batch number: 206 : loss --> 14.502689361572266\n",
            "Training batch number: 207 : loss --> 9.368815422058105\n",
            "Training batch number: 208 : loss --> 29.70541000366211\n",
            "Training batch number: 209 : loss --> 31.526498794555664\n",
            "Training batch number: 210 : loss --> 25.613407135009766\n",
            "Training batch number: 211 : loss --> 7.90399169921875\n",
            "Training batch number: 212 : loss --> 16.12824821472168\n",
            "Training batch number: 213 : loss --> 6.416168212890625\n",
            "Training batch number: 214 : loss --> 15.26924991607666\n",
            "Training batch number: 215 : loss --> 26.600196838378906\n",
            "Training batch number: 216 : loss --> 17.379024505615234\n",
            "Training batch number: 217 : loss --> 19.145610809326172\n",
            "Training batch number: 218 : loss --> 6.154025554656982\n",
            "Training batch number: 219 : loss --> 4.126321315765381\n",
            "Training batch number: 220 : loss --> 8.408722877502441\n",
            "Training batch number: 221 : loss --> 21.47471046447754\n",
            "Training batch number: 222 : loss --> 28.139522552490234\n",
            "Training batch number: 223 : loss --> 21.790069580078125\n",
            "Training batch number: 224 : loss --> 19.26772689819336\n",
            "Training batch number: 225 : loss --> 14.532438278198242\n",
            "Training batch number: 226 : loss --> 4.351483345031738\n",
            "Training batch number: 227 : loss --> 26.57486343383789\n",
            "Training batch number: 228 : loss --> 6.725877285003662\n",
            "Training batch number: 229 : loss --> 7.601629257202148\n",
            "Training batch number: 230 : loss --> 18.712039947509766\n",
            "Training batch number: 231 : loss --> 22.917800903320312\n",
            "Training batch number: 232 : loss --> 24.23128318786621\n",
            "Training batch number: 233 : loss --> 8.159560203552246\n",
            "Training batch number: 234 : loss --> 8.325718879699707\n",
            "Training batch number: 235 : loss --> 13.377618789672852\n",
            "Training batch number: 236 : loss --> 10.171253204345703\n",
            "Training batch number: 237 : loss --> 14.583168983459473\n",
            "Training batch number: 238 : loss --> 13.491666793823242\n",
            "Training batch number: 239 : loss --> 9.763650894165039\n",
            "Training batch number: 240 : loss --> 7.3306145668029785\n",
            "Training batch number: 241 : loss --> 7.7163238525390625\n",
            "Training batch number: 242 : loss --> 19.066139221191406\n",
            "Training batch number: 243 : loss --> 6.245723247528076\n",
            "Training batch number: 244 : loss --> 13.707839965820312\n",
            "Training batch number: 245 : loss --> 22.61726188659668\n",
            "Training batch number: 246 : loss --> 21.196704864501953\n",
            "Training batch number: 247 : loss --> 16.942729949951172\n",
            "Training batch number: 248 : loss --> 10.766446113586426\n",
            "Training batch number: 249 : loss --> 8.77752685546875\n",
            "Training batch number: 250 : loss --> 7.204287052154541\n",
            "Training batch number: 251 : loss --> 6.324345111846924\n",
            "Training batch number: 252 : loss --> 9.974425315856934\n",
            "Training batch number: 253 : loss --> 24.56316375732422\n",
            "Training batch number: 254 : loss --> 19.006254196166992\n",
            "Training batch number: 255 : loss --> 5.775179862976074\n",
            "Training batch number: 256 : loss --> 14.036870002746582\n",
            "Training batch number: 257 : loss --> 5.149148941040039\n",
            "Training batch number: 258 : loss --> 14.575682640075684\n",
            "Training batch number: 259 : loss --> 13.188688278198242\n",
            "Training batch number: 260 : loss --> 6.689306735992432\n",
            "Training batch number: 261 : loss --> 10.982128143310547\n",
            "Training batch number: 262 : loss --> 12.355110168457031\n",
            "Training batch number: 263 : loss --> 30.8811092376709\n",
            "Training batch number: 264 : loss --> 23.2275390625\n",
            "Training batch number: 265 : loss --> 9.82550048828125\n",
            "Training batch number: 266 : loss --> 8.15091609954834\n",
            "Training batch number: 267 : loss --> 5.03209114074707\n",
            "Training batch number: 268 : loss --> 7.569960594177246\n",
            "Training batch number: 269 : loss --> 16.882404327392578\n",
            "Training batch number: 270 : loss --> 17.543195724487305\n",
            "Training batch number: 271 : loss --> 15.068634986877441\n",
            "Training batch number: 272 : loss --> 13.7603759765625\n",
            "Training batch number: 273 : loss --> 22.286893844604492\n",
            "Training batch number: 274 : loss --> 14.85008430480957\n",
            "Training batch number: 275 : loss --> 3.811469316482544\n",
            "Training batch number: 276 : loss --> 8.243583679199219\n",
            "Training batch number: 277 : loss --> 0.7579331994056702\n",
            "Training batch number: 278 : loss --> 7.45618200302124\n",
            "Training batch number: 279 : loss --> 10.862037658691406\n",
            "Training batch number: 280 : loss --> 13.864542007446289\n",
            "Training batch number: 281 : loss --> 1.7065534591674805\n",
            "Training batch number: 282 : loss --> 7.444058895111084\n",
            "Training batch number: 283 : loss --> 4.974852085113525\n",
            "Training batch number: 284 : loss --> 15.790575981140137\n",
            "Training batch number: 285 : loss --> 13.490524291992188\n",
            "Training batch number: 286 : loss --> 2.5246639251708984\n",
            "Training batch number: 287 : loss --> 5.051144123077393\n",
            "Training batch number: 288 : loss --> 2.8598825931549072\n",
            "Training batch number: 289 : loss --> 10.442042350769043\n",
            "Training batch number: 290 : loss --> 3.2882845401763916\n",
            "Training batch number: 291 : loss --> 5.527141094207764\n",
            "Training batch number: 292 : loss --> 78.71031188964844\n",
            "Training batch number: 293 : loss --> 29.019134521484375\n",
            "Training batch number: 294 : loss --> 17.21004295349121\n",
            "Training batch number: 295 : loss --> 23.267715454101562\n",
            "Training batch number: 296 : loss --> 19.132551193237305\n",
            "Training batch number: 297 : loss --> 22.635679244995117\n",
            "Training batch number: 298 : loss --> 24.989665985107422\n",
            "Training batch number: 299 : loss --> 11.41244888305664\n",
            "Training batch number: 300 : loss --> 20.636005401611328\n",
            "Training batch number: 301 : loss --> 17.901966094970703\n",
            "Training batch number: 302 : loss --> 36.488189697265625\n",
            "Training batch number: 303 : loss --> 14.589313507080078\n",
            "Training batch number: 304 : loss --> 14.67726993560791\n",
            "Training batch number: 305 : loss --> 11.988481521606445\n",
            "Training batch number: 306 : loss --> 18.15113067626953\n",
            "Training batch number: 307 : loss --> 12.75998306274414\n",
            "Training batch number: 308 : loss --> 17.275665283203125\n",
            "Training batch number: 309 : loss --> 7.1703948974609375\n",
            "Training batch number: 310 : loss --> 7.1465535163879395\n",
            "Training batch number: 311 : loss --> 2.8879823684692383\n",
            "Training batch number: 312 : loss --> 11.536595344543457\n",
            "Training batch number: 313 : loss --> 19.619844436645508\n",
            "Training batch number: 314 : loss --> 5.488584518432617\n",
            "Training batch number: 315 : loss --> 10.277997016906738\n",
            "Training batch number: 316 : loss --> 6.840839862823486\n",
            "Training batch number: 317 : loss --> 19.107717514038086\n",
            "Training batch number: 318 : loss --> 31.88673973083496\n",
            "Training batch number: 319 : loss --> 22.585561752319336\n",
            "Training batch number: 320 : loss --> 7.410144329071045\n",
            "Training batch number: 321 : loss --> 5.475937843322754\n",
            "Training batch number: 322 : loss --> 19.604724884033203\n",
            "Training batch number: 323 : loss --> 4.695960998535156\n",
            "Training batch number: 324 : loss --> 8.571197509765625\n",
            "Training batch number: 325 : loss --> 12.398588180541992\n",
            "Training batch number: 326 : loss --> 8.194883346557617\n",
            "Training batch number: 327 : loss --> 7.218603134155273\n",
            "Training batch number: 328 : loss --> 8.737339973449707\n",
            "Training batch number: 329 : loss --> 17.360857009887695\n",
            "Training batch number: 330 : loss --> 0.47897064685821533\n",
            "Training batch number: 331 : loss --> 4.575822830200195\n",
            "Training batch number: 332 : loss --> 14.648846626281738\n",
            "Training batch number: 333 : loss --> 2.2045111656188965\n",
            "Training batch number: 334 : loss --> 9.231663703918457\n",
            "Training batch number: 335 : loss --> 15.499137878417969\n",
            "Training batch number: 336 : loss --> 9.953728675842285\n",
            "Training batch number: 337 : loss --> 12.242096900939941\n",
            "Training batch number: 338 : loss --> 3.9029059410095215\n",
            "Training batch number: 339 : loss --> 9.598000526428223\n",
            "Training batch number: 340 : loss --> 3.3215901851654053\n",
            "Training batch number: 341 : loss --> 7.129760265350342\n",
            "Training batch number: 342 : loss --> 9.269773483276367\n",
            "Training batch number: 343 : loss --> 3.8755979537963867\n",
            "Training batch number: 344 : loss --> 2.9179625511169434\n",
            "Training batch number: 345 : loss --> 2.863518238067627\n",
            "Training batch number: 346 : loss --> 6.946514129638672\n",
            "Training batch number: 347 : loss --> 11.731098175048828\n",
            "Training batch number: 348 : loss --> 3.9537672996520996\n",
            "Training batch number: 349 : loss --> 6.986025810241699\n",
            "Training batch number: 350 : loss --> 3.4515862464904785\n",
            "Training batch number: 351 : loss --> 13.682807922363281\n",
            "Training batch number: 352 : loss --> 1.6129567623138428\n",
            "Training batch number: 353 : loss --> 8.406336784362793\n",
            "Training batch number: 354 : loss --> 19.329423904418945\n",
            "Training batch number: 355 : loss --> 15.684248924255371\n",
            "Training batch number: 356 : loss --> 10.242091178894043\n",
            "Training batch number: 357 : loss --> 33.00364303588867\n",
            "Training batch number: 358 : loss --> 25.799213409423828\n",
            "Training batch number: 359 : loss --> 9.4468355178833\n",
            "Training batch number: 360 : loss --> 12.840185165405273\n",
            "Training batch number: 361 : loss --> 13.992292404174805\n",
            "Training batch number: 362 : loss --> 15.045714378356934\n",
            "Training batch number: 363 : loss --> 9.819282531738281\n",
            "Training batch number: 364 : loss --> 4.240375995635986\n",
            "Training batch number: 365 : loss --> 5.543071269989014\n",
            "Training batch number: 366 : loss --> 7.72084903717041\n",
            "Training batch number: 367 : loss --> 6.543847560882568\n",
            "Training batch number: 368 : loss --> 16.40507698059082\n",
            "Training batch number: 369 : loss --> 4.488524436950684\n",
            "Training batch number: 370 : loss --> 7.931197643280029\n",
            "Training batch number: 371 : loss --> 6.009156703948975\n",
            "Training batch number: 372 : loss --> 6.5002264976501465\n",
            "Training batch number: 373 : loss --> 14.936369895935059\n",
            "Training batch number: 374 : loss --> 15.110526084899902\n",
            "Training batch number: 375 : loss --> 1.3372405767440796\n",
            "Training batch number: 376 : loss --> 11.372456550598145\n",
            "Training batch number: 377 : loss --> 12.935212135314941\n",
            "Training batch number: 378 : loss --> 19.374191284179688\n",
            "Training batch number: 379 : loss --> 6.3058085441589355\n",
            "Training batch number: 380 : loss --> 11.77721881866455\n",
            "Training batch number: 381 : loss --> 16.385238647460938\n",
            "Training batch number: 382 : loss --> 9.201916694641113\n",
            "Training batch number: 383 : loss --> 6.220702648162842\n",
            "Training batch number: 384 : loss --> 8.967146873474121\n",
            "Training batch number: 385 : loss --> 14.748892784118652\n",
            "Training batch number: 386 : loss --> 9.298640251159668\n",
            "Training batch number: 387 : loss --> 3.475008964538574\n",
            "Training batch number: 388 : loss --> 3.3616738319396973\n",
            "Training batch number: 389 : loss --> 3.048558473587036\n",
            "Training batch number: 390 : loss --> 26.79662322998047\n",
            "Training batch number: 391 : loss --> 16.67061424255371\n",
            "Training batch number: 392 : loss --> 15.190651893615723\n",
            "Training batch number: 393 : loss --> 8.773504257202148\n",
            "Training batch number: 394 : loss --> 7.636768341064453\n",
            "Training batch number: 395 : loss --> 29.022117614746094\n",
            "Training batch number: 396 : loss --> 6.714467525482178\n",
            "Training batch number: 397 : loss --> 8.455649375915527\n",
            "Training batch number: 398 : loss --> 11.149890899658203\n",
            "Training batch number: 399 : loss --> 1.8658076524734497\n",
            "Training batch number: 400 : loss --> 9.169970512390137\n",
            "Training batch number: 401 : loss --> 2.5303242206573486\n",
            "Training batch number: 402 : loss --> 20.258739471435547\n",
            "Training batch number: 403 : loss --> 5.1202473640441895\n",
            "Training batch number: 404 : loss --> 2.291581153869629\n",
            "Training batch number: 405 : loss --> 3.573643922805786\n",
            "Training batch number: 406 : loss --> 9.165128707885742\n",
            "Training batch number: 407 : loss --> 6.660056114196777\n",
            "Training batch number: 408 : loss --> 13.807180404663086\n",
            "Training batch number: 409 : loss --> 9.047258377075195\n",
            "Training batch number: 410 : loss --> 8.776994705200195\n",
            "Training batch number: 411 : loss --> 10.517839431762695\n",
            "Training batch number: 412 : loss --> 17.559070587158203\n",
            "Training batch number: 413 : loss --> 6.158918857574463\n",
            "Training batch number: 414 : loss --> 12.618964195251465\n",
            "Training batch number: 415 : loss --> 9.079198837280273\n",
            "Training batch number: 416 : loss --> 14.451311111450195\n",
            "Training batch number: 417 : loss --> 0.9976664781570435\n",
            "Training batch number: 418 : loss --> 5.552499771118164\n",
            "Training batch number: 419 : loss --> 24.204683303833008\n",
            "Training batch number: 420 : loss --> 18.024879455566406\n",
            "Training batch number: 421 : loss --> 12.588622093200684\n",
            "Training batch number: 422 : loss --> 3.3164312839508057\n",
            "Training batch number: 423 : loss --> 9.289929389953613\n",
            "Training batch number: 424 : loss --> 5.666814804077148\n",
            "Training batch number: 425 : loss --> 12.110875129699707\n",
            "Training batch number: 426 : loss --> 16.422452926635742\n",
            "Training batch number: 427 : loss --> 6.561726093292236\n",
            "Training batch number: 428 : loss --> 1.513650894165039\n",
            "Training batch number: 429 : loss --> 8.241776466369629\n",
            "Training batch number: 430 : loss --> 16.39835548400879\n",
            "Training batch number: 431 : loss --> 6.580048084259033\n",
            "Training batch number: 432 : loss --> 1.2428265810012817\n",
            "Training batch number: 433 : loss --> 15.072172164916992\n",
            "Training batch number: 434 : loss --> 7.505392074584961\n",
            "Training batch number: 435 : loss --> 7.074804306030273\n",
            "Training batch number: 436 : loss --> 12.094356536865234\n",
            "Training batch number: 437 : loss --> 10.961691856384277\n",
            "Training batch number: 438 : loss --> 18.319459915161133\n",
            "Training batch number: 439 : loss --> 58.57109069824219\n",
            "Training batch number: 440 : loss --> 2.701850652694702\n",
            "Training batch number: 441 : loss --> 21.385982513427734\n",
            "Training batch number: 442 : loss --> 11.871872901916504\n",
            "Training batch number: 443 : loss --> 15.77158260345459\n",
            "Training batch number: 444 : loss --> 4.520639419555664\n",
            "Training batch number: 445 : loss --> 22.483654022216797\n",
            "Training batch number: 446 : loss --> 1.3279526233673096\n",
            "Training batch number: 447 : loss --> 2.4382641315460205\n",
            "Training batch number: 448 : loss --> 9.360248565673828\n",
            "Training batch number: 449 : loss --> 7.401530742645264\n",
            "Training batch number: 450 : loss --> 7.080333232879639\n",
            "Training batch number: 451 : loss --> 12.943801879882812\n",
            "Training batch number: 452 : loss --> 1.9094990491867065\n",
            "Training batch number: 453 : loss --> 14.92933177947998\n",
            "Training batch number: 454 : loss --> 14.233725547790527\n",
            "Training batch number: 455 : loss --> 22.467395782470703\n",
            "Training batch number: 456 : loss --> 8.804060935974121\n",
            "Training batch number: 457 : loss --> 4.541405200958252\n",
            "Training batch number: 458 : loss --> 7.633018493652344\n",
            "Training batch number: 459 : loss --> 16.83197784423828\n",
            "Training batch number: 460 : loss --> 4.394786834716797\n",
            "Training batch number: 461 : loss --> 0.6105906963348389\n",
            "Training batch number: 462 : loss --> 4.313565254211426\n",
            "Training batch number: 463 : loss --> 14.175688743591309\n",
            "Training batch number: 464 : loss --> 8.562668800354004\n",
            "Training batch number: 465 : loss --> 7.791053295135498\n",
            "Training batch number: 466 : loss --> 11.54334545135498\n",
            "Training batch number: 467 : loss --> 4.788082122802734\n",
            "Training batch number: 468 : loss --> 17.508481979370117\n",
            "\n",
            "\n",
            "Training.. Loss: 0.1956 Acc: 0.9471\n",
            "Validating batch number: 0 : loss --> 14.040721893310547\n",
            "Validating batch number: 1 : loss --> 22.683740615844727\n",
            "Validating batch number: 2 : loss --> 17.180086135864258\n",
            "Validating batch number: 3 : loss --> 25.54800796508789\n",
            "Validating batch number: 4 : loss --> 12.698381423950195\n",
            "Validating batch number: 5 : loss --> 12.395540237426758\n",
            "Validating batch number: 6 : loss --> 9.615619659423828\n",
            "Validating batch number: 7 : loss --> 39.73427200317383\n",
            "Validating batch number: 8 : loss --> 24.63287925720215\n",
            "Validating batch number: 9 : loss --> 27.699901580810547\n",
            "Validating batch number: 10 : loss --> 12.801008224487305\n",
            "Validating batch number: 11 : loss --> 24.279296875\n",
            "Validating batch number: 12 : loss --> 10.480415344238281\n",
            "Validating batch number: 13 : loss --> 18.116138458251953\n",
            "Validating batch number: 14 : loss --> 27.248035430908203\n",
            "Validating batch number: 15 : loss --> 17.210542678833008\n",
            "Validating batch number: 16 : loss --> 20.9603214263916\n",
            "Validating batch number: 17 : loss --> 19.435386657714844\n",
            "Validating batch number: 18 : loss --> 18.868160247802734\n",
            "Validating batch number: 19 : loss --> 21.116241455078125\n",
            "Validating batch number: 20 : loss --> 12.104791641235352\n",
            "Validating batch number: 21 : loss --> 12.310279846191406\n",
            "Validating batch number: 22 : loss --> 24.287830352783203\n",
            "Validating batch number: 23 : loss --> 18.144752502441406\n",
            "Validating batch number: 24 : loss --> 16.976587295532227\n",
            "Validating batch number: 25 : loss --> 14.803462982177734\n",
            "Validating batch number: 26 : loss --> 12.883454322814941\n",
            "Validating batch number: 27 : loss --> 24.45522117614746\n",
            "Validating batch number: 28 : loss --> 7.884186744689941\n",
            "Validating batch number: 29 : loss --> 25.765119552612305\n",
            "Validating batch number: 30 : loss --> 33.10704803466797\n",
            "Validating batch number: 31 : loss --> 9.730390548706055\n",
            "Validating batch number: 32 : loss --> 17.29827308654785\n",
            "Validating batch number: 33 : loss --> 12.668105125427246\n",
            "Validating batch number: 34 : loss --> 38.589447021484375\n",
            "Validating batch number: 35 : loss --> 12.047688484191895\n",
            "Validating batch number: 36 : loss --> 39.5845832824707\n",
            "Validating batch number: 37 : loss --> 32.212913513183594\n",
            "Validating batch number: 38 : loss --> 10.748773574829102\n",
            "Validating batch number: 39 : loss --> 13.382380485534668\n",
            "Validating batch number: 40 : loss --> 28.217449188232422\n",
            "Validating batch number: 41 : loss --> 27.475791931152344\n",
            "Validating batch number: 42 : loss --> 11.038058280944824\n",
            "Validating batch number: 43 : loss --> 20.453432083129883\n",
            "Validating batch number: 44 : loss --> 15.315583229064941\n",
            "Validating batch number: 45 : loss --> 20.597665786743164\n",
            "Validating batch number: 46 : loss --> 19.601694107055664\n",
            "Validating batch number: 47 : loss --> 18.22425079345703\n",
            "Validating batch number: 48 : loss --> 10.253795623779297\n",
            "Validating batch number: 49 : loss --> 13.280177116394043\n",
            "Validating batch number: 50 : loss --> 18.133146286010742\n",
            "Validating batch number: 51 : loss --> 28.430980682373047\n",
            "Validating batch number: 52 : loss --> 5.209062099456787\n",
            "Validating batch number: 53 : loss --> 16.529773712158203\n",
            "Validating batch number: 54 : loss --> 6.6973676681518555\n",
            "Validating batch number: 55 : loss --> 12.057134628295898\n",
            "Validating batch number: 56 : loss --> 13.497552871704102\n",
            "Validating batch number: 57 : loss --> 23.313804626464844\n",
            "Validating batch number: 58 : loss --> 9.7347412109375\n",
            "Validating batch number: 59 : loss --> 13.252593040466309\n",
            "Validating batch number: 60 : loss --> 9.264378547668457\n",
            "Validating batch number: 61 : loss --> 28.478445053100586\n",
            "Validating batch number: 62 : loss --> 11.15725040435791\n",
            "Validating batch number: 63 : loss --> 14.089695930480957\n",
            "Validating batch number: 64 : loss --> 26.678491592407227\n",
            "Validating batch number: 65 : loss --> 19.312570571899414\n",
            "Validating batch number: 66 : loss --> 26.293546676635742\n",
            "Validating batch number: 67 : loss --> 9.2682523727417\n",
            "Validating batch number: 68 : loss --> 15.587356567382812\n",
            "Validating batch number: 69 : loss --> 24.828107833862305\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 14%|█▍        | 1/7 [00:29<02:54, 29.07s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validating batch number: 70 : loss --> 20.767513275146484\n",
            "Validating batch number: 71 : loss --> 14.918415069580078\n",
            "Validating batch number: 72 : loss --> 21.744657516479492\n",
            "Validating batch number: 73 : loss --> 20.73491668701172\n",
            "Validating batch number: 74 : loss --> 26.411731719970703\n",
            "Validating batch number: 75 : loss --> 23.966720581054688\n",
            "Validating batch number: 76 : loss --> 6.248996734619141\n",
            "Validating batch number: 77 : loss --> 26.451736450195312\n",
            "Validating batch number: 78 : loss --> 4.832505226135254\n",
            "Testing.. Loss: 0.1468 Acc: 0.9558\n",
            "\n",
            "Epoch 1/6\n",
            "----------\n",
            "Training batch number: 0 : loss --> 13.347970008850098\n",
            "Training batch number: 1 : loss --> 16.470138549804688\n",
            "Training batch number: 2 : loss --> 11.685773849487305\n",
            "Training batch number: 3 : loss --> 11.720813751220703\n",
            "Training batch number: 4 : loss --> 6.150473594665527\n",
            "Training batch number: 5 : loss --> 6.731794834136963\n",
            "Training batch number: 6 : loss --> 6.48577880859375\n",
            "Training batch number: 7 : loss --> 5.638011932373047\n",
            "Training batch number: 8 : loss --> 4.620834827423096\n",
            "Training batch number: 9 : loss --> 7.310709476470947\n",
            "Training batch number: 10 : loss --> 5.937193393707275\n",
            "Training batch number: 11 : loss --> 12.03294849395752\n",
            "Training batch number: 12 : loss --> 7.256185531616211\n",
            "Training batch number: 13 : loss --> 11.741742134094238\n",
            "Training batch number: 14 : loss --> 9.765963554382324\n",
            "Training batch number: 15 : loss --> 10.357770919799805\n",
            "Training batch number: 16 : loss --> 14.143637657165527\n",
            "Training batch number: 17 : loss --> 7.068192005157471\n",
            "Training batch number: 18 : loss --> 11.607080459594727\n",
            "Training batch number: 19 : loss --> 9.16979694366455\n",
            "Training batch number: 20 : loss --> 2.712852716445923\n",
            "Training batch number: 21 : loss --> 2.2486143112182617\n",
            "Training batch number: 22 : loss --> 1.9166483879089355\n",
            "Training batch number: 23 : loss --> 20.44541358947754\n",
            "Training batch number: 24 : loss --> 1.4610499143600464\n",
            "Training batch number: 25 : loss --> 3.110064744949341\n",
            "Training batch number: 26 : loss --> 1.503980278968811\n",
            "Training batch number: 27 : loss --> 2.345275640487671\n",
            "Training batch number: 28 : loss --> 5.745391368865967\n",
            "Training batch number: 29 : loss --> 7.577548980712891\n",
            "Training batch number: 30 : loss --> 9.119555473327637\n",
            "Training batch number: 31 : loss --> 8.424763679504395\n",
            "Training batch number: 32 : loss --> 5.8311333656311035\n",
            "Training batch number: 33 : loss --> 6.4267897605896\n",
            "Training batch number: 34 : loss --> 9.278305053710938\n",
            "Training batch number: 35 : loss --> 6.164926528930664\n",
            "Training batch number: 36 : loss --> 5.210064888000488\n",
            "Training batch number: 37 : loss --> 2.604022741317749\n",
            "Training batch number: 38 : loss --> 6.614396095275879\n",
            "Training batch number: 39 : loss --> 3.1288318634033203\n",
            "Training batch number: 40 : loss --> 9.451715469360352\n",
            "Training batch number: 41 : loss --> 21.185121536254883\n",
            "Training batch number: 42 : loss --> 2.3701443672180176\n",
            "Training batch number: 43 : loss --> 17.942655563354492\n",
            "Training batch number: 44 : loss --> 12.572248458862305\n",
            "Training batch number: 45 : loss --> 2.5212209224700928\n",
            "Training batch number: 46 : loss --> 3.9992213249206543\n",
            "Training batch number: 47 : loss --> 12.042417526245117\n",
            "Training batch number: 48 : loss --> 2.218035936355591\n",
            "Training batch number: 49 : loss --> 10.37484073638916\n",
            "Training batch number: 50 : loss --> 0.8280348181724548\n",
            "Training batch number: 51 : loss --> 7.721704483032227\n",
            "Training batch number: 52 : loss --> 3.6670589447021484\n",
            "Training batch number: 53 : loss --> 10.350638389587402\n",
            "Training batch number: 54 : loss --> 3.162822961807251\n",
            "Training batch number: 55 : loss --> 14.184386253356934\n",
            "Training batch number: 56 : loss --> 5.481052875518799\n",
            "Training batch number: 57 : loss --> 5.56300687789917\n",
            "Training batch number: 58 : loss --> 13.72909164428711\n",
            "Training batch number: 59 : loss --> 1.3670603036880493\n",
            "Training batch number: 60 : loss --> 6.9452619552612305\n",
            "Training batch number: 61 : loss --> 3.3386881351470947\n",
            "Training batch number: 62 : loss --> 6.485812187194824\n",
            "Training batch number: 63 : loss --> 4.221214771270752\n",
            "Training batch number: 64 : loss --> 2.7515530586242676\n",
            "Training batch number: 65 : loss --> 3.5035598278045654\n",
            "Training batch number: 66 : loss --> 15.714311599731445\n",
            "Training batch number: 67 : loss --> 10.450121879577637\n",
            "Training batch number: 68 : loss --> 0.9910925626754761\n",
            "Training batch number: 69 : loss --> 16.0605525970459\n",
            "Training batch number: 70 : loss --> 0.6380589008331299\n",
            "Training batch number: 71 : loss --> 5.852344036102295\n",
            "Training batch number: 72 : loss --> 4.1774821281433105\n",
            "Training batch number: 73 : loss --> 5.570583820343018\n",
            "Training batch number: 74 : loss --> 7.368596076965332\n",
            "Training batch number: 75 : loss --> 4.038426399230957\n",
            "Training batch number: 76 : loss --> 2.750145196914673\n",
            "Training batch number: 77 : loss --> 14.526671409606934\n",
            "Training batch number: 78 : loss --> 11.863100051879883\n",
            "Training batch number: 79 : loss --> 3.6728508472442627\n",
            "Training batch number: 80 : loss --> 0.9395184516906738\n",
            "Training batch number: 81 : loss --> 4.192979335784912\n",
            "Training batch number: 82 : loss --> 10.049654006958008\n",
            "Training batch number: 83 : loss --> 7.74184513092041\n",
            "Training batch number: 84 : loss --> 12.84456729888916\n",
            "Training batch number: 85 : loss --> 7.199014186859131\n",
            "Training batch number: 86 : loss --> 0.6593133807182312\n",
            "Training batch number: 87 : loss --> 19.160505294799805\n",
            "Training batch number: 88 : loss --> 3.7028374671936035\n",
            "Training batch number: 89 : loss --> 5.692363739013672\n",
            "Training batch number: 90 : loss --> 3.1616508960723877\n",
            "Training batch number: 91 : loss --> 5.744185447692871\n",
            "Training batch number: 92 : loss --> 2.7671680450439453\n",
            "Training batch number: 93 : loss --> 7.429999828338623\n",
            "Training batch number: 94 : loss --> 10.038300514221191\n",
            "Training batch number: 95 : loss --> 12.3528413772583\n",
            "Training batch number: 96 : loss --> 5.439393043518066\n",
            "Training batch number: 97 : loss --> 17.33829689025879\n",
            "Training batch number: 98 : loss --> 17.838842391967773\n",
            "Training batch number: 99 : loss --> 0.5056986808776855\n",
            "Training batch number: 100 : loss --> 6.99503231048584\n",
            "Training batch number: 101 : loss --> 20.744436264038086\n",
            "Training batch number: 102 : loss --> 1.2412792444229126\n",
            "Training batch number: 103 : loss --> 3.613395929336548\n",
            "Training batch number: 104 : loss --> 0.7857900857925415\n",
            "Training batch number: 105 : loss --> 7.798315525054932\n",
            "Training batch number: 106 : loss --> 4.328658103942871\n",
            "Training batch number: 107 : loss --> 3.436992645263672\n",
            "Training batch number: 108 : loss --> 1.33432936668396\n",
            "Training batch number: 109 : loss --> 12.256817817687988\n",
            "Training batch number: 110 : loss --> 5.287689685821533\n",
            "Training batch number: 111 : loss --> 1.4425383806228638\n",
            "Training batch number: 112 : loss --> 1.7386250495910645\n",
            "Training batch number: 113 : loss --> 3.908233880996704\n",
            "Training batch number: 114 : loss --> 2.617596387863159\n",
            "Training batch number: 115 : loss --> 10.595100402832031\n",
            "Training batch number: 116 : loss --> 11.06941032409668\n",
            "Training batch number: 117 : loss --> 1.822817087173462\n",
            "Training batch number: 118 : loss --> 7.894748210906982\n",
            "Training batch number: 119 : loss --> 7.869361400604248\n",
            "Training batch number: 120 : loss --> 16.221097946166992\n",
            "Training batch number: 121 : loss --> 4.591270446777344\n",
            "Training batch number: 122 : loss --> 3.707676649093628\n",
            "Training batch number: 123 : loss --> 2.0237882137298584\n",
            "Training batch number: 124 : loss --> 2.8490171432495117\n",
            "Training batch number: 125 : loss --> 1.000428318977356\n",
            "Training batch number: 126 : loss --> 6.526381492614746\n",
            "Training batch number: 127 : loss --> 8.747584342956543\n",
            "Training batch number: 128 : loss --> 7.397134780883789\n",
            "Training batch number: 129 : loss --> 2.758511543273926\n",
            "Training batch number: 130 : loss --> 8.1217622756958\n",
            "Training batch number: 131 : loss --> 6.515111446380615\n",
            "Training batch number: 132 : loss --> 6.610016345977783\n",
            "Training batch number: 133 : loss --> 11.304060935974121\n",
            "Training batch number: 134 : loss --> 2.7452406883239746\n",
            "Training batch number: 135 : loss --> 7.072784423828125\n",
            "Training batch number: 136 : loss --> 3.35528564453125\n",
            "Training batch number: 137 : loss --> 10.769796371459961\n",
            "Training batch number: 138 : loss --> 8.87481689453125\n",
            "Training batch number: 139 : loss --> 6.563411712646484\n",
            "Training batch number: 140 : loss --> 2.7050297260284424\n",
            "Training batch number: 141 : loss --> 1.6746426820755005\n",
            "Training batch number: 142 : loss --> 0.9094855189323425\n",
            "Training batch number: 143 : loss --> 9.418913841247559\n",
            "Training batch number: 144 : loss --> 6.65588903427124\n",
            "Training batch number: 145 : loss --> 11.652649879455566\n",
            "Training batch number: 146 : loss --> 24.33431625366211\n",
            "Training batch number: 147 : loss --> 3.7777600288391113\n",
            "Training batch number: 148 : loss --> 3.944608688354492\n",
            "Training batch number: 149 : loss --> 5.8453688621521\n",
            "Training batch number: 150 : loss --> 7.094658374786377\n",
            "Training batch number: 151 : loss --> 4.118668556213379\n",
            "Training batch number: 152 : loss --> 9.665587425231934\n",
            "Training batch number: 153 : loss --> 4.769613742828369\n",
            "Training batch number: 154 : loss --> 23.261165618896484\n",
            "Training batch number: 155 : loss --> 16.052425384521484\n",
            "Training batch number: 156 : loss --> 1.976401686668396\n",
            "Training batch number: 157 : loss --> 0.7095785737037659\n",
            "Training batch number: 158 : loss --> 11.767322540283203\n",
            "Training batch number: 159 : loss --> 1.9396607875823975\n",
            "Training batch number: 160 : loss --> 20.89435386657715\n",
            "Training batch number: 161 : loss --> 2.202460289001465\n",
            "Training batch number: 162 : loss --> 4.394389629364014\n",
            "Training batch number: 163 : loss --> 19.435832977294922\n",
            "Training batch number: 164 : loss --> 6.421878814697266\n",
            "Training batch number: 165 : loss --> 3.04732608795166\n",
            "Training batch number: 166 : loss --> 6.365715980529785\n",
            "Training batch number: 167 : loss --> 5.568230152130127\n",
            "Training batch number: 168 : loss --> 7.318836212158203\n",
            "Training batch number: 169 : loss --> 2.4599549770355225\n",
            "Training batch number: 170 : loss --> 2.8222594261169434\n",
            "Training batch number: 171 : loss --> 6.785249710083008\n",
            "Training batch number: 172 : loss --> 10.67832088470459\n",
            "Training batch number: 173 : loss --> 9.633869171142578\n",
            "Training batch number: 174 : loss --> 3.469456672668457\n",
            "Training batch number: 175 : loss --> 10.112317085266113\n",
            "Training batch number: 176 : loss --> 6.495710849761963\n",
            "Training batch number: 177 : loss --> 4.080031394958496\n",
            "Training batch number: 178 : loss --> 2.0070488452911377\n",
            "Training batch number: 179 : loss --> 5.325769901275635\n",
            "Training batch number: 180 : loss --> 3.133615493774414\n",
            "Training batch number: 181 : loss --> 8.86476993560791\n",
            "Training batch number: 182 : loss --> 0.8646824955940247\n",
            "Training batch number: 183 : loss --> 9.640494346618652\n",
            "Training batch number: 184 : loss --> 0.50900799036026\n",
            "Training batch number: 185 : loss --> 4.227145195007324\n",
            "Training batch number: 186 : loss --> 9.284087181091309\n",
            "Training batch number: 187 : loss --> 0.6315014362335205\n",
            "Training batch number: 188 : loss --> 7.744081020355225\n",
            "Training batch number: 189 : loss --> 8.60270881652832\n",
            "Training batch number: 190 : loss --> 12.01480484008789\n",
            "Training batch number: 191 : loss --> 4.775529861450195\n",
            "Training batch number: 192 : loss --> 0.44569671154022217\n",
            "Training batch number: 193 : loss --> 1.0767101049423218\n",
            "Training batch number: 194 : loss --> 3.2397420406341553\n",
            "Training batch number: 195 : loss --> 9.391447067260742\n",
            "Training batch number: 196 : loss --> 19.02880859375\n",
            "Training batch number: 197 : loss --> 6.4091572761535645\n",
            "Training batch number: 198 : loss --> 1.6853256225585938\n",
            "Training batch number: 199 : loss --> 2.1855151653289795\n",
            "Training batch number: 200 : loss --> 3.869967222213745\n",
            "Training batch number: 201 : loss --> 0.7372474670410156\n",
            "Training batch number: 202 : loss --> 3.086113452911377\n",
            "Training batch number: 203 : loss --> 1.7132701873779297\n",
            "Training batch number: 204 : loss --> 9.466136932373047\n",
            "Training batch number: 205 : loss --> 9.239388465881348\n",
            "Training batch number: 206 : loss --> 8.210736274719238\n",
            "Training batch number: 207 : loss --> 10.94809341430664\n",
            "Training batch number: 208 : loss --> 0.5294426679611206\n",
            "Training batch number: 209 : loss --> 4.734019756317139\n",
            "Training batch number: 210 : loss --> 7.525135517120361\n",
            "Training batch number: 211 : loss --> 0.6837204098701477\n",
            "Training batch number: 212 : loss --> 16.966487884521484\n",
            "Training batch number: 213 : loss --> 14.755038261413574\n",
            "Training batch number: 214 : loss --> 2.8410117626190186\n",
            "Training batch number: 215 : loss --> 2.9029996395111084\n",
            "Training batch number: 216 : loss --> 10.259359359741211\n",
            "Training batch number: 217 : loss --> 1.4663175344467163\n",
            "Training batch number: 218 : loss --> 5.9413371086120605\n",
            "Training batch number: 219 : loss --> 5.255288124084473\n",
            "Training batch number: 220 : loss --> 0.5249683856964111\n",
            "Training batch number: 221 : loss --> 6.921792030334473\n",
            "Training batch number: 222 : loss --> 3.3891701698303223\n",
            "Training batch number: 223 : loss --> 2.408027172088623\n",
            "Training batch number: 224 : loss --> 9.22919750213623\n",
            "Training batch number: 225 : loss --> 8.08379077911377\n",
            "Training batch number: 226 : loss --> 1.515903115272522\n",
            "Training batch number: 227 : loss --> 7.4657368659973145\n",
            "Training batch number: 228 : loss --> 9.660592079162598\n",
            "Training batch number: 229 : loss --> 1.0450758934020996\n",
            "Training batch number: 230 : loss --> 9.34455680847168\n",
            "Training batch number: 231 : loss --> 5.867650032043457\n",
            "Training batch number: 232 : loss --> 1.6429182291030884\n",
            "Training batch number: 233 : loss --> 5.745063304901123\n",
            "Training batch number: 234 : loss --> 2.7155511379241943\n",
            "Training batch number: 235 : loss --> 10.381017684936523\n",
            "Training batch number: 236 : loss --> 21.452007293701172\n",
            "Training batch number: 237 : loss --> 6.239895820617676\n",
            "Training batch number: 238 : loss --> 2.360621929168701\n",
            "Training batch number: 239 : loss --> 9.549531936645508\n",
            "Training batch number: 240 : loss --> 1.2586690187454224\n",
            "Training batch number: 241 : loss --> 1.8773210048675537\n",
            "Training batch number: 242 : loss --> 4.774137496948242\n",
            "Training batch number: 243 : loss --> 4.793093204498291\n",
            "Training batch number: 244 : loss --> 1.1679208278656006\n",
            "Training batch number: 245 : loss --> 3.2089900970458984\n",
            "Training batch number: 246 : loss --> 10.986433982849121\n",
            "Training batch number: 247 : loss --> 20.4634952545166\n",
            "Training batch number: 248 : loss --> 7.244304656982422\n",
            "Training batch number: 249 : loss --> 9.109551429748535\n",
            "Training batch number: 250 : loss --> 21.444753646850586\n",
            "Training batch number: 251 : loss --> 22.327116012573242\n",
            "Training batch number: 252 : loss --> 6.137210369110107\n",
            "Training batch number: 253 : loss --> 3.950571298599243\n",
            "Training batch number: 254 : loss --> 8.155210494995117\n",
            "Training batch number: 255 : loss --> 2.6094369888305664\n",
            "Training batch number: 256 : loss --> 6.7203569412231445\n",
            "Training batch number: 257 : loss --> 11.980265617370605\n",
            "Training batch number: 258 : loss --> 8.730669975280762\n",
            "Training batch number: 259 : loss --> 8.049941062927246\n",
            "Training batch number: 260 : loss --> 12.040167808532715\n",
            "Training batch number: 261 : loss --> 3.8967034816741943\n",
            "Training batch number: 262 : loss --> 12.723453521728516\n",
            "Training batch number: 263 : loss --> 3.1398770809173584\n",
            "Training batch number: 264 : loss --> 7.223137378692627\n",
            "Training batch number: 265 : loss --> 1.5844824314117432\n",
            "Training batch number: 266 : loss --> 3.9410879611968994\n",
            "Training batch number: 267 : loss --> 2.6046369075775146\n",
            "Training batch number: 268 : loss --> 3.069962501525879\n",
            "Training batch number: 269 : loss --> 1.063062071800232\n",
            "Training batch number: 270 : loss --> 1.4922279119491577\n",
            "Training batch number: 271 : loss --> 0.29750433564186096\n",
            "Training batch number: 272 : loss --> 3.059432029724121\n",
            "Training batch number: 273 : loss --> 13.351523399353027\n",
            "Training batch number: 274 : loss --> 2.9561843872070312\n",
            "Training batch number: 275 : loss --> 5.619447231292725\n",
            "Training batch number: 276 : loss --> 2.8372063636779785\n",
            "Training batch number: 277 : loss --> 14.625737190246582\n",
            "Training batch number: 278 : loss --> 10.00069808959961\n",
            "Training batch number: 279 : loss --> 0.8038555979728699\n",
            "Training batch number: 280 : loss --> 2.0917696952819824\n",
            "Training batch number: 281 : loss --> 3.288137197494507\n",
            "Training batch number: 282 : loss --> 2.7759389877319336\n",
            "Training batch number: 283 : loss --> 7.9577555656433105\n",
            "Training batch number: 284 : loss --> 5.044109344482422\n",
            "Training batch number: 285 : loss --> 8.914928436279297\n",
            "Training batch number: 286 : loss --> 2.3319082260131836\n",
            "Training batch number: 287 : loss --> 11.423857688903809\n",
            "Training batch number: 288 : loss --> 0.7720447182655334\n",
            "Training batch number: 289 : loss --> 2.4581844806671143\n",
            "Training batch number: 290 : loss --> 1.7694698572158813\n",
            "Training batch number: 291 : loss --> 2.6191744804382324\n",
            "Training batch number: 292 : loss --> 4.599111080169678\n",
            "Training batch number: 293 : loss --> 0.7207680344581604\n",
            "Training batch number: 294 : loss --> 10.412155151367188\n",
            "Training batch number: 295 : loss --> 6.585719108581543\n",
            "Training batch number: 296 : loss --> 3.2877025604248047\n",
            "Training batch number: 297 : loss --> 1.3493430614471436\n",
            "Training batch number: 298 : loss --> 8.030975341796875\n",
            "Training batch number: 299 : loss --> 29.70594024658203\n",
            "Training batch number: 300 : loss --> 5.069459438323975\n",
            "Training batch number: 301 : loss --> 7.994162559509277\n",
            "Training batch number: 302 : loss --> 6.529227256774902\n",
            "Training batch number: 303 : loss --> 7.648157119750977\n",
            "Training batch number: 304 : loss --> 4.6402201652526855\n",
            "Training batch number: 305 : loss --> 11.969581604003906\n",
            "Training batch number: 306 : loss --> 3.1217544078826904\n",
            "Training batch number: 307 : loss --> 11.110638618469238\n",
            "Training batch number: 308 : loss --> 1.1046746969223022\n",
            "Training batch number: 309 : loss --> 0.6902910470962524\n",
            "Training batch number: 310 : loss --> 1.914452075958252\n",
            "Training batch number: 311 : loss --> 0.8375572562217712\n",
            "Training batch number: 312 : loss --> 2.950021743774414\n",
            "Training batch number: 313 : loss --> 0.7329733967781067\n",
            "Training batch number: 314 : loss --> 3.367250442504883\n",
            "Training batch number: 315 : loss --> 1.6583670377731323\n",
            "Training batch number: 316 : loss --> 5.784485816955566\n",
            "Training batch number: 317 : loss --> 2.4424304962158203\n",
            "Training batch number: 318 : loss --> 6.215519905090332\n",
            "Training batch number: 319 : loss --> 0.5307791829109192\n",
            "Training batch number: 320 : loss --> 4.692094326019287\n",
            "Training batch number: 321 : loss --> 1.8295257091522217\n",
            "Training batch number: 322 : loss --> 3.8731257915496826\n",
            "Training batch number: 323 : loss --> 3.1409926414489746\n",
            "Training batch number: 324 : loss --> 2.7482285499572754\n",
            "Training batch number: 325 : loss --> 3.115950345993042\n",
            "Training batch number: 326 : loss --> 3.5836899280548096\n",
            "Training batch number: 327 : loss --> 8.495970726013184\n",
            "Training batch number: 328 : loss --> 8.072080612182617\n",
            "Training batch number: 329 : loss --> 6.030678749084473\n",
            "Training batch number: 330 : loss --> 3.7536461353302\n",
            "Training batch number: 331 : loss --> 8.309713363647461\n",
            "Training batch number: 332 : loss --> 8.342790603637695\n",
            "Training batch number: 333 : loss --> 9.990467071533203\n",
            "Training batch number: 334 : loss --> 11.003087997436523\n",
            "Training batch number: 335 : loss --> 6.896652698516846\n",
            "Training batch number: 336 : loss --> 5.4026875495910645\n",
            "Training batch number: 337 : loss --> 2.5496201515197754\n",
            "Training batch number: 338 : loss --> 2.0275819301605225\n",
            "Training batch number: 339 : loss --> 1.5656794309616089\n",
            "Training batch number: 340 : loss --> 7.990112781524658\n",
            "Training batch number: 341 : loss --> 2.4938907623291016\n",
            "Training batch number: 342 : loss --> 0.7348250150680542\n",
            "Training batch number: 343 : loss --> 6.775020599365234\n",
            "Training batch number: 344 : loss --> 4.421013355255127\n",
            "Training batch number: 345 : loss --> 24.222410202026367\n",
            "Training batch number: 346 : loss --> 8.758346557617188\n",
            "Training batch number: 347 : loss --> 0.29348647594451904\n",
            "Training batch number: 348 : loss --> 14.4323091506958\n",
            "Training batch number: 349 : loss --> 5.422138690948486\n",
            "Training batch number: 350 : loss --> 20.82579803466797\n",
            "Training batch number: 351 : loss --> 7.532962799072266\n",
            "Training batch number: 352 : loss --> 9.255035400390625\n",
            "Training batch number: 353 : loss --> 11.391968727111816\n",
            "Training batch number: 354 : loss --> 0.9486252665519714\n",
            "Training batch number: 355 : loss --> 4.708689212799072\n",
            "Training batch number: 356 : loss --> 4.740012168884277\n",
            "Training batch number: 357 : loss --> 12.744933128356934\n",
            "Training batch number: 358 : loss --> 17.215484619140625\n",
            "Training batch number: 359 : loss --> 11.857909202575684\n",
            "Training batch number: 360 : loss --> 4.805398941040039\n",
            "Training batch number: 361 : loss --> 15.883612632751465\n",
            "Training batch number: 362 : loss --> 9.375566482543945\n",
            "Training batch number: 363 : loss --> 20.188020706176758\n",
            "Training batch number: 364 : loss --> 9.01866340637207\n",
            "Training batch number: 365 : loss --> 22.1280460357666\n",
            "Training batch number: 366 : loss --> 15.831607818603516\n",
            "Training batch number: 367 : loss --> 3.6494412422180176\n",
            "Training batch number: 368 : loss --> 0.824481189250946\n",
            "Training batch number: 369 : loss --> 4.008944988250732\n",
            "Training batch number: 370 : loss --> 4.7203145027160645\n",
            "Training batch number: 371 : loss --> 4.120973587036133\n",
            "Training batch number: 372 : loss --> 4.145735263824463\n",
            "Training batch number: 373 : loss --> 4.532810211181641\n",
            "Training batch number: 374 : loss --> 5.620163917541504\n",
            "Training batch number: 375 : loss --> 20.5217342376709\n",
            "Training batch number: 376 : loss --> 3.9070897102355957\n",
            "Training batch number: 377 : loss --> 4.9413161277771\n",
            "Training batch number: 378 : loss --> 0.8159427642822266\n",
            "Training batch number: 379 : loss --> 24.09124183654785\n",
            "Training batch number: 380 : loss --> 1.7665596008300781\n",
            "Training batch number: 381 : loss --> 4.646694660186768\n",
            "Training batch number: 382 : loss --> 8.444525718688965\n",
            "Training batch number: 383 : loss --> 8.199638366699219\n",
            "Training batch number: 384 : loss --> 5.860185146331787\n",
            "Training batch number: 385 : loss --> 2.433385133743286\n",
            "Training batch number: 386 : loss --> 4.439055919647217\n",
            "Training batch number: 387 : loss --> 3.181758403778076\n",
            "Training batch number: 388 : loss --> 2.8465707302093506\n",
            "Training batch number: 389 : loss --> 2.009977102279663\n",
            "Training batch number: 390 : loss --> 8.181038856506348\n",
            "Training batch number: 391 : loss --> 10.657144546508789\n",
            "Training batch number: 392 : loss --> 9.731427192687988\n",
            "Training batch number: 393 : loss --> 4.8213067054748535\n",
            "Training batch number: 394 : loss --> 5.7285475730896\n",
            "Training batch number: 395 : loss --> 9.923223495483398\n",
            "Training batch number: 396 : loss --> 1.3392510414123535\n",
            "Training batch number: 397 : loss --> 2.6628479957580566\n",
            "Training batch number: 398 : loss --> 4.321756362915039\n",
            "Training batch number: 399 : loss --> 2.7927966117858887\n",
            "Training batch number: 400 : loss --> 3.6578006744384766\n",
            "Training batch number: 401 : loss --> 2.1752636432647705\n",
            "Training batch number: 402 : loss --> 4.246365547180176\n",
            "Training batch number: 403 : loss --> 0.6516937613487244\n",
            "Training batch number: 404 : loss --> 1.98637855052948\n",
            "Training batch number: 405 : loss --> 7.196631908416748\n",
            "Training batch number: 406 : loss --> 5.32053279876709\n",
            "Training batch number: 407 : loss --> 2.1124281883239746\n",
            "Training batch number: 408 : loss --> 3.3079912662506104\n",
            "Training batch number: 409 : loss --> 4.19984245300293\n",
            "Training batch number: 410 : loss --> 1.6913825273513794\n",
            "Training batch number: 411 : loss --> 0.7208075523376465\n",
            "Training batch number: 412 : loss --> 0.5991106629371643\n",
            "Training batch number: 413 : loss --> 2.580827474594116\n",
            "Training batch number: 414 : loss --> 14.206558227539062\n",
            "Training batch number: 415 : loss --> 3.1938867568969727\n",
            "Training batch number: 416 : loss --> 3.967410087585449\n",
            "Training batch number: 417 : loss --> 7.545459747314453\n",
            "Training batch number: 418 : loss --> 1.3836644887924194\n",
            "Training batch number: 419 : loss --> 1.0921355485916138\n",
            "Training batch number: 420 : loss --> 2.435189723968506\n",
            "Training batch number: 421 : loss --> 0.4028919041156769\n",
            "Training batch number: 422 : loss --> 3.0806846618652344\n",
            "Training batch number: 423 : loss --> 3.137984037399292\n",
            "Training batch number: 424 : loss --> 2.6111936569213867\n",
            "Training batch number: 425 : loss --> 1.3929948806762695\n",
            "Training batch number: 426 : loss --> 11.094926834106445\n",
            "Training batch number: 427 : loss --> 0.5527623891830444\n",
            "Training batch number: 428 : loss --> 0.3386610746383667\n",
            "Training batch number: 429 : loss --> 1.2108154296875\n",
            "Training batch number: 430 : loss --> 1.2858529090881348\n",
            "Training batch number: 431 : loss --> 0.9173042178153992\n",
            "Training batch number: 432 : loss --> 5.797478675842285\n",
            "Training batch number: 433 : loss --> 0.7485998272895813\n",
            "Training batch number: 434 : loss --> 5.320252418518066\n",
            "Training batch number: 435 : loss --> 9.038566589355469\n",
            "Training batch number: 436 : loss --> 2.7383012771606445\n",
            "Training batch number: 437 : loss --> 3.292022705078125\n",
            "Training batch number: 438 : loss --> 8.081323623657227\n",
            "Training batch number: 439 : loss --> 4.966247081756592\n",
            "Training batch number: 440 : loss --> 1.6182701587677002\n",
            "Training batch number: 441 : loss --> 4.325026512145996\n",
            "Training batch number: 442 : loss --> 5.479552745819092\n",
            "Training batch number: 443 : loss --> 9.639732360839844\n",
            "Training batch number: 444 : loss --> 10.771848678588867\n",
            "Training batch number: 445 : loss --> 18.938737869262695\n",
            "Training batch number: 446 : loss --> 7.538344860076904\n",
            "Training batch number: 447 : loss --> 8.935209274291992\n",
            "Training batch number: 448 : loss --> 6.552305698394775\n",
            "Training batch number: 449 : loss --> 7.135171890258789\n",
            "Training batch number: 450 : loss --> 7.345285892486572\n",
            "Training batch number: 451 : loss --> 5.383561611175537\n",
            "Training batch number: 452 : loss --> 5.682036399841309\n",
            "Training batch number: 453 : loss --> 17.66115951538086\n",
            "Training batch number: 454 : loss --> 6.851768493652344\n",
            "Training batch number: 455 : loss --> 11.509696006774902\n",
            "Training batch number: 456 : loss --> 6.271632194519043\n",
            "Training batch number: 457 : loss --> 8.539271354675293\n",
            "Training batch number: 458 : loss --> 2.8827600479125977\n",
            "Training batch number: 459 : loss --> 2.3668689727783203\n",
            "Training batch number: 460 : loss --> 0.8560365438461304\n",
            "Training batch number: 461 : loss --> 3.9213101863861084\n",
            "Training batch number: 462 : loss --> 1.1774908304214478\n",
            "Training batch number: 463 : loss --> 10.667692184448242\n",
            "Training batch number: 464 : loss --> 1.5599420070648193\n",
            "Training batch number: 465 : loss --> 8.980232238769531\n",
            "Training batch number: 466 : loss --> 1.500697135925293\n",
            "Training batch number: 467 : loss --> 6.115701198577881\n",
            "Training batch number: 468 : loss --> 4.425523281097412\n",
            "\n",
            "\n",
            "Training.. Loss: 0.0504 Acc: 0.9858\n",
            "Validating batch number: 0 : loss --> 9.229331016540527\n",
            "Validating batch number: 1 : loss --> 9.349736213684082\n",
            "Validating batch number: 2 : loss --> 0.2240554392337799\n",
            "Validating batch number: 3 : loss --> 0.21607674658298492\n",
            "Validating batch number: 4 : loss --> 1.1243047714233398\n",
            "Validating batch number: 5 : loss --> 2.7195401191711426\n",
            "Validating batch number: 6 : loss --> 2.2241616249084473\n",
            "Validating batch number: 7 : loss --> 5.377573490142822\n",
            "Validating batch number: 8 : loss --> 4.854582786560059\n",
            "Validating batch number: 9 : loss --> 3.3823747634887695\n",
            "Validating batch number: 10 : loss --> 9.11322021484375\n",
            "Validating batch number: 11 : loss --> 0.42354217171669006\n",
            "Validating batch number: 12 : loss --> 0.3654234707355499\n",
            "Validating batch number: 13 : loss --> 0.4022033214569092\n",
            "Validating batch number: 14 : loss --> 3.623370885848999\n",
            "Validating batch number: 15 : loss --> 2.88942813873291\n",
            "Validating batch number: 16 : loss --> 1.4399369955062866\n",
            "Validating batch number: 17 : loss --> 0.757452666759491\n",
            "Validating batch number: 18 : loss --> 0.7206690311431885\n",
            "Validating batch number: 19 : loss --> 1.4872597455978394\n",
            "Validating batch number: 20 : loss --> 7.505025386810303\n",
            "Validating batch number: 21 : loss --> 0.33529847860336304\n",
            "Validating batch number: 22 : loss --> 6.742613792419434\n",
            "Validating batch number: 23 : loss --> 8.089659690856934\n",
            "Validating batch number: 24 : loss --> 1.719437599182129\n",
            "Validating batch number: 25 : loss --> 3.627518653869629\n",
            "Validating batch number: 26 : loss --> 1.0821311473846436\n",
            "Validating batch number: 27 : loss --> 2.2184507846832275\n",
            "Validating batch number: 28 : loss --> 1.369817852973938\n",
            "Validating batch number: 29 : loss --> 7.836223125457764\n",
            "Validating batch number: 30 : loss --> 3.2230749130249023\n",
            "Validating batch number: 31 : loss --> 0.7674463391304016\n",
            "Validating batch number: 32 : loss --> 0.8777991533279419\n",
            "Validating batch number: 33 : loss --> 2.7763822078704834\n",
            "Validating batch number: 34 : loss --> 10.878070831298828\n",
            "Validating batch number: 35 : loss --> 5.335959434509277\n",
            "Validating batch number: 36 : loss --> 7.8419413566589355\n",
            "Validating batch number: 37 : loss --> 0.44608253240585327\n",
            "Validating batch number: 38 : loss --> 11.232373237609863\n",
            "Validating batch number: 39 : loss --> 9.382279396057129\n",
            "Validating batch number: 40 : loss --> 2.9365642070770264\n",
            "Validating batch number: 41 : loss --> 7.355379581451416\n",
            "Validating batch number: 42 : loss --> 14.426877975463867\n",
            "Validating batch number: 43 : loss --> 7.971793174743652\n",
            "Validating batch number: 44 : loss --> 7.44687557220459\n",
            "Validating batch number: 45 : loss --> 5.351199626922607\n",
            "Validating batch number: 46 : loss --> 0.22932401299476624\n",
            "Validating batch number: 47 : loss --> 2.4051403999328613\n",
            "Validating batch number: 48 : loss --> 1.8569694757461548\n",
            "Validating batch number: 49 : loss --> 3.13474702835083\n",
            "Validating batch number: 50 : loss --> 0.8691552877426147\n",
            "Validating batch number: 51 : loss --> 6.804410457611084\n",
            "Validating batch number: 52 : loss --> 0.8701624870300293\n",
            "Validating batch number: 53 : loss --> 0.6404553055763245\n",
            "Validating batch number: 54 : loss --> 0.4788197875022888\n",
            "Validating batch number: 55 : loss --> 0.42527130246162415\n",
            "Validating batch number: 56 : loss --> 2.15616512298584\n",
            "Validating batch number: 57 : loss --> 0.7407147288322449\n",
            "Validating batch number: 58 : loss --> 2.5452075004577637\n",
            "Validating batch number: 59 : loss --> 0.5570612549781799\n",
            "Validating batch number: 60 : loss --> 11.270939826965332\n",
            "Validating batch number: 61 : loss --> 3.017730474472046\n",
            "Validating batch number: 62 : loss --> 2.514619827270508\n",
            "Validating batch number: 63 : loss --> 0.7694598436355591\n",
            "Validating batch number: 64 : loss --> 2.0489325523376465\n",
            "Validating batch number: 65 : loss --> 8.892733573913574\n",
            "Validating batch number: 66 : loss --> 9.306662559509277\n",
            "Validating batch number: 67 : loss --> 6.351640701293945\n",
            "Validating batch number: 68 : loss --> 9.946748733520508\n",
            "Validating batch number: 69 : loss --> 3.0880234241485596\n",
            "Validating batch number: 70 : loss --> 9.454032897949219\n",
            "Validating batch number: 71 : loss --> 9.643861770629883\n",
            "Validating batch number: 72 : loss --> 0.4369288682937622\n",
            "Validating batch number: 73 : loss --> 4.716030120849609\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 29%|██▊       | 2/7 [00:57<02:24, 28.91s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validating batch number: 74 : loss --> 1.861401915550232\n",
            "Validating batch number: 75 : loss --> 5.753533363342285\n",
            "Validating batch number: 76 : loss --> 0.8456918597221375\n",
            "Validating batch number: 77 : loss --> 3.675405502319336\n",
            "Validating batch number: 78 : loss --> 1.0668883323669434\n",
            "Testing.. Loss: 0.0317 Acc: 0.9909\n",
            "\n",
            "Epoch 2/6\n",
            "----------\n",
            "Training batch number: 0 : loss --> 0.2511656880378723\n",
            "Training batch number: 1 : loss --> 0.6392956972122192\n",
            "Training batch number: 2 : loss --> 16.077308654785156\n",
            "Training batch number: 3 : loss --> 5.43925142288208\n",
            "Training batch number: 4 : loss --> 4.05754280090332\n",
            "Training batch number: 5 : loss --> 5.754485130310059\n",
            "Training batch number: 6 : loss --> 5.272332191467285\n",
            "Training batch number: 7 : loss --> 5.922573566436768\n",
            "Training batch number: 8 : loss --> 0.45090344548225403\n",
            "Training batch number: 9 : loss --> 4.3552656173706055\n",
            "Training batch number: 10 : loss --> 3.423917055130005\n",
            "Training batch number: 11 : loss --> 2.096402168273926\n",
            "Training batch number: 12 : loss --> 11.612166404724121\n",
            "Training batch number: 13 : loss --> 5.748455047607422\n",
            "Training batch number: 14 : loss --> 1.8320993185043335\n",
            "Training batch number: 15 : loss --> 3.6201462745666504\n",
            "Training batch number: 16 : loss --> 10.93393325805664\n",
            "Training batch number: 17 : loss --> 7.940079689025879\n",
            "Training batch number: 18 : loss --> 2.8033103942871094\n",
            "Training batch number: 19 : loss --> 4.533799171447754\n",
            "Training batch number: 20 : loss --> 7.147861957550049\n",
            "Training batch number: 21 : loss --> 3.443518877029419\n",
            "Training batch number: 22 : loss --> 3.2171173095703125\n",
            "Training batch number: 23 : loss --> 4.684641361236572\n",
            "Training batch number: 24 : loss --> 10.183220863342285\n",
            "Training batch number: 25 : loss --> 4.831164836883545\n",
            "Training batch number: 26 : loss --> 1.5303025245666504\n",
            "Training batch number: 27 : loss --> 6.703012943267822\n",
            "Training batch number: 28 : loss --> 2.6001954078674316\n",
            "Training batch number: 29 : loss --> 11.811637878417969\n",
            "Training batch number: 30 : loss --> 4.167987823486328\n",
            "Training batch number: 31 : loss --> 0.4005618989467621\n",
            "Training batch number: 32 : loss --> 1.0418792963027954\n",
            "Training batch number: 33 : loss --> 7.428795337677002\n",
            "Training batch number: 34 : loss --> 0.40387797355651855\n",
            "Training batch number: 35 : loss --> 0.6001959443092346\n",
            "Training batch number: 36 : loss --> 5.718107223510742\n",
            "Training batch number: 37 : loss --> 0.7450101971626282\n",
            "Training batch number: 38 : loss --> 0.8298599720001221\n",
            "Training batch number: 39 : loss --> 0.8633884191513062\n",
            "Training batch number: 40 : loss --> 4.748458385467529\n",
            "Training batch number: 41 : loss --> 5.380227088928223\n",
            "Training batch number: 42 : loss --> 1.8286449909210205\n",
            "Training batch number: 43 : loss --> 0.13681133091449738\n",
            "Training batch number: 44 : loss --> 7.251100540161133\n",
            "Training batch number: 45 : loss --> 1.42857027053833\n",
            "Training batch number: 46 : loss --> 4.626039028167725\n",
            "Training batch number: 47 : loss --> 7.001106262207031\n",
            "Training batch number: 48 : loss --> 2.7544305324554443\n",
            "Training batch number: 49 : loss --> 0.19681425392627716\n",
            "Training batch number: 50 : loss --> 4.578472137451172\n",
            "Training batch number: 51 : loss --> 4.16573429107666\n",
            "Training batch number: 52 : loss --> 1.4759286642074585\n",
            "Training batch number: 53 : loss --> 2.6692397594451904\n",
            "Training batch number: 54 : loss --> 0.9526495337486267\n",
            "Training batch number: 55 : loss --> 3.310152053833008\n",
            "Training batch number: 56 : loss --> 14.789077758789062\n",
            "Training batch number: 57 : loss --> 0.9013537764549255\n",
            "Training batch number: 58 : loss --> 2.49923038482666\n",
            "Training batch number: 59 : loss --> 4.0719709396362305\n",
            "Training batch number: 60 : loss --> 2.2447054386138916\n",
            "Training batch number: 61 : loss --> 1.3178595304489136\n",
            "Training batch number: 62 : loss --> 4.13524055480957\n",
            "Training batch number: 63 : loss --> 3.023078203201294\n",
            "Training batch number: 64 : loss --> 2.530261278152466\n",
            "Training batch number: 65 : loss --> 12.926578521728516\n",
            "Training batch number: 66 : loss --> 7.247912406921387\n",
            "Training batch number: 67 : loss --> 8.760295867919922\n",
            "Training batch number: 68 : loss --> 13.942365646362305\n",
            "Training batch number: 69 : loss --> 8.739474296569824\n",
            "Training batch number: 70 : loss --> 9.515768051147461\n",
            "Training batch number: 71 : loss --> 2.4794399738311768\n",
            "Training batch number: 72 : loss --> 2.8766627311706543\n",
            "Training batch number: 73 : loss --> 4.489786148071289\n",
            "Training batch number: 74 : loss --> 0.44627103209495544\n",
            "Training batch number: 75 : loss --> 2.4202983379364014\n",
            "Training batch number: 76 : loss --> 2.644270181655884\n",
            "Training batch number: 77 : loss --> 5.324148178100586\n",
            "Training batch number: 78 : loss --> 4.223543167114258\n",
            "Training batch number: 79 : loss --> 2.5078043937683105\n",
            "Training batch number: 80 : loss --> 1.3747711181640625\n",
            "Training batch number: 81 : loss --> 4.689533710479736\n",
            "Training batch number: 82 : loss --> 5.247972011566162\n",
            "Training batch number: 83 : loss --> 2.1779654026031494\n",
            "Training batch number: 84 : loss --> 6.33011531829834\n",
            "Training batch number: 85 : loss --> 2.259906530380249\n",
            "Training batch number: 86 : loss --> 0.8837223052978516\n",
            "Training batch number: 87 : loss --> 3.7919960021972656\n",
            "Training batch number: 88 : loss --> 11.070261001586914\n",
            "Training batch number: 89 : loss --> 4.046234607696533\n",
            "Training batch number: 90 : loss --> 4.569512367248535\n",
            "Training batch number: 91 : loss --> 0.8929488658905029\n",
            "Training batch number: 92 : loss --> 4.6189866065979\n",
            "Training batch number: 93 : loss --> 0.5257294178009033\n",
            "Training batch number: 94 : loss --> 3.8092362880706787\n",
            "Training batch number: 95 : loss --> 7.172455310821533\n",
            "Training batch number: 96 : loss --> 2.6146373748779297\n",
            "Training batch number: 97 : loss --> 1.603408694267273\n",
            "Training batch number: 98 : loss --> 2.655857563018799\n",
            "Training batch number: 99 : loss --> 7.06676721572876\n",
            "Training batch number: 100 : loss --> 1.1167049407958984\n",
            "Training batch number: 101 : loss --> 2.6349740028381348\n",
            "Training batch number: 102 : loss --> 4.895964622497559\n",
            "Training batch number: 103 : loss --> 3.218639850616455\n",
            "Training batch number: 104 : loss --> 1.604980707168579\n",
            "Training batch number: 105 : loss --> 6.535771369934082\n",
            "Training batch number: 106 : loss --> 7.834692478179932\n",
            "Training batch number: 107 : loss --> 0.8934218287467957\n",
            "Training batch number: 108 : loss --> 2.2227680683135986\n",
            "Training batch number: 109 : loss --> 2.8210182189941406\n",
            "Training batch number: 110 : loss --> 4.698137283325195\n",
            "Training batch number: 111 : loss --> 0.6067323684692383\n",
            "Training batch number: 112 : loss --> 4.522706031799316\n",
            "Training batch number: 113 : loss --> 10.156023979187012\n",
            "Training batch number: 114 : loss --> 7.483916759490967\n",
            "Training batch number: 115 : loss --> 3.1150999069213867\n",
            "Training batch number: 116 : loss --> 7.206543445587158\n",
            "Training batch number: 117 : loss --> 7.225979328155518\n",
            "Training batch number: 118 : loss --> 7.364281177520752\n",
            "Training batch number: 119 : loss --> 5.732001304626465\n",
            "Training batch number: 120 : loss --> 6.817709445953369\n",
            "Training batch number: 121 : loss --> 3.6068854331970215\n",
            "Training batch number: 122 : loss --> 7.397557735443115\n",
            "Training batch number: 123 : loss --> 1.624773383140564\n",
            "Training batch number: 124 : loss --> 9.12376880645752\n",
            "Training batch number: 125 : loss --> 1.0671902894973755\n",
            "Training batch number: 126 : loss --> 0.8620323538780212\n",
            "Training batch number: 127 : loss --> 5.594599723815918\n",
            "Training batch number: 128 : loss --> 12.6592378616333\n",
            "Training batch number: 129 : loss --> 2.4480884075164795\n",
            "Training batch number: 130 : loss --> 0.5060346722602844\n",
            "Training batch number: 131 : loss --> 10.960801124572754\n",
            "Training batch number: 132 : loss --> 0.4122489094734192\n",
            "Training batch number: 133 : loss --> 4.866890907287598\n",
            "Training batch number: 134 : loss --> 11.683473587036133\n",
            "Training batch number: 135 : loss --> 16.11369514465332\n",
            "Training batch number: 136 : loss --> 2.1556649208068848\n",
            "Training batch number: 137 : loss --> 4.419820785522461\n",
            "Training batch number: 138 : loss --> 10.533544540405273\n",
            "Training batch number: 139 : loss --> 7.466920852661133\n",
            "Training batch number: 140 : loss --> 5.376541614532471\n",
            "Training batch number: 141 : loss --> 1.4417695999145508\n",
            "Training batch number: 142 : loss --> 0.5619668960571289\n",
            "Training batch number: 143 : loss --> 2.642967939376831\n",
            "Training batch number: 144 : loss --> 1.1604852676391602\n",
            "Training batch number: 145 : loss --> 0.45183292031288147\n",
            "Training batch number: 146 : loss --> 1.8361549377441406\n",
            "Training batch number: 147 : loss --> 6.477198123931885\n",
            "Training batch number: 148 : loss --> 5.143235206604004\n",
            "Training batch number: 149 : loss --> 7.4499688148498535\n",
            "Training batch number: 150 : loss --> 1.3604531288146973\n",
            "Training batch number: 151 : loss --> 8.5634126663208\n",
            "Training batch number: 152 : loss --> 3.31754207611084\n",
            "Training batch number: 153 : loss --> 5.520990371704102\n",
            "Training batch number: 154 : loss --> 2.678386688232422\n",
            "Training batch number: 155 : loss --> 0.3722056448459625\n",
            "Training batch number: 156 : loss --> 0.3555055558681488\n",
            "Training batch number: 157 : loss --> 4.954484939575195\n",
            "Training batch number: 158 : loss --> 2.1177473068237305\n",
            "Training batch number: 159 : loss --> 3.1194286346435547\n",
            "Training batch number: 160 : loss --> 1.1595818996429443\n",
            "Training batch number: 161 : loss --> 1.1439861059188843\n",
            "Training batch number: 162 : loss --> 2.4811527729034424\n",
            "Training batch number: 163 : loss --> 1.0359523296356201\n",
            "Training batch number: 164 : loss --> 3.666003704071045\n",
            "Training batch number: 165 : loss --> 3.921248197555542\n",
            "Training batch number: 166 : loss --> 2.6531877517700195\n",
            "Training batch number: 167 : loss --> 0.7992463707923889\n",
            "Training batch number: 168 : loss --> 7.114556789398193\n",
            "Training batch number: 169 : loss --> 3.990055799484253\n",
            "Training batch number: 170 : loss --> 4.0861592292785645\n",
            "Training batch number: 171 : loss --> 12.932747840881348\n",
            "Training batch number: 172 : loss --> 7.366558074951172\n",
            "Training batch number: 173 : loss --> 0.7447718381881714\n",
            "Training batch number: 174 : loss --> 0.3941386640071869\n",
            "Training batch number: 175 : loss --> 4.530618190765381\n",
            "Training batch number: 176 : loss --> 5.07202672958374\n",
            "Training batch number: 177 : loss --> 0.9605144262313843\n",
            "Training batch number: 178 : loss --> 1.2841711044311523\n",
            "Training batch number: 179 : loss --> 1.984472393989563\n",
            "Training batch number: 180 : loss --> 0.8526178598403931\n",
            "Training batch number: 181 : loss --> 0.24547240138053894\n",
            "Training batch number: 182 : loss --> 7.644519329071045\n",
            "Training batch number: 183 : loss --> 4.57115364074707\n",
            "Training batch number: 184 : loss --> 3.742997884750366\n",
            "Training batch number: 185 : loss --> 0.9668123126029968\n",
            "Training batch number: 186 : loss --> 13.2488431930542\n",
            "Training batch number: 187 : loss --> 7.0267333984375\n",
            "Training batch number: 188 : loss --> 3.758650064468384\n",
            "Training batch number: 189 : loss --> 0.7572746872901917\n",
            "Training batch number: 190 : loss --> 4.879960060119629\n",
            "Training batch number: 191 : loss --> 2.9385738372802734\n",
            "Training batch number: 192 : loss --> 0.2981080114841461\n",
            "Training batch number: 193 : loss --> 1.1647192239761353\n",
            "Training batch number: 194 : loss --> 3.552319049835205\n",
            "Training batch number: 195 : loss --> 7.688568592071533\n",
            "Training batch number: 196 : loss --> 1.8731735944747925\n",
            "Training batch number: 197 : loss --> 1.040019154548645\n",
            "Training batch number: 198 : loss --> 0.986615002155304\n",
            "Training batch number: 199 : loss --> 2.73111891746521\n",
            "Training batch number: 200 : loss --> 6.750840663909912\n",
            "Training batch number: 201 : loss --> 0.7702221274375916\n",
            "Training batch number: 202 : loss --> 1.8495243787765503\n",
            "Training batch number: 203 : loss --> 1.818703293800354\n",
            "Training batch number: 204 : loss --> 1.732845664024353\n",
            "Training batch number: 205 : loss --> 0.45806002616882324\n",
            "Training batch number: 206 : loss --> 5.706601142883301\n",
            "Training batch number: 207 : loss --> 2.880122184753418\n",
            "Training batch number: 208 : loss --> 1.0726274251937866\n",
            "Training batch number: 209 : loss --> 0.28438040614128113\n",
            "Training batch number: 210 : loss --> 0.4695659875869751\n",
            "Training batch number: 211 : loss --> 0.8773750066757202\n",
            "Training batch number: 212 : loss --> 0.5506806969642639\n",
            "Training batch number: 213 : loss --> 1.07672119140625\n",
            "Training batch number: 214 : loss --> 3.8455426692962646\n",
            "Training batch number: 215 : loss --> 1.303426742553711\n",
            "Training batch number: 216 : loss --> 0.5592905879020691\n",
            "Training batch number: 217 : loss --> 14.169867515563965\n",
            "Training batch number: 218 : loss --> 0.2483479380607605\n",
            "Training batch number: 219 : loss --> 3.4622581005096436\n",
            "Training batch number: 220 : loss --> 7.668539047241211\n",
            "Training batch number: 221 : loss --> 0.9627240896224976\n",
            "Training batch number: 222 : loss --> 3.25064754486084\n",
            "Training batch number: 223 : loss --> 10.320770263671875\n",
            "Training batch number: 224 : loss --> 8.897965431213379\n",
            "Training batch number: 225 : loss --> 0.3364282250404358\n",
            "Training batch number: 226 : loss --> 1.2557623386383057\n",
            "Training batch number: 227 : loss --> 1.2665106058120728\n",
            "Training batch number: 228 : loss --> 0.295006662607193\n",
            "Training batch number: 229 : loss --> 2.8879384994506836\n",
            "Training batch number: 230 : loss --> 8.314844131469727\n",
            "Training batch number: 231 : loss --> 1.1289674043655396\n",
            "Training batch number: 232 : loss --> 0.22685609757900238\n",
            "Training batch number: 233 : loss --> 11.89223861694336\n",
            "Training batch number: 234 : loss --> 6.869169235229492\n",
            "Training batch number: 235 : loss --> 0.8748711943626404\n",
            "Training batch number: 236 : loss --> 0.8964796662330627\n",
            "Training batch number: 237 : loss --> 0.550112783908844\n",
            "Training batch number: 238 : loss --> 7.244023323059082\n",
            "Training batch number: 239 : loss --> 3.281996965408325\n",
            "Training batch number: 240 : loss --> 9.190899848937988\n",
            "Training batch number: 241 : loss --> 7.391236782073975\n",
            "Training batch number: 242 : loss --> 4.0574631690979\n",
            "Training batch number: 243 : loss --> 2.2529945373535156\n",
            "Training batch number: 244 : loss --> 1.0505987405776978\n",
            "Training batch number: 245 : loss --> 13.863018989562988\n",
            "Training batch number: 246 : loss --> 6.892516613006592\n",
            "Training batch number: 247 : loss --> 6.3431010246276855\n",
            "Training batch number: 248 : loss --> 3.0786499977111816\n",
            "Training batch number: 249 : loss --> 0.10427241027355194\n",
            "Training batch number: 250 : loss --> 1.8324851989746094\n",
            "Training batch number: 251 : loss --> 0.7166456580162048\n",
            "Training batch number: 252 : loss --> 4.757927894592285\n",
            "Training batch number: 253 : loss --> 4.1924333572387695\n",
            "Training batch number: 254 : loss --> 4.592245578765869\n",
            "Training batch number: 255 : loss --> 0.8433149456977844\n",
            "Training batch number: 256 : loss --> 0.8380837440490723\n",
            "Training batch number: 257 : loss --> 0.5494251847267151\n",
            "Training batch number: 258 : loss --> 3.161576271057129\n",
            "Training batch number: 259 : loss --> 3.563563108444214\n",
            "Training batch number: 260 : loss --> 2.1360912322998047\n",
            "Training batch number: 261 : loss --> 11.202118873596191\n",
            "Training batch number: 262 : loss --> 5.727227210998535\n",
            "Training batch number: 263 : loss --> 0.6416550278663635\n",
            "Training batch number: 264 : loss --> 4.373656272888184\n",
            "Training batch number: 265 : loss --> 1.2933274507522583\n",
            "Training batch number: 266 : loss --> 1.4681869745254517\n",
            "Training batch number: 267 : loss --> 3.0532360076904297\n",
            "Training batch number: 268 : loss --> 0.6882290244102478\n",
            "Training batch number: 269 : loss --> 1.384183406829834\n",
            "Training batch number: 270 : loss --> 1.2232310771942139\n",
            "Training batch number: 271 : loss --> 1.311789870262146\n",
            "Training batch number: 272 : loss --> 1.4544053077697754\n",
            "Training batch number: 273 : loss --> 2.538587808609009\n",
            "Training batch number: 274 : loss --> 0.1389954537153244\n",
            "Training batch number: 275 : loss --> 4.853011131286621\n",
            "Training batch number: 276 : loss --> 9.535533905029297\n",
            "Training batch number: 277 : loss --> 0.7901875376701355\n",
            "Training batch number: 278 : loss --> 12.024160385131836\n",
            "Training batch number: 279 : loss --> 8.991352081298828\n",
            "Training batch number: 280 : loss --> 4.360033988952637\n",
            "Training batch number: 281 : loss --> 4.523626327514648\n",
            "Training batch number: 282 : loss --> 4.951996803283691\n",
            "Training batch number: 283 : loss --> 2.9283390045166016\n",
            "Training batch number: 284 : loss --> 1.5426136255264282\n",
            "Training batch number: 285 : loss --> 5.267439365386963\n",
            "Training batch number: 286 : loss --> 4.687946319580078\n",
            "Training batch number: 287 : loss --> 2.889817953109741\n",
            "Training batch number: 288 : loss --> 7.394966125488281\n",
            "Training batch number: 289 : loss --> 7.741603374481201\n",
            "Training batch number: 290 : loss --> 6.0666890144348145\n",
            "Training batch number: 291 : loss --> 6.265091419219971\n",
            "Training batch number: 292 : loss --> 0.711336612701416\n",
            "Training batch number: 293 : loss --> 1.0200879573822021\n",
            "Training batch number: 294 : loss --> 6.9271135330200195\n",
            "Training batch number: 295 : loss --> 0.5614915490150452\n",
            "Training batch number: 296 : loss --> 1.774573802947998\n",
            "Training batch number: 297 : loss --> 1.0650620460510254\n",
            "Training batch number: 298 : loss --> 0.5422768592834473\n",
            "Training batch number: 299 : loss --> 12.248099327087402\n",
            "Training batch number: 300 : loss --> 1.2192800045013428\n",
            "Training batch number: 301 : loss --> 0.1730092614889145\n",
            "Training batch number: 302 : loss --> 4.902885913848877\n",
            "Training batch number: 303 : loss --> 7.012101650238037\n",
            "Training batch number: 304 : loss --> 4.779364109039307\n",
            "Training batch number: 305 : loss --> 3.063100814819336\n",
            "Training batch number: 306 : loss --> 4.961054801940918\n",
            "Training batch number: 307 : loss --> 2.1093692779541016\n",
            "Training batch number: 308 : loss --> 7.926554203033447\n",
            "Training batch number: 309 : loss --> 9.514854431152344\n",
            "Training batch number: 310 : loss --> 10.738844871520996\n",
            "Training batch number: 311 : loss --> 16.816648483276367\n",
            "Training batch number: 312 : loss --> 1.612837314605713\n",
            "Training batch number: 313 : loss --> 0.6787803173065186\n",
            "Training batch number: 314 : loss --> 3.6264517307281494\n",
            "Training batch number: 315 : loss --> 0.5743013620376587\n",
            "Training batch number: 316 : loss --> 3.262650489807129\n",
            "Training batch number: 317 : loss --> 11.25096607208252\n",
            "Training batch number: 318 : loss --> 5.30372428894043\n",
            "Training batch number: 319 : loss --> 0.7030730247497559\n",
            "Training batch number: 320 : loss --> 2.377504348754883\n",
            "Training batch number: 321 : loss --> 6.494862079620361\n",
            "Training batch number: 322 : loss --> 0.20633095502853394\n",
            "Training batch number: 323 : loss --> 6.448756217956543\n",
            "Training batch number: 324 : loss --> 6.54494571685791\n",
            "Training batch number: 325 : loss --> 9.17532730102539\n",
            "Training batch number: 326 : loss --> 2.780750036239624\n",
            "Training batch number: 327 : loss --> 8.027506828308105\n",
            "Training batch number: 328 : loss --> 7.590917587280273\n",
            "Training batch number: 329 : loss --> 2.7192225456237793\n",
            "Training batch number: 330 : loss --> 5.2572126388549805\n",
            "Training batch number: 331 : loss --> 9.585874557495117\n",
            "Training batch number: 332 : loss --> 4.366969108581543\n",
            "Training batch number: 333 : loss --> 3.4682364463806152\n",
            "Training batch number: 334 : loss --> 8.051889419555664\n",
            "Training batch number: 335 : loss --> 7.389305114746094\n",
            "Training batch number: 336 : loss --> 0.597819983959198\n",
            "Training batch number: 337 : loss --> 7.799851417541504\n",
            "Training batch number: 338 : loss --> 7.312826156616211\n",
            "Training batch number: 339 : loss --> 1.3065558671951294\n",
            "Training batch number: 340 : loss --> 1.8091325759887695\n",
            "Training batch number: 341 : loss --> 11.4852294921875\n",
            "Training batch number: 342 : loss --> 3.30505108833313\n",
            "Training batch number: 343 : loss --> 1.3932602405548096\n",
            "Training batch number: 344 : loss --> 12.783117294311523\n",
            "Training batch number: 345 : loss --> 13.722081184387207\n",
            "Training batch number: 346 : loss --> 13.684075355529785\n",
            "Training batch number: 347 : loss --> 7.532960414886475\n",
            "Training batch number: 348 : loss --> 1.0174673795700073\n",
            "Training batch number: 349 : loss --> 1.3856828212738037\n",
            "Training batch number: 350 : loss --> 0.7147390842437744\n",
            "Training batch number: 351 : loss --> 0.3783014714717865\n",
            "Training batch number: 352 : loss --> 3.274549722671509\n",
            "Training batch number: 353 : loss --> 5.8940887451171875\n",
            "Training batch number: 354 : loss --> 2.4634814262390137\n",
            "Training batch number: 355 : loss --> 0.3335312604904175\n",
            "Training batch number: 356 : loss --> 0.714900553226471\n",
            "Training batch number: 357 : loss --> 1.270545244216919\n",
            "Training batch number: 358 : loss --> 4.976176738739014\n",
            "Training batch number: 359 : loss --> 0.6429446935653687\n",
            "Training batch number: 360 : loss --> 3.373883008956909\n",
            "Training batch number: 361 : loss --> 2.6726222038269043\n",
            "Training batch number: 362 : loss --> 6.411436080932617\n",
            "Training batch number: 363 : loss --> 1.6708866357803345\n",
            "Training batch number: 364 : loss --> 1.8479403257369995\n",
            "Training batch number: 365 : loss --> 5.815865993499756\n",
            "Training batch number: 366 : loss --> 3.6528635025024414\n",
            "Training batch number: 367 : loss --> 4.1275634765625\n",
            "Training batch number: 368 : loss --> 3.8499724864959717\n",
            "Training batch number: 369 : loss --> 2.3068106174468994\n",
            "Training batch number: 370 : loss --> 11.142005920410156\n",
            "Training batch number: 371 : loss --> 11.424251556396484\n",
            "Training batch number: 372 : loss --> 15.065580368041992\n",
            "Training batch number: 373 : loss --> 2.5507941246032715\n",
            "Training batch number: 374 : loss --> 0.38107195496559143\n",
            "Training batch number: 375 : loss --> 0.6407779455184937\n",
            "Training batch number: 376 : loss --> 8.301109313964844\n",
            "Training batch number: 377 : loss --> 0.5850824117660522\n",
            "Training batch number: 378 : loss --> 5.063358306884766\n",
            "Training batch number: 379 : loss --> 0.7832711338996887\n",
            "Training batch number: 380 : loss --> 3.5394914150238037\n",
            "Training batch number: 381 : loss --> 1.4579439163208008\n",
            "Training batch number: 382 : loss --> 8.02636432647705\n",
            "Training batch number: 383 : loss --> 0.48992919921875\n",
            "Training batch number: 384 : loss --> 17.846508026123047\n",
            "Training batch number: 385 : loss --> 5.412923336029053\n",
            "Training batch number: 386 : loss --> 2.716172695159912\n",
            "Training batch number: 387 : loss --> 2.348328113555908\n",
            "Training batch number: 388 : loss --> 0.8341282606124878\n",
            "Training batch number: 389 : loss --> 2.5524837970733643\n",
            "Training batch number: 390 : loss --> 10.753178596496582\n",
            "Training batch number: 391 : loss --> 0.9295750856399536\n",
            "Training batch number: 392 : loss --> 1.5140414237976074\n",
            "Training batch number: 393 : loss --> 0.3195692300796509\n",
            "Training batch number: 394 : loss --> 2.3651037216186523\n",
            "Training batch number: 395 : loss --> 9.785895347595215\n",
            "Training batch number: 396 : loss --> 0.8405343294143677\n",
            "Training batch number: 397 : loss --> 1.330727458000183\n",
            "Training batch number: 398 : loss --> 0.3247642517089844\n",
            "Training batch number: 399 : loss --> 0.6905213594436646\n",
            "Training batch number: 400 : loss --> 0.6971091628074646\n",
            "Training batch number: 401 : loss --> 0.7501322627067566\n",
            "Training batch number: 402 : loss --> 0.634018063545227\n",
            "Training batch number: 403 : loss --> 0.14182867109775543\n",
            "Training batch number: 404 : loss --> 2.244450330734253\n",
            "Training batch number: 405 : loss --> 5.889358997344971\n",
            "Training batch number: 406 : loss --> 1.0580171346664429\n",
            "Training batch number: 407 : loss --> 3.878255605697632\n",
            "Training batch number: 408 : loss --> 6.212738037109375\n",
            "Training batch number: 409 : loss --> 4.58059549331665\n",
            "Training batch number: 410 : loss --> 4.608917236328125\n",
            "Training batch number: 411 : loss --> 3.053335189819336\n",
            "Training batch number: 412 : loss --> 3.7555952072143555\n",
            "Training batch number: 413 : loss --> 1.0331894159317017\n",
            "Training batch number: 414 : loss --> 9.85383415222168\n",
            "Training batch number: 415 : loss --> 4.4296135902404785\n",
            "Training batch number: 416 : loss --> 3.581238269805908\n",
            "Training batch number: 417 : loss --> 1.7970062494277954\n",
            "Training batch number: 418 : loss --> 0.9533113837242126\n",
            "Training batch number: 419 : loss --> 14.763249397277832\n",
            "Training batch number: 420 : loss --> 13.157590866088867\n",
            "Training batch number: 421 : loss --> 4.882462978363037\n",
            "Training batch number: 422 : loss --> 2.371260166168213\n",
            "Training batch number: 423 : loss --> 2.117302656173706\n",
            "Training batch number: 424 : loss --> 9.050667762756348\n",
            "Training batch number: 425 : loss --> 19.24761390686035\n",
            "Training batch number: 426 : loss --> 1.7392957210540771\n",
            "Training batch number: 427 : loss --> 9.765839576721191\n",
            "Training batch number: 428 : loss --> 1.6438509225845337\n",
            "Training batch number: 429 : loss --> 0.5033280849456787\n",
            "Training batch number: 430 : loss --> 3.2385404109954834\n",
            "Training batch number: 431 : loss --> 4.762842178344727\n",
            "Training batch number: 432 : loss --> 2.3542160987854004\n",
            "Training batch number: 433 : loss --> 5.123447418212891\n",
            "Training batch number: 434 : loss --> 6.1890997886657715\n",
            "Training batch number: 435 : loss --> 2.8544704914093018\n",
            "Training batch number: 436 : loss --> 0.30329081416130066\n",
            "Training batch number: 437 : loss --> 0.207749605178833\n",
            "Training batch number: 438 : loss --> 6.473340034484863\n",
            "Training batch number: 439 : loss --> 7.049555778503418\n",
            "Training batch number: 440 : loss --> 2.1260130405426025\n",
            "Training batch number: 441 : loss --> 6.136307239532471\n",
            "Training batch number: 442 : loss --> 0.7809620499610901\n",
            "Training batch number: 443 : loss --> 5.434328556060791\n",
            "Training batch number: 444 : loss --> 6.696113109588623\n",
            "Training batch number: 445 : loss --> 4.246363639831543\n",
            "Training batch number: 446 : loss --> 0.664943516254425\n",
            "Training batch number: 447 : loss --> 3.9497945308685303\n",
            "Training batch number: 448 : loss --> 2.6254162788391113\n",
            "Training batch number: 449 : loss --> 0.853104829788208\n",
            "Training batch number: 450 : loss --> 5.878161907196045\n",
            "Training batch number: 451 : loss --> 0.6635988354682922\n",
            "Training batch number: 452 : loss --> 2.8848865032196045\n",
            "Training batch number: 453 : loss --> 0.4430079460144043\n",
            "Training batch number: 454 : loss --> 14.540266990661621\n",
            "Training batch number: 455 : loss --> 5.621396541595459\n",
            "Training batch number: 456 : loss --> 1.7433925867080688\n",
            "Training batch number: 457 : loss --> 1.7848204374313354\n",
            "Training batch number: 458 : loss --> 4.188077449798584\n",
            "Training batch number: 459 : loss --> 6.193252086639404\n",
            "Training batch number: 460 : loss --> 0.3391643762588501\n",
            "Training batch number: 461 : loss --> 6.153417587280273\n",
            "Training batch number: 462 : loss --> 11.30927562713623\n",
            "Training batch number: 463 : loss --> 0.8426952362060547\n",
            "Training batch number: 464 : loss --> 2.2697184085845947\n",
            "Training batch number: 465 : loss --> 0.23001621663570404\n",
            "Training batch number: 466 : loss --> 3.753692865371704\n",
            "Training batch number: 467 : loss --> 3.6731536388397217\n",
            "Training batch number: 468 : loss --> 1.699446201324463\n",
            "\n",
            "\n",
            "Training.. Loss: 0.0327 Acc: 0.9907\n",
            "Validating batch number: 0 : loss --> 2.44543194770813\n",
            "Validating batch number: 1 : loss --> 15.567521095275879\n",
            "Validating batch number: 2 : loss --> 6.820317268371582\n",
            "Validating batch number: 3 : loss --> 12.034295082092285\n",
            "Validating batch number: 4 : loss --> 12.681846618652344\n",
            "Validating batch number: 5 : loss --> 6.181173801422119\n",
            "Validating batch number: 6 : loss --> 7.578694820404053\n",
            "Validating batch number: 7 : loss --> 7.175588130950928\n",
            "Validating batch number: 8 : loss --> 2.5083470344543457\n",
            "Validating batch number: 9 : loss --> 2.6054511070251465\n",
            "Validating batch number: 10 : loss --> 12.710345268249512\n",
            "Validating batch number: 11 : loss --> 2.3097445964813232\n",
            "Validating batch number: 12 : loss --> 2.1000306606292725\n",
            "Validating batch number: 13 : loss --> 5.638262748718262\n",
            "Validating batch number: 14 : loss --> 3.6074159145355225\n",
            "Validating batch number: 15 : loss --> 10.850249290466309\n",
            "Validating batch number: 16 : loss --> 6.256217956542969\n",
            "Validating batch number: 17 : loss --> 9.974554061889648\n",
            "Validating batch number: 18 : loss --> 7.077493667602539\n",
            "Validating batch number: 19 : loss --> 5.486949920654297\n",
            "Validating batch number: 20 : loss --> 2.380500078201294\n",
            "Validating batch number: 21 : loss --> 11.919066429138184\n",
            "Validating batch number: 22 : loss --> 9.378220558166504\n",
            "Validating batch number: 23 : loss --> 10.477898597717285\n",
            "Validating batch number: 24 : loss --> 1.935202956199646\n",
            "Validating batch number: 25 : loss --> 4.061575889587402\n",
            "Validating batch number: 26 : loss --> 7.0994720458984375\n",
            "Validating batch number: 27 : loss --> 0.842659592628479\n",
            "Validating batch number: 28 : loss --> 6.733087539672852\n",
            "Validating batch number: 29 : loss --> 6.5679521560668945\n",
            "Validating batch number: 30 : loss --> 1.280092716217041\n",
            "Validating batch number: 31 : loss --> 1.2077956199645996\n",
            "Validating batch number: 32 : loss --> 12.746882438659668\n",
            "Validating batch number: 33 : loss --> 6.802423477172852\n",
            "Validating batch number: 34 : loss --> 7.253393650054932\n",
            "Validating batch number: 35 : loss --> 5.259875774383545\n",
            "Validating batch number: 36 : loss --> 9.989340782165527\n",
            "Validating batch number: 37 : loss --> 3.3843469619750977\n",
            "Validating batch number: 38 : loss --> 9.495439529418945\n",
            "Validating batch number: 39 : loss --> 13.517641067504883\n",
            "Validating batch number: 40 : loss --> 2.0957846641540527\n",
            "Validating batch number: 41 : loss --> 4.295072555541992\n",
            "Validating batch number: 42 : loss --> 4.143736839294434\n",
            "Validating batch number: 43 : loss --> 2.545661211013794\n",
            "Validating batch number: 44 : loss --> 1.861737608909607\n",
            "Validating batch number: 45 : loss --> 3.0605885982513428\n",
            "Validating batch number: 46 : loss --> 13.76657772064209\n",
            "Validating batch number: 47 : loss --> 3.286454439163208\n",
            "Validating batch number: 48 : loss --> 4.481378555297852\n",
            "Validating batch number: 49 : loss --> 6.856172561645508\n",
            "Validating batch number: 50 : loss --> 5.568197727203369\n",
            "Validating batch number: 51 : loss --> 1.3875356912612915\n",
            "Validating batch number: 52 : loss --> 8.147092819213867\n",
            "Validating batch number: 53 : loss --> 7.440476894378662\n",
            "Validating batch number: 54 : loss --> 0.424243688583374\n",
            "Validating batch number: 55 : loss --> 0.790391206741333\n",
            "Validating batch number: 56 : loss --> 16.886150360107422\n",
            "Validating batch number: 57 : loss --> 20.82162857055664\n",
            "Validating batch number: 58 : loss --> 4.631324291229248\n",
            "Validating batch number: 59 : loss --> 10.422564506530762\n",
            "Validating batch number: 60 : loss --> 13.123627662658691\n",
            "Validating batch number: 61 : loss --> 9.087451934814453\n",
            "Validating batch number: 62 : loss --> 2.683069944381714\n",
            "Validating batch number: 63 : loss --> 7.07746696472168\n",
            "Validating batch number: 64 : loss --> 2.7709059715270996\n",
            "Validating batch number: 65 : loss --> 1.0181502103805542\n",
            "Validating batch number: 66 : loss --> 9.112526893615723\n",
            "Validating batch number: 67 : loss --> 0.8112719058990479\n",
            "Validating batch number: 68 : loss --> 7.981687545776367\n",
            "Validating batch number: 69 : loss --> 0.7467638850212097\n",
            "Validating batch number: 70 : loss --> 2.7753069400787354\n",
            "Validating batch number: 71 : loss --> 6.312606334686279\n",
            "Validating batch number: 72 : loss --> 18.292848587036133\n",
            "Validating batch number: 73 : loss --> 3.8583171367645264\n",
            "Validating batch number: 74 : loss --> 9.332322120666504\n",
            "Validating batch number: 75 : loss --> 0.7487772703170776\n",
            "Validating batch number: 76 : loss --> 5.1590256690979\n",
            "Validating batch number: 77 : loss --> 11.033178329467773\n",
            "Validating batch number: 78 : loss --> 1.8728712797164917\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 43%|████▎     | 3/7 [01:26<01:55, 28.76s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Testing.. Loss: 0.0511 Acc: 0.9862\n",
            "\n",
            "Epoch 3/6\n",
            "----------\n",
            "Training batch number: 0 : loss --> 4.299592971801758\n",
            "Training batch number: 1 : loss --> 3.850473642349243\n",
            "Training batch number: 2 : loss --> 0.08634919673204422\n",
            "Training batch number: 3 : loss --> 1.8260523080825806\n",
            "Training batch number: 4 : loss --> 0.903410792350769\n",
            "Training batch number: 5 : loss --> 3.700014114379883\n",
            "Training batch number: 6 : loss --> 1.310125708580017\n",
            "Training batch number: 7 : loss --> 5.689643383026123\n",
            "Training batch number: 8 : loss --> 0.5255649089813232\n",
            "Training batch number: 9 : loss --> 2.13242769241333\n",
            "Training batch number: 10 : loss --> 2.6768507957458496\n",
            "Training batch number: 11 : loss --> 4.320952892303467\n",
            "Training batch number: 12 : loss --> 8.906730651855469\n",
            "Training batch number: 13 : loss --> 5.088099479675293\n",
            "Training batch number: 14 : loss --> 1.8690460920333862\n",
            "Training batch number: 15 : loss --> 5.5034098625183105\n",
            "Training batch number: 16 : loss --> 1.5200527906417847\n",
            "Training batch number: 17 : loss --> 0.5123947858810425\n",
            "Training batch number: 18 : loss --> 0.5014681220054626\n",
            "Training batch number: 19 : loss --> 2.2625372409820557\n",
            "Training batch number: 20 : loss --> 1.089113712310791\n",
            "Training batch number: 21 : loss --> 0.5567735433578491\n",
            "Training batch number: 22 : loss --> 0.176263689994812\n",
            "Training batch number: 23 : loss --> 3.2122690677642822\n",
            "Training batch number: 24 : loss --> 1.32294762134552\n",
            "Training batch number: 25 : loss --> 1.0066235065460205\n",
            "Training batch number: 26 : loss --> 1.2687747478485107\n",
            "Training batch number: 27 : loss --> 3.980517864227295\n",
            "Training batch number: 28 : loss --> 11.489581108093262\n",
            "Training batch number: 29 : loss --> 6.79623556137085\n",
            "Training batch number: 30 : loss --> 5.631059169769287\n",
            "Training batch number: 31 : loss --> 0.2660478353500366\n",
            "Training batch number: 32 : loss --> 7.64841365814209\n",
            "Training batch number: 33 : loss --> 13.979196548461914\n",
            "Training batch number: 34 : loss --> 2.6278610229492188\n",
            "Training batch number: 35 : loss --> 0.3977104723453522\n",
            "Training batch number: 36 : loss --> 0.6786862015724182\n",
            "Training batch number: 37 : loss --> 1.5245392322540283\n",
            "Training batch number: 38 : loss --> 0.36322805285453796\n",
            "Training batch number: 39 : loss --> 5.891777992248535\n",
            "Training batch number: 40 : loss --> 8.326190948486328\n",
            "Training batch number: 41 : loss --> 0.6992475986480713\n",
            "Training batch number: 42 : loss --> 0.9055803418159485\n",
            "Training batch number: 43 : loss --> 1.1475446224212646\n",
            "Training batch number: 44 : loss --> 1.5261220932006836\n",
            "Training batch number: 45 : loss --> 0.06006084010004997\n",
            "Training batch number: 46 : loss --> 2.9204905033111572\n",
            "Training batch number: 47 : loss --> 2.7902722358703613\n",
            "Training batch number: 48 : loss --> 1.2228326797485352\n",
            "Training batch number: 49 : loss --> 6.6048808097839355\n",
            "Training batch number: 50 : loss --> 0.8813450932502747\n",
            "Training batch number: 51 : loss --> 8.25024127960205\n",
            "Training batch number: 52 : loss --> 8.226616859436035\n",
            "Training batch number: 53 : loss --> 3.429511308670044\n",
            "Training batch number: 54 : loss --> 0.22897592186927795\n",
            "Training batch number: 55 : loss --> 1.2354964017868042\n",
            "Training batch number: 56 : loss --> 3.9764060974121094\n",
            "Training batch number: 57 : loss --> 2.123669385910034\n",
            "Training batch number: 58 : loss --> 0.7088605165481567\n",
            "Training batch number: 59 : loss --> 4.169895648956299\n",
            "Training batch number: 60 : loss --> 2.032803535461426\n",
            "Training batch number: 61 : loss --> 0.16789522767066956\n",
            "Training batch number: 62 : loss --> 1.1286357641220093\n",
            "Training batch number: 63 : loss --> 0.4226454496383667\n",
            "Training batch number: 64 : loss --> 2.846622943878174\n",
            "Training batch number: 65 : loss --> 3.9427225589752197\n",
            "Training batch number: 66 : loss --> 3.262108325958252\n",
            "Training batch number: 67 : loss --> 3.3275303840637207\n",
            "Training batch number: 68 : loss --> 1.64774489402771\n",
            "Training batch number: 69 : loss --> 16.246095657348633\n",
            "Training batch number: 70 : loss --> 4.502366542816162\n",
            "Training batch number: 71 : loss --> 1.4457710981369019\n",
            "Training batch number: 72 : loss --> 2.3841440677642822\n",
            "Training batch number: 73 : loss --> 5.541654586791992\n",
            "Training batch number: 74 : loss --> 0.25803157687187195\n",
            "Training batch number: 75 : loss --> 1.5857774019241333\n",
            "Training batch number: 76 : loss --> 0.05514740198850632\n",
            "Training batch number: 77 : loss --> 2.050114631652832\n",
            "Training batch number: 78 : loss --> 1.763311505317688\n",
            "Training batch number: 79 : loss --> 7.713866710662842\n",
            "Training batch number: 80 : loss --> 1.0534173250198364\n",
            "Training batch number: 81 : loss --> 0.22006580233573914\n",
            "Training batch number: 82 : loss --> 2.379948616027832\n",
            "Training batch number: 83 : loss --> 0.7571057677268982\n",
            "Training batch number: 84 : loss --> 1.3683574199676514\n",
            "Training batch number: 85 : loss --> 3.3223040103912354\n",
            "Training batch number: 86 : loss --> 0.21834206581115723\n",
            "Training batch number: 87 : loss --> 6.78374719619751\n",
            "Training batch number: 88 : loss --> 0.1708969920873642\n",
            "Training batch number: 89 : loss --> 4.613287925720215\n",
            "Training batch number: 90 : loss --> 2.052201509475708\n",
            "Training batch number: 91 : loss --> 1.3729499578475952\n",
            "Training batch number: 92 : loss --> 1.5941966772079468\n",
            "Training batch number: 93 : loss --> 2.862043857574463\n",
            "Training batch number: 94 : loss --> 5.505937099456787\n",
            "Training batch number: 95 : loss --> 8.248787879943848\n",
            "Training batch number: 96 : loss --> 7.949788570404053\n",
            "Training batch number: 97 : loss --> 1.4009031057357788\n",
            "Training batch number: 98 : loss --> 7.108715057373047\n",
            "Training batch number: 99 : loss --> 1.956516146659851\n",
            "Training batch number: 100 : loss --> 14.724045753479004\n",
            "Training batch number: 101 : loss --> 1.080682635307312\n",
            "Training batch number: 102 : loss --> 1.4638782739639282\n",
            "Training batch number: 103 : loss --> 5.348007678985596\n",
            "Training batch number: 104 : loss --> 1.158852458000183\n",
            "Training batch number: 105 : loss --> 2.3468892574310303\n",
            "Training batch number: 106 : loss --> 0.8207954168319702\n",
            "Training batch number: 107 : loss --> 4.37542200088501\n",
            "Training batch number: 108 : loss --> 0.9550263285636902\n",
            "Training batch number: 109 : loss --> 0.4709818661212921\n",
            "Training batch number: 110 : loss --> 1.2448023557662964\n",
            "Training batch number: 111 : loss --> 0.0695769339799881\n",
            "Training batch number: 112 : loss --> 0.5408315658569336\n",
            "Training batch number: 113 : loss --> 1.5853577852249146\n",
            "Training batch number: 114 : loss --> 3.7385921478271484\n",
            "Training batch number: 115 : loss --> 0.4959317445755005\n",
            "Training batch number: 116 : loss --> 1.132832407951355\n",
            "Training batch number: 117 : loss --> 1.0389454364776611\n",
            "Training batch number: 118 : loss --> 0.7360561490058899\n",
            "Training batch number: 119 : loss --> 1.6406605243682861\n",
            "Training batch number: 120 : loss --> 11.910357475280762\n",
            "Training batch number: 121 : loss --> 2.0619702339172363\n",
            "Training batch number: 122 : loss --> 0.10337421298027039\n",
            "Training batch number: 123 : loss --> 4.74202823638916\n",
            "Training batch number: 124 : loss --> 5.351382732391357\n",
            "Training batch number: 125 : loss --> 6.998646259307861\n",
            "Training batch number: 126 : loss --> 0.9120628833770752\n",
            "Training batch number: 127 : loss --> 7.993511199951172\n",
            "Training batch number: 128 : loss --> 5.1579203605651855\n",
            "Training batch number: 129 : loss --> 2.727349281311035\n",
            "Training batch number: 130 : loss --> 0.528627872467041\n",
            "Training batch number: 131 : loss --> 1.5728867053985596\n",
            "Training batch number: 132 : loss --> 0.3466438353061676\n",
            "Training batch number: 133 : loss --> 0.5641887784004211\n",
            "Training batch number: 134 : loss --> 6.537841796875\n",
            "Training batch number: 135 : loss --> 0.5570632219314575\n",
            "Training batch number: 136 : loss --> 8.592413902282715\n",
            "Training batch number: 137 : loss --> 4.963433742523193\n",
            "Training batch number: 138 : loss --> 3.8125455379486084\n",
            "Training batch number: 139 : loss --> 8.32440185546875\n",
            "Training batch number: 140 : loss --> 4.666135311126709\n",
            "Training batch number: 141 : loss --> 1.1934986114501953\n",
            "Training batch number: 142 : loss --> 3.8241236209869385\n",
            "Training batch number: 143 : loss --> 10.197270393371582\n",
            "Training batch number: 144 : loss --> 2.2148635387420654\n",
            "Training batch number: 145 : loss --> 2.0216174125671387\n",
            "Training batch number: 146 : loss --> 0.7631386518478394\n",
            "Training batch number: 147 : loss --> 0.21571916341781616\n",
            "Training batch number: 148 : loss --> 2.7700517177581787\n",
            "Training batch number: 149 : loss --> 3.070422410964966\n",
            "Training batch number: 150 : loss --> 7.2313008308410645\n",
            "Training batch number: 151 : loss --> 0.777705192565918\n",
            "Training batch number: 152 : loss --> 3.6668717861175537\n",
            "Training batch number: 153 : loss --> 6.654481887817383\n",
            "Training batch number: 154 : loss --> 5.192019939422607\n",
            "Training batch number: 155 : loss --> 0.170011967420578\n",
            "Training batch number: 156 : loss --> 1.1082847118377686\n",
            "Training batch number: 157 : loss --> 2.4797613620758057\n",
            "Training batch number: 158 : loss --> 0.19790737330913544\n",
            "Training batch number: 159 : loss --> 1.4281611442565918\n",
            "Training batch number: 160 : loss --> 2.7733452320098877\n",
            "Training batch number: 161 : loss --> 0.7972574830055237\n",
            "Training batch number: 162 : loss --> 6.392806053161621\n",
            "Training batch number: 163 : loss --> 6.862649440765381\n",
            "Training batch number: 164 : loss --> 5.190464973449707\n",
            "Training batch number: 165 : loss --> 7.986139297485352\n",
            "Training batch number: 166 : loss --> 0.3850589692592621\n",
            "Training batch number: 167 : loss --> 2.660660982131958\n",
            "Training batch number: 168 : loss --> 3.1994736194610596\n",
            "Training batch number: 169 : loss --> 6.479665756225586\n",
            "Training batch number: 170 : loss --> 6.387791156768799\n",
            "Training batch number: 171 : loss --> 1.8408920764923096\n",
            "Training batch number: 172 : loss --> 0.32728004455566406\n",
            "Training batch number: 173 : loss --> 1.785796880722046\n",
            "Training batch number: 174 : loss --> 0.8423981070518494\n",
            "Training batch number: 175 : loss --> 0.854033350944519\n",
            "Training batch number: 176 : loss --> 0.5258049964904785\n",
            "Training batch number: 177 : loss --> 4.570034503936768\n",
            "Training batch number: 178 : loss --> 1.7314648628234863\n",
            "Training batch number: 179 : loss --> 0.3897453844547272\n",
            "Training batch number: 180 : loss --> 7.774657726287842\n",
            "Training batch number: 181 : loss --> 1.2587486505508423\n",
            "Training batch number: 182 : loss --> 0.175546795129776\n",
            "Training batch number: 183 : loss --> 3.5727639198303223\n",
            "Training batch number: 184 : loss --> 6.471078872680664\n",
            "Training batch number: 185 : loss --> 1.6252145767211914\n",
            "Training batch number: 186 : loss --> 9.314850807189941\n",
            "Training batch number: 187 : loss --> 0.3974035084247589\n",
            "Training batch number: 188 : loss --> 2.433993101119995\n",
            "Training batch number: 189 : loss --> 1.504859447479248\n",
            "Training batch number: 190 : loss --> 0.09621036052703857\n",
            "Training batch number: 191 : loss --> 0.8832095861434937\n",
            "Training batch number: 192 : loss --> 0.221866175532341\n",
            "Training batch number: 193 : loss --> 1.244512677192688\n",
            "Training batch number: 194 : loss --> 11.193361282348633\n",
            "Training batch number: 195 : loss --> 1.0794618129730225\n",
            "Training batch number: 196 : loss --> 0.5286133885383606\n",
            "Training batch number: 197 : loss --> 5.579995155334473\n",
            "Training batch number: 198 : loss --> 1.8068242073059082\n",
            "Training batch number: 199 : loss --> 2.810697555541992\n",
            "Training batch number: 200 : loss --> 2.8392796516418457\n",
            "Training batch number: 201 : loss --> 0.7256457209587097\n",
            "Training batch number: 202 : loss --> 1.252455234527588\n",
            "Training batch number: 203 : loss --> 6.637896537780762\n",
            "Training batch number: 204 : loss --> 11.424606323242188\n",
            "Training batch number: 205 : loss --> 9.31998062133789\n",
            "Training batch number: 206 : loss --> 4.556966304779053\n",
            "Training batch number: 207 : loss --> 8.12807846069336\n",
            "Training batch number: 208 : loss --> 0.958404541015625\n",
            "Training batch number: 209 : loss --> 4.392187118530273\n",
            "Training batch number: 210 : loss --> 2.6336758136749268\n",
            "Training batch number: 211 : loss --> 1.2289496660232544\n",
            "Training batch number: 212 : loss --> 9.279818534851074\n",
            "Training batch number: 213 : loss --> 6.449044227600098\n",
            "Training batch number: 214 : loss --> 3.6144676208496094\n",
            "Training batch number: 215 : loss --> 2.652219772338867\n",
            "Training batch number: 216 : loss --> 1.504046082496643\n",
            "Training batch number: 217 : loss --> 0.8994652032852173\n",
            "Training batch number: 218 : loss --> 0.5614946484565735\n",
            "Training batch number: 219 : loss --> 2.3041467666625977\n",
            "Training batch number: 220 : loss --> 0.1800805926322937\n",
            "Training batch number: 221 : loss --> 1.0107407569885254\n",
            "Training batch number: 222 : loss --> 4.202855587005615\n",
            "Training batch number: 223 : loss --> 0.2609710097312927\n",
            "Training batch number: 224 : loss --> 0.37642619013786316\n",
            "Training batch number: 225 : loss --> 3.0807242393493652\n",
            "Training batch number: 226 : loss --> 0.6266008019447327\n",
            "Training batch number: 227 : loss --> 4.284688949584961\n",
            "Training batch number: 228 : loss --> 7.411044120788574\n",
            "Training batch number: 229 : loss --> 9.30168342590332\n",
            "Training batch number: 230 : loss --> 4.61401891708374\n",
            "Training batch number: 231 : loss --> 0.6304053664207458\n",
            "Training batch number: 232 : loss --> 2.7100846767425537\n",
            "Training batch number: 233 : loss --> 1.3132988214492798\n",
            "Training batch number: 234 : loss --> 1.5276846885681152\n",
            "Training batch number: 235 : loss --> 0.13660970330238342\n",
            "Training batch number: 236 : loss --> 0.7046305537223816\n",
            "Training batch number: 237 : loss --> 2.3341526985168457\n",
            "Training batch number: 238 : loss --> 0.19978855550289154\n",
            "Training batch number: 239 : loss --> 7.129560470581055\n",
            "Training batch number: 240 : loss --> 0.0745169147849083\n",
            "Training batch number: 241 : loss --> 9.822773933410645\n",
            "Training batch number: 242 : loss --> 0.41382119059562683\n",
            "Training batch number: 243 : loss --> 6.967747688293457\n",
            "Training batch number: 244 : loss --> 2.7481002807617188\n",
            "Training batch number: 245 : loss --> 1.991443157196045\n",
            "Training batch number: 246 : loss --> 2.3575069904327393\n",
            "Training batch number: 247 : loss --> 8.89234733581543\n",
            "Training batch number: 248 : loss --> 3.3842554092407227\n",
            "Training batch number: 249 : loss --> 10.456552505493164\n",
            "Training batch number: 250 : loss --> 3.1887078285217285\n",
            "Training batch number: 251 : loss --> 1.0533998012542725\n",
            "Training batch number: 252 : loss --> 0.6461687684059143\n",
            "Training batch number: 253 : loss --> 0.9254484176635742\n",
            "Training batch number: 254 : loss --> 0.5482933521270752\n",
            "Training batch number: 255 : loss --> 1.354813814163208\n",
            "Training batch number: 256 : loss --> 2.200592041015625\n",
            "Training batch number: 257 : loss --> 6.850022792816162\n",
            "Training batch number: 258 : loss --> 3.4733026027679443\n",
            "Training batch number: 259 : loss --> 0.99118971824646\n",
            "Training batch number: 260 : loss --> 2.085219383239746\n",
            "Training batch number: 261 : loss --> 2.1523118019104004\n",
            "Training batch number: 262 : loss --> 0.15310800075531006\n",
            "Training batch number: 263 : loss --> 0.13357339799404144\n",
            "Training batch number: 264 : loss --> 10.9144868850708\n",
            "Training batch number: 265 : loss --> 0.40116554498672485\n",
            "Training batch number: 266 : loss --> 15.022466659545898\n",
            "Training batch number: 267 : loss --> 1.7537254095077515\n",
            "Training batch number: 268 : loss --> 0.7496441006660461\n",
            "Training batch number: 269 : loss --> 0.49402883648872375\n",
            "Training batch number: 270 : loss --> 5.28328800201416\n",
            "Training batch number: 271 : loss --> 3.1767866611480713\n",
            "Training batch number: 272 : loss --> 0.4046461582183838\n",
            "Training batch number: 273 : loss --> 12.19037914276123\n",
            "Training batch number: 274 : loss --> 1.5918253660202026\n",
            "Training batch number: 275 : loss --> 1.533064842224121\n",
            "Training batch number: 276 : loss --> 3.1172008514404297\n",
            "Training batch number: 277 : loss --> 6.333371162414551\n",
            "Training batch number: 278 : loss --> 0.5100055932998657\n",
            "Training batch number: 279 : loss --> 0.3625357747077942\n",
            "Training batch number: 280 : loss --> 0.6013690829277039\n",
            "Training batch number: 281 : loss --> 0.9056661128997803\n",
            "Training batch number: 282 : loss --> 4.881282806396484\n",
            "Training batch number: 283 : loss --> 7.019817352294922\n",
            "Training batch number: 284 : loss --> 1.4481512308120728\n",
            "Training batch number: 285 : loss --> 0.49308276176452637\n",
            "Training batch number: 286 : loss --> 0.2274293601512909\n",
            "Training batch number: 287 : loss --> 2.108832359313965\n",
            "Training batch number: 288 : loss --> 0.2593648433685303\n",
            "Training batch number: 289 : loss --> 3.1848130226135254\n",
            "Training batch number: 290 : loss --> 4.907867908477783\n",
            "Training batch number: 291 : loss --> 1.535057783126831\n",
            "Training batch number: 292 : loss --> 0.3481636047363281\n",
            "Training batch number: 293 : loss --> 5.465899467468262\n",
            "Training batch number: 294 : loss --> 2.5285911560058594\n",
            "Training batch number: 295 : loss --> 2.4617905616760254\n",
            "Training batch number: 296 : loss --> 4.6648077964782715\n",
            "Training batch number: 297 : loss --> 0.9572786688804626\n",
            "Training batch number: 298 : loss --> 3.37386417388916\n",
            "Training batch number: 299 : loss --> 1.5516510009765625\n",
            "Training batch number: 300 : loss --> 0.0890474021434784\n",
            "Training batch number: 301 : loss --> 0.7667878270149231\n",
            "Training batch number: 302 : loss --> 4.417702674865723\n",
            "Training batch number: 303 : loss --> 0.31264352798461914\n",
            "Training batch number: 304 : loss --> 3.4031362533569336\n",
            "Training batch number: 305 : loss --> 0.5766855478286743\n",
            "Training batch number: 306 : loss --> 9.756345748901367\n",
            "Training batch number: 307 : loss --> 0.1281585842370987\n",
            "Training batch number: 308 : loss --> 0.6867433190345764\n",
            "Training batch number: 309 : loss --> 3.104586601257324\n",
            "Training batch number: 310 : loss --> 7.2873382568359375\n",
            "Training batch number: 311 : loss --> 4.785928726196289\n",
            "Training batch number: 312 : loss --> 4.304403781890869\n",
            "Training batch number: 313 : loss --> 0.7928127646446228\n",
            "Training batch number: 314 : loss --> 10.100604057312012\n",
            "Training batch number: 315 : loss --> 0.999937891960144\n",
            "Training batch number: 316 : loss --> 12.490736961364746\n",
            "Training batch number: 317 : loss --> 3.7351322174072266\n",
            "Training batch number: 318 : loss --> 0.40790778398513794\n",
            "Training batch number: 319 : loss --> 3.06595516204834\n",
            "Training batch number: 320 : loss --> 4.309226036071777\n",
            "Training batch number: 321 : loss --> 5.020235061645508\n",
            "Training batch number: 322 : loss --> 2.294428586959839\n",
            "Training batch number: 323 : loss --> 5.9197492599487305\n",
            "Training batch number: 324 : loss --> 2.1489832401275635\n",
            "Training batch number: 325 : loss --> 2.2645955085754395\n",
            "Training batch number: 326 : loss --> 8.97024917602539\n",
            "Training batch number: 327 : loss --> 0.7749713659286499\n",
            "Training batch number: 328 : loss --> 0.2185574471950531\n",
            "Training batch number: 329 : loss --> 8.842576026916504\n",
            "Training batch number: 330 : loss --> 0.6305966973304749\n",
            "Training batch number: 331 : loss --> 4.801408290863037\n",
            "Training batch number: 332 : loss --> 5.011136054992676\n",
            "Training batch number: 333 : loss --> 3.4113903045654297\n",
            "Training batch number: 334 : loss --> 4.655738353729248\n",
            "Training batch number: 335 : loss --> 14.586506843566895\n",
            "Training batch number: 336 : loss --> 2.2775158882141113\n",
            "Training batch number: 337 : loss --> 2.5867228507995605\n",
            "Training batch number: 338 : loss --> 5.182314872741699\n",
            "Training batch number: 339 : loss --> 13.63289737701416\n",
            "Training batch number: 340 : loss --> 7.817623615264893\n",
            "Training batch number: 341 : loss --> 0.6148257255554199\n",
            "Training batch number: 342 : loss --> 3.616166830062866\n",
            "Training batch number: 343 : loss --> 0.3600320518016815\n",
            "Training batch number: 344 : loss --> 2.231494426727295\n",
            "Training batch number: 345 : loss --> 12.95401668548584\n",
            "Training batch number: 346 : loss --> 0.22778810560703278\n",
            "Training batch number: 347 : loss --> 3.2621045112609863\n",
            "Training batch number: 348 : loss --> 1.3797582387924194\n",
            "Training batch number: 349 : loss --> 5.742575168609619\n",
            "Training batch number: 350 : loss --> 1.1712218523025513\n",
            "Training batch number: 351 : loss --> 1.2856659889221191\n",
            "Training batch number: 352 : loss --> 1.7260831594467163\n",
            "Training batch number: 353 : loss --> 2.300999164581299\n",
            "Training batch number: 354 : loss --> 0.10433051735162735\n",
            "Training batch number: 355 : loss --> 0.6746131181716919\n",
            "Training batch number: 356 : loss --> 0.17976631224155426\n",
            "Training batch number: 357 : loss --> 8.206729888916016\n",
            "Training batch number: 358 : loss --> 2.7413229942321777\n",
            "Training batch number: 359 : loss --> 4.335434913635254\n",
            "Training batch number: 360 : loss --> 9.054975509643555\n",
            "Training batch number: 361 : loss --> 1.2356910705566406\n",
            "Training batch number: 362 : loss --> 11.999469757080078\n",
            "Training batch number: 363 : loss --> 0.6310197710990906\n",
            "Training batch number: 364 : loss --> 1.1209758520126343\n",
            "Training batch number: 365 : loss --> 0.870848536491394\n",
            "Training batch number: 366 : loss --> 5.9078898429870605\n",
            "Training batch number: 367 : loss --> 1.881134271621704\n",
            "Training batch number: 368 : loss --> 8.039709091186523\n",
            "Training batch number: 369 : loss --> 5.410628795623779\n",
            "Training batch number: 370 : loss --> 2.928569793701172\n",
            "Training batch number: 371 : loss --> 0.18157631158828735\n",
            "Training batch number: 372 : loss --> 0.1108248233795166\n",
            "Training batch number: 373 : loss --> 5.9621758460998535\n",
            "Training batch number: 374 : loss --> 0.5082265138626099\n",
            "Training batch number: 375 : loss --> 6.432363986968994\n",
            "Training batch number: 376 : loss --> 3.2682673931121826\n",
            "Training batch number: 377 : loss --> 2.601491928100586\n",
            "Training batch number: 378 : loss --> 5.629332542419434\n",
            "Training batch number: 379 : loss --> 2.992173194885254\n",
            "Training batch number: 380 : loss --> 10.03297233581543\n",
            "Training batch number: 381 : loss --> 2.3638181686401367\n",
            "Training batch number: 382 : loss --> 3.6432621479034424\n",
            "Training batch number: 383 : loss --> 6.082213878631592\n",
            "Training batch number: 384 : loss --> 2.4605138301849365\n",
            "Training batch number: 385 : loss --> 1.530067801475525\n",
            "Training batch number: 386 : loss --> 5.5209503173828125\n",
            "Training batch number: 387 : loss --> 1.7246185541152954\n",
            "Training batch number: 388 : loss --> 0.9907141327857971\n",
            "Training batch number: 389 : loss --> 0.5337984561920166\n",
            "Training batch number: 390 : loss --> 0.2693832814693451\n",
            "Training batch number: 391 : loss --> 0.4306189715862274\n",
            "Training batch number: 392 : loss --> 3.810572385787964\n",
            "Training batch number: 393 : loss --> 0.5494186878204346\n",
            "Training batch number: 394 : loss --> 1.0021835565567017\n",
            "Training batch number: 395 : loss --> 0.8584122657775879\n",
            "Training batch number: 396 : loss --> 0.8234531283378601\n",
            "Training batch number: 397 : loss --> 0.5870586037635803\n",
            "Training batch number: 398 : loss --> 0.7855955362319946\n",
            "Training batch number: 399 : loss --> 7.534791946411133\n",
            "Training batch number: 400 : loss --> 10.559743881225586\n",
            "Training batch number: 401 : loss --> 1.6596770286560059\n",
            "Training batch number: 402 : loss --> 0.7456420660018921\n",
            "Training batch number: 403 : loss --> 11.1939115524292\n",
            "Training batch number: 404 : loss --> 5.0310959815979\n",
            "Training batch number: 405 : loss --> 0.6364692449569702\n",
            "Training batch number: 406 : loss --> 2.2870991230010986\n",
            "Training batch number: 407 : loss --> 0.8783917427062988\n",
            "Training batch number: 408 : loss --> 12.436220169067383\n",
            "Training batch number: 409 : loss --> 1.145919919013977\n",
            "Training batch number: 410 : loss --> 1.9924674034118652\n",
            "Training batch number: 411 : loss --> 3.639683961868286\n",
            "Training batch number: 412 : loss --> 1.661432147026062\n",
            "Training batch number: 413 : loss --> 4.188262462615967\n",
            "Training batch number: 414 : loss --> 0.3562990725040436\n",
            "Training batch number: 415 : loss --> 0.3533479869365692\n",
            "Training batch number: 416 : loss --> 2.577021598815918\n",
            "Training batch number: 417 : loss --> 0.3311956524848938\n",
            "Training batch number: 418 : loss --> 0.1598275899887085\n",
            "Training batch number: 419 : loss --> 2.891437292098999\n",
            "Training batch number: 420 : loss --> 4.177399158477783\n",
            "Training batch number: 421 : loss --> 11.760780334472656\n",
            "Training batch number: 422 : loss --> 1.9276179075241089\n",
            "Training batch number: 423 : loss --> 0.42588818073272705\n",
            "Training batch number: 424 : loss --> 5.239597320556641\n",
            "Training batch number: 425 : loss --> 1.4722671508789062\n",
            "Training batch number: 426 : loss --> 2.799891948699951\n",
            "Training batch number: 427 : loss --> 0.1544826626777649\n",
            "Training batch number: 428 : loss --> 1.1739575862884521\n",
            "Training batch number: 429 : loss --> 3.452270984649658\n",
            "Training batch number: 430 : loss --> 4.649238586425781\n",
            "Training batch number: 431 : loss --> 2.649456024169922\n",
            "Training batch number: 432 : loss --> 0.5308656692504883\n",
            "Training batch number: 433 : loss --> 0.09804893285036087\n",
            "Training batch number: 434 : loss --> 2.1038246154785156\n",
            "Training batch number: 435 : loss --> 1.0602301359176636\n",
            "Training batch number: 436 : loss --> 1.6152719259262085\n",
            "Training batch number: 437 : loss --> 1.9748525619506836\n",
            "Training batch number: 438 : loss --> 3.1112754344940186\n",
            "Training batch number: 439 : loss --> 1.8135240077972412\n",
            "Training batch number: 440 : loss --> 6.658154487609863\n",
            "Training batch number: 441 : loss --> 6.481949806213379\n",
            "Training batch number: 442 : loss --> 3.765763282775879\n",
            "Training batch number: 443 : loss --> 5.703165054321289\n",
            "Training batch number: 444 : loss --> 4.1532087326049805\n",
            "Training batch number: 445 : loss --> 2.0072224140167236\n",
            "Training batch number: 446 : loss --> 0.8719411492347717\n",
            "Training batch number: 447 : loss --> 1.0596846342086792\n",
            "Training batch number: 448 : loss --> 0.3339739739894867\n",
            "Training batch number: 449 : loss --> 0.3579542636871338\n",
            "Training batch number: 450 : loss --> 0.10653769224882126\n",
            "Training batch number: 451 : loss --> 2.2777113914489746\n",
            "Training batch number: 452 : loss --> 3.131903648376465\n",
            "Training batch number: 453 : loss --> 4.2994232177734375\n",
            "Training batch number: 454 : loss --> 3.352369785308838\n",
            "Training batch number: 455 : loss --> 2.5289254188537598\n",
            "Training batch number: 456 : loss --> 4.47470235824585\n",
            "Training batch number: 457 : loss --> 1.4902443885803223\n",
            "Training batch number: 458 : loss --> 1.640714406967163\n",
            "Training batch number: 459 : loss --> 0.4079831540584564\n",
            "Training batch number: 460 : loss --> 3.097034215927124\n",
            "Training batch number: 461 : loss --> 3.820323944091797\n",
            "Training batch number: 462 : loss --> 5.970010280609131\n",
            "Training batch number: 463 : loss --> 13.037652969360352\n",
            "Training batch number: 464 : loss --> 3.007128953933716\n",
            "Training batch number: 465 : loss --> 2.6612460613250732\n",
            "Training batch number: 466 : loss --> 9.114995002746582\n",
            "Training batch number: 467 : loss --> 4.28324031829834\n",
            "Training batch number: 468 : loss --> 1.1912058591842651\n",
            "\n",
            "\n",
            "Training.. Loss: 0.0253 Acc: 0.9927\n",
            "Validating batch number: 0 : loss --> 2.35428786277771\n",
            "Validating batch number: 1 : loss --> 2.4637539386749268\n",
            "Validating batch number: 2 : loss --> 0.2125164270401001\n",
            "Validating batch number: 3 : loss --> 6.3065409660339355\n",
            "Validating batch number: 4 : loss --> 2.696031093597412\n",
            "Validating batch number: 5 : loss --> 2.373091220855713\n",
            "Validating batch number: 6 : loss --> 7.441041946411133\n",
            "Validating batch number: 7 : loss --> 5.40830659866333\n",
            "Validating batch number: 8 : loss --> 5.128515243530273\n",
            "Validating batch number: 9 : loss --> 0.4425728917121887\n",
            "Validating batch number: 10 : loss --> 1.5379362106323242\n",
            "Validating batch number: 11 : loss --> 3.5192387104034424\n",
            "Validating batch number: 12 : loss --> 0.7685041427612305\n",
            "Validating batch number: 13 : loss --> 1.3938915729522705\n",
            "Validating batch number: 14 : loss --> 11.729622840881348\n",
            "Validating batch number: 15 : loss --> 10.335166931152344\n",
            "Validating batch number: 16 : loss --> 2.0286598205566406\n",
            "Validating batch number: 17 : loss --> 4.762929916381836\n",
            "Validating batch number: 18 : loss --> 0.34250131249427795\n",
            "Validating batch number: 19 : loss --> 2.2469022274017334\n",
            "Validating batch number: 20 : loss --> 1.4392534494400024\n",
            "Validating batch number: 21 : loss --> 1.0957430601119995\n",
            "Validating batch number: 22 : loss --> 0.18698115646839142\n",
            "Validating batch number: 23 : loss --> 0.5940389633178711\n",
            "Validating batch number: 24 : loss --> 4.418280601501465\n",
            "Validating batch number: 25 : loss --> 1.0666159391403198\n",
            "Validating batch number: 26 : loss --> 3.1156532764434814\n",
            "Validating batch number: 27 : loss --> 0.3662324845790863\n",
            "Validating batch number: 28 : loss --> 5.948631286621094\n",
            "Validating batch number: 29 : loss --> 5.504526138305664\n",
            "Validating batch number: 30 : loss --> 2.8150110244750977\n",
            "Validating batch number: 31 : loss --> 1.6944020986557007\n",
            "Validating batch number: 32 : loss --> 5.548365116119385\n",
            "Validating batch number: 33 : loss --> 0.9786930084228516\n",
            "Validating batch number: 34 : loss --> 3.4989569187164307\n",
            "Validating batch number: 35 : loss --> 5.265219211578369\n",
            "Validating batch number: 36 : loss --> 1.0131127834320068\n",
            "Validating batch number: 37 : loss --> 0.9805426597595215\n",
            "Validating batch number: 38 : loss --> 5.552633285522461\n",
            "Validating batch number: 39 : loss --> 0.803543210029602\n",
            "Validating batch number: 40 : loss --> 4.5552191734313965\n",
            "Validating batch number: 41 : loss --> 1.2430999279022217\n",
            "Validating batch number: 42 : loss --> 2.382244825363159\n",
            "Validating batch number: 43 : loss --> 4.878355503082275\n",
            "Validating batch number: 44 : loss --> 0.7081518769264221\n",
            "Validating batch number: 45 : loss --> 0.4668149948120117\n",
            "Validating batch number: 46 : loss --> 5.446980953216553\n",
            "Validating batch number: 47 : loss --> 4.226265907287598\n",
            "Validating batch number: 48 : loss --> 2.308871030807495\n",
            "Validating batch number: 49 : loss --> 1.5310202836990356\n",
            "Validating batch number: 50 : loss --> 5.792591094970703\n",
            "Validating batch number: 51 : loss --> 3.8306736946105957\n",
            "Validating batch number: 52 : loss --> 4.346871376037598\n",
            "Validating batch number: 53 : loss --> 0.22000794112682343\n",
            "Validating batch number: 54 : loss --> 4.497352600097656\n",
            "Validating batch number: 55 : loss --> 1.1232072114944458\n",
            "Validating batch number: 56 : loss --> 0.9798837304115295\n",
            "Validating batch number: 57 : loss --> 4.955626010894775\n",
            "Validating batch number: 58 : loss --> 0.9910089373588562\n",
            "Validating batch number: 59 : loss --> 8.882392883300781\n",
            "Validating batch number: 60 : loss --> 3.8022022247314453\n",
            "Validating batch number: 61 : loss --> 2.393730640411377\n",
            "Validating batch number: 62 : loss --> 3.962292432785034\n",
            "Validating batch number: 63 : loss --> 6.750164985656738\n",
            "Validating batch number: 64 : loss --> 3.8794987201690674\n",
            "Validating batch number: 65 : loss --> 0.37847763299942017\n",
            "Validating batch number: 66 : loss --> 2.143869400024414\n",
            "Validating batch number: 67 : loss --> 4.445858955383301\n",
            "Validating batch number: 68 : loss --> 6.126737594604492\n",
            "Validating batch number: 69 : loss --> 4.348878383636475\n",
            "Validating batch number: 70 : loss --> 3.300037145614624\n",
            "Validating batch number: 71 : loss --> 1.038980484008789\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 57%|█████▋    | 4/7 [01:54<01:26, 28.70s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validating batch number: 72 : loss --> 1.2762668132781982\n",
            "Validating batch number: 73 : loss --> 7.747483253479004\n",
            "Validating batch number: 74 : loss --> 2.0548946857452393\n",
            "Validating batch number: 75 : loss --> 1.15261971950531\n",
            "Validating batch number: 76 : loss --> 3.6605753898620605\n",
            "Validating batch number: 77 : loss --> 1.9402215480804443\n",
            "Validating batch number: 78 : loss --> 0.011582870967686176\n",
            "Testing.. Loss: 0.0249 Acc: 0.9924\n",
            "\n",
            "Epoch 4/6\n",
            "----------\n",
            "Training batch number: 0 : loss --> 2.730466365814209\n",
            "Training batch number: 1 : loss --> 6.192716598510742\n",
            "Training batch number: 2 : loss --> 3.849557876586914\n",
            "Training batch number: 3 : loss --> 2.912607431411743\n",
            "Training batch number: 4 : loss --> 0.3933674395084381\n",
            "Training batch number: 5 : loss --> 0.5128409266471863\n",
            "Training batch number: 6 : loss --> 0.21603743731975555\n",
            "Training batch number: 7 : loss --> 3.7326815128326416\n",
            "Training batch number: 8 : loss --> 2.470991373062134\n",
            "Training batch number: 9 : loss --> 0.8307873010635376\n",
            "Training batch number: 10 : loss --> 7.093456268310547\n",
            "Training batch number: 11 : loss --> 2.8236896991729736\n",
            "Training batch number: 12 : loss --> 3.3075788021087646\n",
            "Training batch number: 13 : loss --> 6.596787452697754\n",
            "Training batch number: 14 : loss --> 0.6358975768089294\n",
            "Training batch number: 15 : loss --> 1.9998340606689453\n",
            "Training batch number: 16 : loss --> 5.572041988372803\n",
            "Training batch number: 17 : loss --> 11.386726379394531\n",
            "Training batch number: 18 : loss --> 4.067141056060791\n",
            "Training batch number: 19 : loss --> 11.63248348236084\n",
            "Training batch number: 20 : loss --> 4.763585567474365\n",
            "Training batch number: 21 : loss --> 8.558836936950684\n",
            "Training batch number: 22 : loss --> 1.9324814081192017\n",
            "Training batch number: 23 : loss --> 0.7426801323890686\n",
            "Training batch number: 24 : loss --> 5.218952655792236\n",
            "Training batch number: 25 : loss --> 0.7171054482460022\n",
            "Training batch number: 26 : loss --> 1.0075076818466187\n",
            "Training batch number: 27 : loss --> 1.3062726259231567\n",
            "Training batch number: 28 : loss --> 0.1188356801867485\n",
            "Training batch number: 29 : loss --> 4.191202163696289\n",
            "Training batch number: 30 : loss --> 0.049326978623867035\n",
            "Training batch number: 31 : loss --> 0.31294408440589905\n",
            "Training batch number: 32 : loss --> 3.678603172302246\n",
            "Training batch number: 33 : loss --> 1.0981463193893433\n",
            "Training batch number: 34 : loss --> 8.665770530700684\n",
            "Training batch number: 35 : loss --> 0.43317288160324097\n",
            "Training batch number: 36 : loss --> 3.4220080375671387\n",
            "Training batch number: 37 : loss --> 1.7291539907455444\n",
            "Training batch number: 38 : loss --> 1.8154407739639282\n",
            "Training batch number: 39 : loss --> 0.8398667573928833\n",
            "Training batch number: 40 : loss --> 0.04470932111144066\n",
            "Training batch number: 41 : loss --> 9.817954063415527\n",
            "Training batch number: 42 : loss --> 2.355015516281128\n",
            "Training batch number: 43 : loss --> 3.6303677558898926\n",
            "Training batch number: 44 : loss --> 0.9016463756561279\n",
            "Training batch number: 45 : loss --> 3.416271209716797\n",
            "Training batch number: 46 : loss --> 0.1192048192024231\n",
            "Training batch number: 47 : loss --> 1.2198333740234375\n",
            "Training batch number: 48 : loss --> 0.14601892232894897\n",
            "Training batch number: 49 : loss --> 7.8295159339904785\n",
            "Training batch number: 50 : loss --> 0.32100027799606323\n",
            "Training batch number: 51 : loss --> 1.3535873889923096\n",
            "Training batch number: 52 : loss --> 3.2647290229797363\n",
            "Training batch number: 53 : loss --> 0.5045754313468933\n",
            "Training batch number: 54 : loss --> 2.7932512760162354\n",
            "Training batch number: 55 : loss --> 0.6836562156677246\n",
            "Training batch number: 56 : loss --> 1.5382441282272339\n",
            "Training batch number: 57 : loss --> 1.3150264024734497\n",
            "Training batch number: 58 : loss --> 0.8880764842033386\n",
            "Training batch number: 59 : loss --> 0.17237837612628937\n",
            "Training batch number: 60 : loss --> 0.32239505648612976\n",
            "Training batch number: 61 : loss --> 2.9758434295654297\n",
            "Training batch number: 62 : loss --> 4.509457588195801\n",
            "Training batch number: 63 : loss --> 1.6089072227478027\n",
            "Training batch number: 64 : loss --> 0.21615225076675415\n",
            "Training batch number: 65 : loss --> 0.11821622401475906\n",
            "Training batch number: 66 : loss --> 0.16119563579559326\n",
            "Training batch number: 67 : loss --> 4.198225498199463\n",
            "Training batch number: 68 : loss --> 7.635858535766602\n",
            "Training batch number: 69 : loss --> 6.026378631591797\n",
            "Training batch number: 70 : loss --> 0.22927063703536987\n",
            "Training batch number: 71 : loss --> 2.2057557106018066\n",
            "Training batch number: 72 : loss --> 5.475464344024658\n",
            "Training batch number: 73 : loss --> 0.8947536945343018\n",
            "Training batch number: 74 : loss --> 0.30819231271743774\n",
            "Training batch number: 75 : loss --> 0.4157369136810303\n",
            "Training batch number: 76 : loss --> 0.172359898686409\n",
            "Training batch number: 77 : loss --> 12.338676452636719\n",
            "Training batch number: 78 : loss --> 8.57958698272705\n",
            "Training batch number: 79 : loss --> 0.2079080492258072\n",
            "Training batch number: 80 : loss --> 0.7467910051345825\n",
            "Training batch number: 81 : loss --> 0.8711040019989014\n",
            "Training batch number: 82 : loss --> 3.8811020851135254\n",
            "Training batch number: 83 : loss --> 0.12473782896995544\n",
            "Training batch number: 84 : loss --> 0.5783209800720215\n",
            "Training batch number: 85 : loss --> 1.3756847381591797\n",
            "Training batch number: 86 : loss --> 0.16368994116783142\n",
            "Training batch number: 87 : loss --> 1.502079725265503\n",
            "Training batch number: 88 : loss --> 1.332186222076416\n",
            "Training batch number: 89 : loss --> 0.5192038416862488\n",
            "Training batch number: 90 : loss --> 3.939443826675415\n",
            "Training batch number: 91 : loss --> 1.4728655815124512\n",
            "Training batch number: 92 : loss --> 5.787097930908203\n",
            "Training batch number: 93 : loss --> 9.681618690490723\n",
            "Training batch number: 94 : loss --> 0.21651805937290192\n",
            "Training batch number: 95 : loss --> 2.1792476177215576\n",
            "Training batch number: 96 : loss --> 0.1639770120382309\n",
            "Training batch number: 97 : loss --> 13.660317420959473\n",
            "Training batch number: 98 : loss --> 0.6671286821365356\n",
            "Training batch number: 99 : loss --> 0.30799391865730286\n",
            "Training batch number: 100 : loss --> 1.403342843055725\n",
            "Training batch number: 101 : loss --> 8.524840354919434\n",
            "Training batch number: 102 : loss --> 2.6605122089385986\n",
            "Training batch number: 103 : loss --> 2.9965689182281494\n",
            "Training batch number: 104 : loss --> 0.5188063979148865\n",
            "Training batch number: 105 : loss --> 6.611295700073242\n",
            "Training batch number: 106 : loss --> 0.25559255480766296\n",
            "Training batch number: 107 : loss --> 0.21219699084758759\n",
            "Training batch number: 108 : loss --> 1.9419995546340942\n",
            "Training batch number: 109 : loss --> 1.552579402923584\n",
            "Training batch number: 110 : loss --> 9.779040336608887\n",
            "Training batch number: 111 : loss --> 4.867181301116943\n",
            "Training batch number: 112 : loss --> 0.8680331707000732\n",
            "Training batch number: 113 : loss --> 0.907196044921875\n",
            "Training batch number: 114 : loss --> 7.453472137451172\n",
            "Training batch number: 115 : loss --> 1.5987731218338013\n",
            "Training batch number: 116 : loss --> 0.20604898035526276\n",
            "Training batch number: 117 : loss --> 2.8701305389404297\n",
            "Training batch number: 118 : loss --> 0.9820592999458313\n",
            "Training batch number: 119 : loss --> 3.74003267288208\n",
            "Training batch number: 120 : loss --> 6.995238780975342\n",
            "Training batch number: 121 : loss --> 3.632967710494995\n",
            "Training batch number: 122 : loss --> 1.0054129362106323\n",
            "Training batch number: 123 : loss --> 3.540983200073242\n",
            "Training batch number: 124 : loss --> 1.3238327503204346\n",
            "Training batch number: 125 : loss --> 1.1660284996032715\n",
            "Training batch number: 126 : loss --> 0.3908098638057709\n",
            "Training batch number: 127 : loss --> 0.736193060874939\n",
            "Training batch number: 128 : loss --> 4.53127908706665\n",
            "Training batch number: 129 : loss --> 0.6178364753723145\n",
            "Training batch number: 130 : loss --> 0.09777507930994034\n",
            "Training batch number: 131 : loss --> 0.19556020200252533\n",
            "Training batch number: 132 : loss --> 5.197279930114746\n",
            "Training batch number: 133 : loss --> 1.0333353281021118\n",
            "Training batch number: 134 : loss --> 0.2785193920135498\n",
            "Training batch number: 135 : loss --> 0.08454900234937668\n",
            "Training batch number: 136 : loss --> 0.32642436027526855\n",
            "Training batch number: 137 : loss --> 1.4881861209869385\n",
            "Training batch number: 138 : loss --> 0.23082701861858368\n",
            "Training batch number: 139 : loss --> 1.5837877988815308\n",
            "Training batch number: 140 : loss --> 0.5960262417793274\n",
            "Training batch number: 141 : loss --> 0.9156090021133423\n",
            "Training batch number: 142 : loss --> 0.7639824151992798\n",
            "Training batch number: 143 : loss --> 0.6051896810531616\n",
            "Training batch number: 144 : loss --> 0.9026522636413574\n",
            "Training batch number: 145 : loss --> 1.8471403121948242\n",
            "Training batch number: 146 : loss --> 0.1759713739156723\n",
            "Training batch number: 147 : loss --> 1.5599987506866455\n",
            "Training batch number: 148 : loss --> 5.925644397735596\n",
            "Training batch number: 149 : loss --> 0.11227583885192871\n",
            "Training batch number: 150 : loss --> 3.6378417015075684\n",
            "Training batch number: 151 : loss --> 0.79657381772995\n",
            "Training batch number: 152 : loss --> 5.239957332611084\n",
            "Training batch number: 153 : loss --> 5.667788028717041\n",
            "Training batch number: 154 : loss --> 14.175154685974121\n",
            "Training batch number: 155 : loss --> 3.1239700317382812\n",
            "Training batch number: 156 : loss --> 1.927648663520813\n",
            "Training batch number: 157 : loss --> 0.24804964661598206\n",
            "Training batch number: 158 : loss --> 9.478641510009766\n",
            "Training batch number: 159 : loss --> 3.0981502532958984\n",
            "Training batch number: 160 : loss --> 0.6657324433326721\n",
            "Training batch number: 161 : loss --> 3.7166807651519775\n",
            "Training batch number: 162 : loss --> 1.3291395902633667\n",
            "Training batch number: 163 : loss --> 3.3826498985290527\n",
            "Training batch number: 164 : loss --> 0.11915819346904755\n",
            "Training batch number: 165 : loss --> 1.3675721883773804\n",
            "Training batch number: 166 : loss --> 4.662755489349365\n",
            "Training batch number: 167 : loss --> 9.525585174560547\n",
            "Training batch number: 168 : loss --> 6.682774543762207\n",
            "Training batch number: 169 : loss --> 1.065761923789978\n",
            "Training batch number: 170 : loss --> 1.4324101209640503\n",
            "Training batch number: 171 : loss --> 2.1446731090545654\n",
            "Training batch number: 172 : loss --> 0.1719231754541397\n",
            "Training batch number: 173 : loss --> 8.391008377075195\n",
            "Training batch number: 174 : loss --> 4.299938201904297\n",
            "Training batch number: 175 : loss --> 0.71888667345047\n",
            "Training batch number: 176 : loss --> 4.29833984375\n",
            "Training batch number: 177 : loss --> 0.3380688726902008\n",
            "Training batch number: 178 : loss --> 0.35459521412849426\n",
            "Training batch number: 179 : loss --> 4.2379231452941895\n",
            "Training batch number: 180 : loss --> 5.560897350311279\n",
            "Training batch number: 181 : loss --> 5.682872295379639\n",
            "Training batch number: 182 : loss --> 5.267759323120117\n",
            "Training batch number: 183 : loss --> 0.21335995197296143\n",
            "Training batch number: 184 : loss --> 0.5162960886955261\n",
            "Training batch number: 185 : loss --> 5.747498035430908\n",
            "Training batch number: 186 : loss --> 4.0866594314575195\n",
            "Training batch number: 187 : loss --> 0.12995266914367676\n",
            "Training batch number: 188 : loss --> 1.8627272844314575\n",
            "Training batch number: 189 : loss --> 6.362697124481201\n",
            "Training batch number: 190 : loss --> 1.2878717184066772\n",
            "Training batch number: 191 : loss --> 7.919905662536621\n",
            "Training batch number: 192 : loss --> 3.3876101970672607\n",
            "Training batch number: 193 : loss --> 0.238311305642128\n",
            "Training batch number: 194 : loss --> 0.4187258780002594\n",
            "Training batch number: 195 : loss --> 3.5208003520965576\n",
            "Training batch number: 196 : loss --> 0.2697961628437042\n",
            "Training batch number: 197 : loss --> 2.9445457458496094\n",
            "Training batch number: 198 : loss --> 3.340219736099243\n",
            "Training batch number: 199 : loss --> 1.2179862260818481\n",
            "Training batch number: 200 : loss --> 3.1304643154144287\n",
            "Training batch number: 201 : loss --> 3.7202224731445312\n",
            "Training batch number: 202 : loss --> 1.4153145551681519\n",
            "Training batch number: 203 : loss --> 7.132584571838379\n",
            "Training batch number: 204 : loss --> 0.6935219764709473\n",
            "Training batch number: 205 : loss --> 2.064096689224243\n",
            "Training batch number: 206 : loss --> 5.564233303070068\n",
            "Training batch number: 207 : loss --> 0.9404940009117126\n",
            "Training batch number: 208 : loss --> 0.12741346657276154\n",
            "Training batch number: 209 : loss --> 0.3930591344833374\n",
            "Training batch number: 210 : loss --> 0.087140291929245\n",
            "Training batch number: 211 : loss --> 3.014150857925415\n",
            "Training batch number: 212 : loss --> 0.6577886343002319\n",
            "Training batch number: 213 : loss --> 0.6002473831176758\n",
            "Training batch number: 214 : loss --> 0.0533008836209774\n",
            "Training batch number: 215 : loss --> 0.17056876420974731\n",
            "Training batch number: 216 : loss --> 0.7478264570236206\n",
            "Training batch number: 217 : loss --> 0.29751521348953247\n",
            "Training batch number: 218 : loss --> 12.779191970825195\n",
            "Training batch number: 219 : loss --> 4.893805980682373\n",
            "Training batch number: 220 : loss --> 0.3065780997276306\n",
            "Training batch number: 221 : loss --> 3.2483325004577637\n",
            "Training batch number: 222 : loss --> 1.2965595722198486\n",
            "Training batch number: 223 : loss --> 0.6873630881309509\n",
            "Training batch number: 224 : loss --> 0.2822944223880768\n",
            "Training batch number: 225 : loss --> 3.30301570892334\n",
            "Training batch number: 226 : loss --> 5.782537937164307\n",
            "Training batch number: 227 : loss --> 1.006364107131958\n",
            "Training batch number: 228 : loss --> 3.5346884727478027\n",
            "Training batch number: 229 : loss --> 11.096839904785156\n",
            "Training batch number: 230 : loss --> 6.942425727844238\n",
            "Training batch number: 231 : loss --> 15.209787368774414\n",
            "Training batch number: 232 : loss --> 0.8948992490768433\n",
            "Training batch number: 233 : loss --> 5.668720722198486\n",
            "Training batch number: 234 : loss --> 2.4861340522766113\n",
            "Training batch number: 235 : loss --> 11.282061576843262\n",
            "Training batch number: 236 : loss --> 1.150528073310852\n",
            "Training batch number: 237 : loss --> 1.0337046384811401\n",
            "Training batch number: 238 : loss --> 0.22748175263404846\n",
            "Training batch number: 239 : loss --> 5.428138732910156\n",
            "Training batch number: 240 : loss --> 0.921363353729248\n",
            "Training batch number: 241 : loss --> 0.5439693331718445\n",
            "Training batch number: 242 : loss --> 7.658513069152832\n",
            "Training batch number: 243 : loss --> 1.2334482669830322\n",
            "Training batch number: 244 : loss --> 3.822017192840576\n",
            "Training batch number: 245 : loss --> 0.9313329458236694\n",
            "Training batch number: 246 : loss --> 5.408034801483154\n",
            "Training batch number: 247 : loss --> 2.7549221515655518\n",
            "Training batch number: 248 : loss --> 0.4096873700618744\n",
            "Training batch number: 249 : loss --> 0.319749653339386\n",
            "Training batch number: 250 : loss --> 0.4880768358707428\n",
            "Training batch number: 251 : loss --> 0.20389267802238464\n",
            "Training batch number: 252 : loss --> 7.8866424560546875\n",
            "Training batch number: 253 : loss --> 1.9487847089767456\n",
            "Training batch number: 254 : loss --> 3.1793270111083984\n",
            "Training batch number: 255 : loss --> 0.7253048419952393\n",
            "Training batch number: 256 : loss --> 1.4332367181777954\n",
            "Training batch number: 257 : loss --> 7.251194477081299\n",
            "Training batch number: 258 : loss --> 6.685975551605225\n",
            "Training batch number: 259 : loss --> 1.1374725103378296\n",
            "Training batch number: 260 : loss --> 0.0794597938656807\n",
            "Training batch number: 261 : loss --> 2.282780408859253\n",
            "Training batch number: 262 : loss --> 1.499955177307129\n",
            "Training batch number: 263 : loss --> 0.6484593152999878\n",
            "Training batch number: 264 : loss --> 2.7385222911834717\n",
            "Training batch number: 265 : loss --> 6.131932258605957\n",
            "Training batch number: 266 : loss --> 3.747889518737793\n",
            "Training batch number: 267 : loss --> 3.9883618354797363\n",
            "Training batch number: 268 : loss --> 3.5442817211151123\n",
            "Training batch number: 269 : loss --> 0.8579387664794922\n",
            "Training batch number: 270 : loss --> 6.466053009033203\n",
            "Training batch number: 271 : loss --> 1.0234297513961792\n",
            "Training batch number: 272 : loss --> 4.972568035125732\n",
            "Training batch number: 273 : loss --> 1.5222232341766357\n",
            "Training batch number: 274 : loss --> 8.310986518859863\n",
            "Training batch number: 275 : loss --> 0.13776211440563202\n",
            "Training batch number: 276 : loss --> 0.23628628253936768\n",
            "Training batch number: 277 : loss --> 2.722137212753296\n",
            "Training batch number: 278 : loss --> 3.511012315750122\n",
            "Training batch number: 279 : loss --> 0.3672766387462616\n",
            "Training batch number: 280 : loss --> 0.7097012400627136\n",
            "Training batch number: 281 : loss --> 0.588180661201477\n",
            "Training batch number: 282 : loss --> 2.4109573364257812\n",
            "Training batch number: 283 : loss --> 4.223318099975586\n",
            "Training batch number: 284 : loss --> 2.600996494293213\n",
            "Training batch number: 285 : loss --> 0.1347905844449997\n",
            "Training batch number: 286 : loss --> 10.213729858398438\n",
            "Training batch number: 287 : loss --> 1.342538833618164\n",
            "Training batch number: 288 : loss --> 0.8989086151123047\n",
            "Training batch number: 289 : loss --> 12.971370697021484\n",
            "Training batch number: 290 : loss --> 1.4722495079040527\n",
            "Training batch number: 291 : loss --> 5.932766914367676\n",
            "Training batch number: 292 : loss --> 3.5933823585510254\n",
            "Training batch number: 293 : loss --> 2.74534010887146\n",
            "Training batch number: 294 : loss --> 4.539630889892578\n",
            "Training batch number: 295 : loss --> 12.572042465209961\n",
            "Training batch number: 296 : loss --> 0.4051848351955414\n",
            "Training batch number: 297 : loss --> 0.5746883153915405\n",
            "Training batch number: 298 : loss --> 4.954771518707275\n",
            "Training batch number: 299 : loss --> 6.153511047363281\n",
            "Training batch number: 300 : loss --> 0.9812971353530884\n",
            "Training batch number: 301 : loss --> 5.142302989959717\n",
            "Training batch number: 302 : loss --> 1.1646528244018555\n",
            "Training batch number: 303 : loss --> 0.7149426937103271\n",
            "Training batch number: 304 : loss --> 0.8361598253250122\n",
            "Training batch number: 305 : loss --> 2.304302215576172\n",
            "Training batch number: 306 : loss --> 1.9619088172912598\n",
            "Training batch number: 307 : loss --> 0.7308377027511597\n",
            "Training batch number: 308 : loss --> 3.133596897125244\n",
            "Training batch number: 309 : loss --> 1.132999300956726\n",
            "Training batch number: 310 : loss --> 2.4247612953186035\n",
            "Training batch number: 311 : loss --> 0.7019588351249695\n",
            "Training batch number: 312 : loss --> 5.094433307647705\n",
            "Training batch number: 313 : loss --> 11.260574340820312\n",
            "Training batch number: 314 : loss --> 13.677257537841797\n",
            "Training batch number: 315 : loss --> 3.1194162368774414\n",
            "Training batch number: 316 : loss --> 0.2337130457162857\n",
            "Training batch number: 317 : loss --> 0.392505943775177\n",
            "Training batch number: 318 : loss --> 1.604879379272461\n",
            "Training batch number: 319 : loss --> 0.07733933627605438\n",
            "Training batch number: 320 : loss --> 0.6527081727981567\n",
            "Training batch number: 321 : loss --> 2.3208065032958984\n",
            "Training batch number: 322 : loss --> 3.1701271533966064\n",
            "Training batch number: 323 : loss --> 1.693947672843933\n",
            "Training batch number: 324 : loss --> 0.7258973717689514\n",
            "Training batch number: 325 : loss --> 2.187544584274292\n",
            "Training batch number: 326 : loss --> 2.2665164470672607\n",
            "Training batch number: 327 : loss --> 1.321773648262024\n",
            "Training batch number: 328 : loss --> 0.1437217742204666\n",
            "Training batch number: 329 : loss --> 4.875931262969971\n",
            "Training batch number: 330 : loss --> 3.337676763534546\n",
            "Training batch number: 331 : loss --> 0.192727193236351\n",
            "Training batch number: 332 : loss --> 5.331852436065674\n",
            "Training batch number: 333 : loss --> 0.1685047447681427\n",
            "Training batch number: 334 : loss --> 10.152607917785645\n",
            "Training batch number: 335 : loss --> 5.65108585357666\n",
            "Training batch number: 336 : loss --> 1.7384735345840454\n",
            "Training batch number: 337 : loss --> 8.785901069641113\n",
            "Training batch number: 338 : loss --> 0.2608391046524048\n",
            "Training batch number: 339 : loss --> 0.8131905198097229\n",
            "Training batch number: 340 : loss --> 0.24005082249641418\n",
            "Training batch number: 341 : loss --> 6.270369529724121\n",
            "Training batch number: 342 : loss --> 8.917424201965332\n",
            "Training batch number: 343 : loss --> 7.326832294464111\n",
            "Training batch number: 344 : loss --> 1.4772617816925049\n",
            "Training batch number: 345 : loss --> 0.6290721893310547\n",
            "Training batch number: 346 : loss --> 3.4740984439849854\n",
            "Training batch number: 347 : loss --> 0.76275235414505\n",
            "Training batch number: 348 : loss --> 0.1905660182237625\n",
            "Training batch number: 349 : loss --> 8.984135627746582\n",
            "Training batch number: 350 : loss --> 1.9730665683746338\n",
            "Training batch number: 351 : loss --> 6.486340522766113\n",
            "Training batch number: 352 : loss --> 0.3089163899421692\n",
            "Training batch number: 353 : loss --> 0.40682393312454224\n",
            "Training batch number: 354 : loss --> 2.6440534591674805\n",
            "Training batch number: 355 : loss --> 0.244062140583992\n",
            "Training batch number: 356 : loss --> 4.883993625640869\n",
            "Training batch number: 357 : loss --> 7.028615474700928\n",
            "Training batch number: 358 : loss --> 2.896075487136841\n",
            "Training batch number: 359 : loss --> 1.6443142890930176\n",
            "Training batch number: 360 : loss --> 0.4111173152923584\n",
            "Training batch number: 361 : loss --> 1.1120538711547852\n",
            "Training batch number: 362 : loss --> 0.774391233921051\n",
            "Training batch number: 363 : loss --> 1.605315089225769\n",
            "Training batch number: 364 : loss --> 1.0423192977905273\n",
            "Training batch number: 365 : loss --> 4.5289764404296875\n",
            "Training batch number: 366 : loss --> 14.633190155029297\n",
            "Training batch number: 367 : loss --> 0.8269931674003601\n",
            "Training batch number: 368 : loss --> 1.2028727531433105\n",
            "Training batch number: 369 : loss --> 4.205589771270752\n",
            "Training batch number: 370 : loss --> 3.409564971923828\n",
            "Training batch number: 371 : loss --> 0.9204397797584534\n",
            "Training batch number: 372 : loss --> 2.7844748497009277\n",
            "Training batch number: 373 : loss --> 0.7825899720191956\n",
            "Training batch number: 374 : loss --> 1.1006778478622437\n",
            "Training batch number: 375 : loss --> 0.2499786913394928\n",
            "Training batch number: 376 : loss --> 6.118186950683594\n",
            "Training batch number: 377 : loss --> 2.6355714797973633\n",
            "Training batch number: 378 : loss --> 3.3824946880340576\n",
            "Training batch number: 379 : loss --> 0.6392491459846497\n",
            "Training batch number: 380 : loss --> 1.503857970237732\n",
            "Training batch number: 381 : loss --> 2.5885143280029297\n",
            "Training batch number: 382 : loss --> 1.9358478784561157\n",
            "Training batch number: 383 : loss --> 1.3667716979980469\n",
            "Training batch number: 384 : loss --> 6.1029791831970215\n",
            "Training batch number: 385 : loss --> 1.178539752960205\n",
            "Training batch number: 386 : loss --> 0.34657034277915955\n",
            "Training batch number: 387 : loss --> 0.5150007605552673\n",
            "Training batch number: 388 : loss --> 5.179457664489746\n",
            "Training batch number: 389 : loss --> 6.560605049133301\n",
            "Training batch number: 390 : loss --> 2.8216214179992676\n",
            "Training batch number: 391 : loss --> 5.692986011505127\n",
            "Training batch number: 392 : loss --> 4.761199474334717\n",
            "Training batch number: 393 : loss --> 6.366036415100098\n",
            "Training batch number: 394 : loss --> 2.71506404876709\n",
            "Training batch number: 395 : loss --> 0.7220990061759949\n",
            "Training batch number: 396 : loss --> 0.8392259478569031\n",
            "Training batch number: 397 : loss --> 0.831631600856781\n",
            "Training batch number: 398 : loss --> 0.7500125765800476\n",
            "Training batch number: 399 : loss --> 0.8038678169250488\n",
            "Training batch number: 400 : loss --> 0.6069509387016296\n",
            "Training batch number: 401 : loss --> 10.735542297363281\n",
            "Training batch number: 402 : loss --> 3.067002058029175\n",
            "Training batch number: 403 : loss --> 3.1243515014648438\n",
            "Training batch number: 404 : loss --> 0.2685090899467468\n",
            "Training batch number: 405 : loss --> 0.08845868706703186\n",
            "Training batch number: 406 : loss --> 3.0793771743774414\n",
            "Training batch number: 407 : loss --> 4.220040321350098\n",
            "Training batch number: 408 : loss --> 2.0567078590393066\n",
            "Training batch number: 409 : loss --> 4.146312236785889\n",
            "Training batch number: 410 : loss --> 2.2943625450134277\n",
            "Training batch number: 411 : loss --> 0.6208215951919556\n",
            "Training batch number: 412 : loss --> 0.33641645312309265\n",
            "Training batch number: 413 : loss --> 0.8004612922668457\n",
            "Training batch number: 414 : loss --> 2.6426022052764893\n",
            "Training batch number: 415 : loss --> 3.7707295417785645\n",
            "Training batch number: 416 : loss --> 0.35123777389526367\n",
            "Training batch number: 417 : loss --> 6.278657913208008\n",
            "Training batch number: 418 : loss --> 1.1823183298110962\n",
            "Training batch number: 419 : loss --> 0.26530078053474426\n",
            "Training batch number: 420 : loss --> 0.4491754174232483\n",
            "Training batch number: 421 : loss --> 5.70699405670166\n",
            "Training batch number: 422 : loss --> 7.150903224945068\n",
            "Training batch number: 423 : loss --> 0.37430569529533386\n",
            "Training batch number: 424 : loss --> 1.0584406852722168\n",
            "Training batch number: 425 : loss --> 1.1205445528030396\n",
            "Training batch number: 426 : loss --> 2.3665287494659424\n",
            "Training batch number: 427 : loss --> 0.49311909079551697\n",
            "Training batch number: 428 : loss --> 0.7024487853050232\n",
            "Training batch number: 429 : loss --> 2.6261823177337646\n",
            "Training batch number: 430 : loss --> 6.1490631103515625\n",
            "Training batch number: 431 : loss --> 0.30452802777290344\n",
            "Training batch number: 432 : loss --> 1.0320628881454468\n",
            "Training batch number: 433 : loss --> 3.0510025024414062\n",
            "Training batch number: 434 : loss --> 0.31740421056747437\n",
            "Training batch number: 435 : loss --> 1.48441481590271\n",
            "Training batch number: 436 : loss --> 9.635408401489258\n",
            "Training batch number: 437 : loss --> 0.2635718584060669\n",
            "Training batch number: 438 : loss --> 1.1245206594467163\n",
            "Training batch number: 439 : loss --> 3.863767623901367\n",
            "Training batch number: 440 : loss --> 1.767553448677063\n",
            "Training batch number: 441 : loss --> 5.722105979919434\n",
            "Training batch number: 442 : loss --> 0.08423523604869843\n",
            "Training batch number: 443 : loss --> 0.45640912652015686\n",
            "Training batch number: 444 : loss --> 6.974695682525635\n",
            "Training batch number: 445 : loss --> 1.7742595672607422\n",
            "Training batch number: 446 : loss --> 3.279566764831543\n",
            "Training batch number: 447 : loss --> 3.5598807334899902\n",
            "Training batch number: 448 : loss --> 2.666353702545166\n",
            "Training batch number: 449 : loss --> 0.21043381094932556\n",
            "Training batch number: 450 : loss --> 1.0358014106750488\n",
            "Training batch number: 451 : loss --> 0.25143691897392273\n",
            "Training batch number: 452 : loss --> 2.8343446254730225\n",
            "Training batch number: 453 : loss --> 6.71408224105835\n",
            "Training batch number: 454 : loss --> 1.6519348621368408\n",
            "Training batch number: 455 : loss --> 1.3113290071487427\n",
            "Training batch number: 456 : loss --> 0.8459575176239014\n",
            "Training batch number: 457 : loss --> 0.29842355847358704\n",
            "Training batch number: 458 : loss --> 2.90134859085083\n",
            "Training batch number: 459 : loss --> 0.2260371595621109\n",
            "Training batch number: 460 : loss --> 0.844866156578064\n",
            "Training batch number: 461 : loss --> 0.8327473998069763\n",
            "Training batch number: 462 : loss --> 0.7733139991760254\n",
            "Training batch number: 463 : loss --> 3.673675060272217\n",
            "Training batch number: 464 : loss --> 1.5635852813720703\n",
            "Training batch number: 465 : loss --> 0.6218016743659973\n",
            "Training batch number: 466 : loss --> 14.3453369140625\n",
            "Training batch number: 467 : loss --> 0.15430612862110138\n",
            "Training batch number: 468 : loss --> 2.4872437119483948\n",
            "\n",
            "\n",
            "Training.. Loss: 0.0221 Acc: 0.9935\n",
            "Validating batch number: 0 : loss --> 0.7069316506385803\n",
            "Validating batch number: 1 : loss --> 0.42036229372024536\n",
            "Validating batch number: 2 : loss --> 1.3534997701644897\n",
            "Validating batch number: 3 : loss --> 0.7312660813331604\n",
            "Validating batch number: 4 : loss --> 1.9482903480529785\n",
            "Validating batch number: 5 : loss --> 0.817160427570343\n",
            "Validating batch number: 6 : loss --> 0.6323872208595276\n",
            "Validating batch number: 7 : loss --> 0.5662875175476074\n",
            "Validating batch number: 8 : loss --> 1.994154930114746\n",
            "Validating batch number: 9 : loss --> 0.6268178224563599\n",
            "Validating batch number: 10 : loss --> 5.360225200653076\n",
            "Validating batch number: 11 : loss --> 1.5460598468780518\n",
            "Validating batch number: 12 : loss --> 9.003114700317383\n",
            "Validating batch number: 13 : loss --> 14.831418991088867\n",
            "Validating batch number: 14 : loss --> 2.0016887187957764\n",
            "Validating batch number: 15 : loss --> 4.775661945343018\n",
            "Validating batch number: 16 : loss --> 2.407958745956421\n",
            "Validating batch number: 17 : loss --> 2.0433712005615234\n",
            "Validating batch number: 18 : loss --> 0.4627039134502411\n",
            "Validating batch number: 19 : loss --> 0.18580970168113708\n",
            "Validating batch number: 20 : loss --> 1.3825815916061401\n",
            "Validating batch number: 21 : loss --> 0.7918124794960022\n",
            "Validating batch number: 22 : loss --> 5.3248724937438965\n",
            "Validating batch number: 23 : loss --> 3.706071376800537\n",
            "Validating batch number: 24 : loss --> 0.27006784081459045\n",
            "Validating batch number: 25 : loss --> 8.312931060791016\n",
            "Validating batch number: 26 : loss --> 1.4094394445419312\n",
            "Validating batch number: 27 : loss --> 1.1140316724777222\n",
            "Validating batch number: 28 : loss --> 1.4706002473831177\n",
            "Validating batch number: 29 : loss --> 0.10467561334371567\n",
            "Validating batch number: 30 : loss --> 2.1335859298706055\n",
            "Validating batch number: 31 : loss --> 12.260933876037598\n",
            "Validating batch number: 32 : loss --> 4.647585868835449\n",
            "Validating batch number: 33 : loss --> 9.415024757385254\n",
            "Validating batch number: 34 : loss --> 3.824476480484009\n",
            "Validating batch number: 35 : loss --> 7.349332809448242\n",
            "Validating batch number: 36 : loss --> 14.392723083496094\n",
            "Validating batch number: 37 : loss --> 10.52825927734375\n",
            "Validating batch number: 38 : loss --> 2.7135708332061768\n",
            "Validating batch number: 39 : loss --> 1.5412722826004028\n",
            "Validating batch number: 40 : loss --> 2.5285234451293945\n",
            "Validating batch number: 41 : loss --> 5.356149673461914\n",
            "Validating batch number: 42 : loss --> 1.3013612031936646\n",
            "Validating batch number: 43 : loss --> 0.1285988837480545\n",
            "Validating batch number: 44 : loss --> 2.0854156017303467\n",
            "Validating batch number: 45 : loss --> 1.3874515295028687\n",
            "Validating batch number: 46 : loss --> 10.040567398071289\n",
            "Validating batch number: 47 : loss --> 6.147414207458496\n",
            "Validating batch number: 48 : loss --> 4.419983863830566\n",
            "Validating batch number: 49 : loss --> 0.22991517186164856\n",
            "Validating batch number: 50 : loss --> 2.3257014751434326\n",
            "Validating batch number: 51 : loss --> 3.47002911567688\n",
            "Validating batch number: 52 : loss --> 0.3716372549533844\n",
            "Validating batch number: 53 : loss --> 2.783794403076172\n",
            "Validating batch number: 54 : loss --> 3.8059051036834717\n",
            "Validating batch number: 55 : loss --> 0.21658501029014587\n",
            "Validating batch number: 56 : loss --> 3.4782609939575195\n",
            "Validating batch number: 57 : loss --> 0.16107892990112305\n",
            "Validating batch number: 58 : loss --> 3.394566535949707\n",
            "Validating batch number: 59 : loss --> 6.135988712310791\n",
            "Validating batch number: 60 : loss --> 2.0869734287261963\n",
            "Validating batch number: 61 : loss --> 6.083302021026611\n",
            "Validating batch number: 62 : loss --> 1.9590266942977905\n",
            "Validating batch number: 63 : loss --> 7.689604759216309\n",
            "Validating batch number: 64 : loss --> 3.0875840187072754\n",
            "Validating batch number: 65 : loss --> 0.9234287142753601\n",
            "Validating batch number: 66 : loss --> 14.375288009643555\n",
            "Validating batch number: 67 : loss --> 4.20502233505249\n",
            "Validating batch number: 68 : loss --> 0.5481622815132141\n",
            "Validating batch number: 69 : loss --> 2.273789644241333\n",
            "Validating batch number: 70 : loss --> 15.062643051147461\n",
            "Validating batch number: 71 : loss --> 2.9509730339050293\n",
            "Validating batch number: 72 : loss --> 1.084067463874817\n",
            "Validating batch number: 73 : loss --> 0.3703920543193817\n",
            "Validating batch number: 74 : loss --> 3.8014397621154785\n",
            "Validating batch number: 75 : loss --> 0.09898144006729126\n",
            "Validating batch number: 76 : loss --> 0.06826366484165192\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 71%|███████▏  | 5/7 [02:23<00:57, 28.66s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validating batch number: 77 : loss --> 1.5021696090698242\n",
            "Validating batch number: 78 : loss --> 0.06116839870810509\n",
            "Testing.. Loss: 0.0273 Acc: 0.9920\n",
            "\n",
            "Epoch 5/6\n",
            "----------\n",
            "Training batch number: 0 : loss --> 1.5795648097991943\n",
            "Training batch number: 1 : loss --> 0.06349880248308182\n",
            "Training batch number: 2 : loss --> 5.298452377319336\n",
            "Training batch number: 3 : loss --> 0.22548341751098633\n",
            "Training batch number: 4 : loss --> 1.1875203847885132\n",
            "Training batch number: 5 : loss --> 0.12815478444099426\n",
            "Training batch number: 6 : loss --> 2.8582801818847656\n",
            "Training batch number: 7 : loss --> 0.28668782114982605\n",
            "Training batch number: 8 : loss --> 0.07430735975503922\n",
            "Training batch number: 9 : loss --> 0.03806587681174278\n",
            "Training batch number: 10 : loss --> 0.046730320900678635\n",
            "Training batch number: 11 : loss --> 1.3055768013000488\n",
            "Training batch number: 12 : loss --> 3.0599610805511475\n",
            "Training batch number: 13 : loss --> 1.949658989906311\n",
            "Training batch number: 14 : loss --> 0.36847376823425293\n",
            "Training batch number: 15 : loss --> 0.47635725140571594\n",
            "Training batch number: 16 : loss --> 1.8402016162872314\n",
            "Training batch number: 17 : loss --> 12.282490730285645\n",
            "Training batch number: 18 : loss --> 4.081661701202393\n",
            "Training batch number: 19 : loss --> 2.7893152236938477\n",
            "Training batch number: 20 : loss --> 6.971347808837891\n",
            "Training batch number: 21 : loss --> 4.572186470031738\n",
            "Training batch number: 22 : loss --> 0.1912470906972885\n",
            "Training batch number: 23 : loss --> 2.0787980556488037\n",
            "Training batch number: 24 : loss --> 0.5851258635520935\n",
            "Training batch number: 25 : loss --> 0.039450086653232574\n",
            "Training batch number: 26 : loss --> 0.2797769010066986\n",
            "Training batch number: 27 : loss --> 2.238358974456787\n",
            "Training batch number: 28 : loss --> 0.4726959466934204\n",
            "Training batch number: 29 : loss --> 6.974520206451416\n",
            "Training batch number: 30 : loss --> 0.3070885241031647\n",
            "Training batch number: 31 : loss --> 1.1558928489685059\n",
            "Training batch number: 32 : loss --> 0.156140998005867\n",
            "Training batch number: 33 : loss --> 2.0397250652313232\n",
            "Training batch number: 34 : loss --> 0.63018399477005\n",
            "Training batch number: 35 : loss --> 0.32308951020240784\n",
            "Training batch number: 36 : loss --> 1.0466972589492798\n",
            "Training batch number: 37 : loss --> 0.7922566533088684\n",
            "Training batch number: 38 : loss --> 0.4104822278022766\n",
            "Training batch number: 39 : loss --> 0.1701955497264862\n",
            "Training batch number: 40 : loss --> 0.03829807788133621\n",
            "Training batch number: 41 : loss --> 1.607033133506775\n",
            "Training batch number: 42 : loss --> 7.092491149902344\n",
            "Training batch number: 43 : loss --> 4.037292957305908\n",
            "Training batch number: 44 : loss --> 0.13175836205482483\n",
            "Training batch number: 45 : loss --> 0.7264692783355713\n",
            "Training batch number: 46 : loss --> 10.479458808898926\n",
            "Training batch number: 47 : loss --> 4.827206134796143\n",
            "Training batch number: 48 : loss --> 0.15671399235725403\n",
            "Training batch number: 49 : loss --> 0.8432708382606506\n",
            "Training batch number: 50 : loss --> 0.32242780923843384\n",
            "Training batch number: 51 : loss --> 2.884580135345459\n",
            "Training batch number: 52 : loss --> 0.17619159817695618\n",
            "Training batch number: 53 : loss --> 2.1933815479278564\n",
            "Training batch number: 54 : loss --> 0.4358657896518707\n",
            "Training batch number: 55 : loss --> 0.2555496096611023\n",
            "Training batch number: 56 : loss --> 4.504768371582031\n",
            "Training batch number: 57 : loss --> 1.0793662071228027\n",
            "Training batch number: 58 : loss --> 3.62593412399292\n",
            "Training batch number: 59 : loss --> 12.719263076782227\n",
            "Training batch number: 60 : loss --> 7.363022804260254\n",
            "Training batch number: 61 : loss --> 1.64706289768219\n",
            "Training batch number: 62 : loss --> 0.3055756688117981\n",
            "Training batch number: 63 : loss --> 1.3380582332611084\n",
            "Training batch number: 64 : loss --> 0.44171130657196045\n",
            "Training batch number: 65 : loss --> 0.06091424822807312\n",
            "Training batch number: 66 : loss --> 4.262411594390869\n",
            "Training batch number: 67 : loss --> 4.731063365936279\n",
            "Training batch number: 68 : loss --> 2.522238254547119\n",
            "Training batch number: 69 : loss --> 4.86838960647583\n",
            "Training batch number: 70 : loss --> 0.8691788911819458\n",
            "Training batch number: 71 : loss --> 0.5572376847267151\n",
            "Training batch number: 72 : loss --> 0.3010011315345764\n",
            "Training batch number: 73 : loss --> 0.08381349593400955\n",
            "Training batch number: 74 : loss --> 0.16886216402053833\n",
            "Training batch number: 75 : loss --> 3.3840858936309814\n",
            "Training batch number: 76 : loss --> 0.3094296455383301\n",
            "Training batch number: 77 : loss --> 0.6047217845916748\n",
            "Training batch number: 78 : loss --> 10.883368492126465\n",
            "Training batch number: 79 : loss --> 2.0129554271698\n",
            "Training batch number: 80 : loss --> 1.889864206314087\n",
            "Training batch number: 81 : loss --> 0.8002778887748718\n",
            "Training batch number: 82 : loss --> 0.12049248814582825\n",
            "Training batch number: 83 : loss --> 0.0475454218685627\n",
            "Training batch number: 84 : loss --> 0.13015596568584442\n",
            "Training batch number: 85 : loss --> 0.3906342387199402\n",
            "Training batch number: 86 : loss --> 0.18902339041233063\n",
            "Training batch number: 87 : loss --> 1.8200185298919678\n",
            "Training batch number: 88 : loss --> 0.04329606145620346\n",
            "Training batch number: 89 : loss --> 3.013953924179077\n",
            "Training batch number: 90 : loss --> 0.045848168432712555\n",
            "Training batch number: 91 : loss --> 5.36050271987915\n",
            "Training batch number: 92 : loss --> 0.16760297119617462\n",
            "Training batch number: 93 : loss --> 0.10156062245368958\n",
            "Training batch number: 94 : loss --> 1.007015585899353\n",
            "Training batch number: 95 : loss --> 0.132259801030159\n",
            "Training batch number: 96 : loss --> 3.6233878135681152\n",
            "Training batch number: 97 : loss --> 0.11819557845592499\n",
            "Training batch number: 98 : loss --> 0.11684133112430573\n",
            "Training batch number: 99 : loss --> 16.154796600341797\n",
            "Training batch number: 100 : loss --> 4.868276596069336\n",
            "Training batch number: 101 : loss --> 0.19096969068050385\n",
            "Training batch number: 102 : loss --> 0.15172594785690308\n",
            "Training batch number: 103 : loss --> 0.35143181681632996\n",
            "Training batch number: 104 : loss --> 0.9180065989494324\n",
            "Training batch number: 105 : loss --> 3.0233187675476074\n",
            "Training batch number: 106 : loss --> 0.2610167860984802\n",
            "Training batch number: 107 : loss --> 0.44923773407936096\n",
            "Training batch number: 108 : loss --> 1.1597427129745483\n",
            "Training batch number: 109 : loss --> 0.5214610695838928\n",
            "Training batch number: 110 : loss --> 2.221327066421509\n",
            "Training batch number: 111 : loss --> 0.28287309408187866\n",
            "Training batch number: 112 : loss --> 0.0423453263938427\n",
            "Training batch number: 113 : loss --> 0.5855132937431335\n",
            "Training batch number: 114 : loss --> 2.788078546524048\n",
            "Training batch number: 115 : loss --> 0.21073757112026215\n",
            "Training batch number: 116 : loss --> 2.6351122856140137\n",
            "Training batch number: 117 : loss --> 0.21167781949043274\n",
            "Training batch number: 118 : loss --> 0.5132914185523987\n",
            "Training batch number: 119 : loss --> 5.214089870452881\n",
            "Training batch number: 120 : loss --> 0.2715280055999756\n",
            "Training batch number: 121 : loss --> 1.4104557037353516\n",
            "Training batch number: 122 : loss --> 0.1455995738506317\n",
            "Training batch number: 123 : loss --> 0.20387384295463562\n",
            "Training batch number: 124 : loss --> 4.091005325317383\n",
            "Training batch number: 125 : loss --> 1.4228100776672363\n",
            "Training batch number: 126 : loss --> 0.23045265674591064\n",
            "Training batch number: 127 : loss --> 0.7687450051307678\n",
            "Training batch number: 128 : loss --> 0.059497278183698654\n",
            "Training batch number: 129 : loss --> 9.14391040802002\n",
            "Training batch number: 130 : loss --> 0.33392637968063354\n",
            "Training batch number: 131 : loss --> 0.6829993724822998\n",
            "Training batch number: 132 : loss --> 0.19906076788902283\n",
            "Training batch number: 133 : loss --> 2.1919848918914795\n",
            "Training batch number: 134 : loss --> 0.3360026478767395\n",
            "Training batch number: 135 : loss --> 0.3050919771194458\n",
            "Training batch number: 136 : loss --> 0.5808613896369934\n",
            "Training batch number: 137 : loss --> 0.24775430560112\n",
            "Training batch number: 138 : loss --> 0.47569888830184937\n",
            "Training batch number: 139 : loss --> 0.7370300889015198\n",
            "Training batch number: 140 : loss --> 1.5455347299575806\n",
            "Training batch number: 141 : loss --> 2.6530425548553467\n",
            "Training batch number: 142 : loss --> 1.815375804901123\n",
            "Training batch number: 143 : loss --> 0.3227708339691162\n",
            "Training batch number: 144 : loss --> 0.4643106162548065\n",
            "Training batch number: 145 : loss --> 2.5845818519592285\n",
            "Training batch number: 146 : loss --> 9.761122703552246\n",
            "Training batch number: 147 : loss --> 0.060625165700912476\n",
            "Training batch number: 148 : loss --> 9.967999458312988\n",
            "Training batch number: 149 : loss --> 2.4008679389953613\n",
            "Training batch number: 150 : loss --> 0.5244714617729187\n",
            "Training batch number: 151 : loss --> 2.0217578411102295\n",
            "Training batch number: 152 : loss --> 3.3539836406707764\n",
            "Training batch number: 153 : loss --> 3.7025630474090576\n",
            "Training batch number: 154 : loss --> 0.18016380071640015\n",
            "Training batch number: 155 : loss --> 8.148106575012207\n",
            "Training batch number: 156 : loss --> 0.0712270513176918\n",
            "Training batch number: 157 : loss --> 0.042540520429611206\n",
            "Training batch number: 158 : loss --> 2.7263214588165283\n",
            "Training batch number: 159 : loss --> 0.15465782582759857\n",
            "Training batch number: 160 : loss --> 3.635072708129883\n",
            "Training batch number: 161 : loss --> 0.29815229773521423\n",
            "Training batch number: 162 : loss --> 7.23071813583374\n",
            "Training batch number: 163 : loss --> 0.24342545866966248\n",
            "Training batch number: 164 : loss --> 0.6589486598968506\n",
            "Training batch number: 165 : loss --> 0.4896709620952606\n",
            "Training batch number: 166 : loss --> 5.514796257019043\n",
            "Training batch number: 167 : loss --> 6.086814880371094\n",
            "Training batch number: 168 : loss --> 1.4303525686264038\n",
            "Training batch number: 169 : loss --> 0.4156121015548706\n",
            "Training batch number: 170 : loss --> 0.8437061905860901\n",
            "Training batch number: 171 : loss --> 0.24596227705478668\n",
            "Training batch number: 172 : loss --> 0.7838042974472046\n",
            "Training batch number: 173 : loss --> 1.0910789966583252\n",
            "Training batch number: 174 : loss --> 0.5786293745040894\n",
            "Training batch number: 175 : loss --> 0.33699309825897217\n",
            "Training batch number: 176 : loss --> 1.199580192565918\n",
            "Training batch number: 177 : loss --> 0.6316806674003601\n",
            "Training batch number: 178 : loss --> 0.24763736128807068\n",
            "Training batch number: 179 : loss --> 4.492100238800049\n",
            "Training batch number: 180 : loss --> 4.0827484130859375\n",
            "Training batch number: 181 : loss --> 1.8071130514144897\n",
            "Training batch number: 182 : loss --> 2.1545825004577637\n",
            "Training batch number: 183 : loss --> 2.5856575965881348\n",
            "Training batch number: 184 : loss --> 0.2946905791759491\n",
            "Training batch number: 185 : loss --> 0.021072935312986374\n",
            "Training batch number: 186 : loss --> 1.15382981300354\n",
            "Training batch number: 187 : loss --> 0.937842071056366\n",
            "Training batch number: 188 : loss --> 0.892728328704834\n",
            "Training batch number: 189 : loss --> 3.451646327972412\n",
            "Training batch number: 190 : loss --> 3.978914976119995\n",
            "Training batch number: 191 : loss --> 0.13538187742233276\n",
            "Training batch number: 192 : loss --> 3.8900539875030518\n",
            "Training batch number: 193 : loss --> 0.84505295753479\n",
            "Training batch number: 194 : loss --> 3.117560625076294\n",
            "Training batch number: 195 : loss --> 2.192701578140259\n",
            "Training batch number: 196 : loss --> 3.886101245880127\n",
            "Training batch number: 197 : loss --> 0.6524485945701599\n",
            "Training batch number: 198 : loss --> 0.35752731561660767\n",
            "Training batch number: 199 : loss --> 4.766804218292236\n",
            "Training batch number: 200 : loss --> 6.892854690551758\n",
            "Training batch number: 201 : loss --> 0.42224499583244324\n",
            "Training batch number: 202 : loss --> 3.6267800331115723\n",
            "Training batch number: 203 : loss --> 1.6225882768630981\n",
            "Training batch number: 204 : loss --> 8.070899963378906\n",
            "Training batch number: 205 : loss --> 1.695926547050476\n",
            "Training batch number: 206 : loss --> 1.980043888092041\n",
            "Training batch number: 207 : loss --> 6.189230918884277\n",
            "Training batch number: 208 : loss --> 0.29891207814216614\n",
            "Training batch number: 209 : loss --> 4.1960368156433105\n",
            "Training batch number: 210 : loss --> 3.7042019367218018\n",
            "Training batch number: 211 : loss --> 0.07951635122299194\n",
            "Training batch number: 212 : loss --> 13.096714973449707\n",
            "Training batch number: 213 : loss --> 0.8140519261360168\n",
            "Training batch number: 214 : loss --> 1.145385980606079\n",
            "Training batch number: 215 : loss --> 0.374523401260376\n",
            "Training batch number: 216 : loss --> 0.5949609875679016\n",
            "Training batch number: 217 : loss --> 2.3630926609039307\n",
            "Training batch number: 218 : loss --> 21.692251205444336\n",
            "Training batch number: 219 : loss --> 0.35946205258369446\n",
            "Training batch number: 220 : loss --> 3.628493309020996\n",
            "Training batch number: 221 : loss --> 1.6901768445968628\n",
            "Training batch number: 222 : loss --> 2.0746402740478516\n",
            "Training batch number: 223 : loss --> 0.18996982276439667\n",
            "Training batch number: 224 : loss --> 0.8568547964096069\n",
            "Training batch number: 225 : loss --> 0.21618689596652985\n",
            "Training batch number: 226 : loss --> 2.0395987033843994\n",
            "Training batch number: 227 : loss --> 3.742393732070923\n",
            "Training batch number: 228 : loss --> 3.7277719974517822\n",
            "Training batch number: 229 : loss --> 2.200212240219116\n",
            "Training batch number: 230 : loss --> 3.0838494300842285\n",
            "Training batch number: 231 : loss --> 0.22254009544849396\n",
            "Training batch number: 232 : loss --> 0.5022484064102173\n",
            "Training batch number: 233 : loss --> 1.8948698043823242\n",
            "Training batch number: 234 : loss --> 0.27752843499183655\n",
            "Training batch number: 235 : loss --> 1.0113264322280884\n",
            "Training batch number: 236 : loss --> 2.4513044357299805\n",
            "Training batch number: 237 : loss --> 0.12991011142730713\n",
            "Training batch number: 238 : loss --> 0.7056620121002197\n",
            "Training batch number: 239 : loss --> 4.038999557495117\n",
            "Training batch number: 240 : loss --> 0.36972394585609436\n",
            "Training batch number: 241 : loss --> 1.607494831085205\n",
            "Training batch number: 242 : loss --> 5.187553882598877\n",
            "Training batch number: 243 : loss --> 4.895208835601807\n",
            "Training batch number: 244 : loss --> 0.18902918696403503\n",
            "Training batch number: 245 : loss --> 0.46490710973739624\n",
            "Training batch number: 246 : loss --> 0.35392436385154724\n",
            "Training batch number: 247 : loss --> 0.8661482334136963\n",
            "Training batch number: 248 : loss --> 0.044502660632133484\n",
            "Training batch number: 249 : loss --> 0.1544983834028244\n",
            "Training batch number: 250 : loss --> 0.25636813044548035\n",
            "Training batch number: 251 : loss --> 0.4105381369590759\n",
            "Training batch number: 252 : loss --> 0.3814518451690674\n",
            "Training batch number: 253 : loss --> 0.19490373134613037\n",
            "Training batch number: 254 : loss --> 0.13660593330860138\n",
            "Training batch number: 255 : loss --> 1.9002025127410889\n",
            "Training batch number: 256 : loss --> 0.6737222671508789\n",
            "Training batch number: 257 : loss --> 0.3722694218158722\n",
            "Training batch number: 258 : loss --> 1.735184669494629\n",
            "Training batch number: 259 : loss --> 6.7482428550720215\n",
            "Training batch number: 260 : loss --> 5.1880011558532715\n",
            "Training batch number: 261 : loss --> 2.7702691555023193\n",
            "Training batch number: 262 : loss --> 0.09901770204305649\n",
            "Training batch number: 263 : loss --> 2.923590898513794\n",
            "Training batch number: 264 : loss --> 0.2028190642595291\n",
            "Training batch number: 265 : loss --> 1.9511644840240479\n",
            "Training batch number: 266 : loss --> 0.8808748126029968\n",
            "Training batch number: 267 : loss --> 2.1957356929779053\n",
            "Training batch number: 268 : loss --> 4.610166072845459\n",
            "Training batch number: 269 : loss --> 0.24922595918178558\n",
            "Training batch number: 270 : loss --> 2.159268856048584\n",
            "Training batch number: 271 : loss --> 5.142445087432861\n",
            "Training batch number: 272 : loss --> 0.14362819492816925\n",
            "Training batch number: 273 : loss --> 0.22736573219299316\n",
            "Training batch number: 274 : loss --> 0.1847328096628189\n",
            "Training batch number: 275 : loss --> 0.9137042164802551\n",
            "Training batch number: 276 : loss --> 0.33320629596710205\n",
            "Training batch number: 277 : loss --> 0.10206694900989532\n",
            "Training batch number: 278 : loss --> 0.5674501061439514\n",
            "Training batch number: 279 : loss --> 0.6982594728469849\n",
            "Training batch number: 280 : loss --> 5.8440351486206055\n",
            "Training batch number: 281 : loss --> 0.07422280311584473\n",
            "Training batch number: 282 : loss --> 1.847163200378418\n",
            "Training batch number: 283 : loss --> 1.3204591274261475\n",
            "Training batch number: 284 : loss --> 1.1847829818725586\n",
            "Training batch number: 285 : loss --> 0.750866174697876\n",
            "Training batch number: 286 : loss --> 7.329287528991699\n",
            "Training batch number: 287 : loss --> 4.844687461853027\n",
            "Training batch number: 288 : loss --> 1.0858129262924194\n",
            "Training batch number: 289 : loss --> 3.579362392425537\n",
            "Training batch number: 290 : loss --> 0.3094753623008728\n",
            "Training batch number: 291 : loss --> 1.2887427806854248\n",
            "Training batch number: 292 : loss --> 1.092239260673523\n",
            "Training batch number: 293 : loss --> 0.7658838629722595\n",
            "Training batch number: 294 : loss --> 5.219964027404785\n",
            "Training batch number: 295 : loss --> 0.6359772086143494\n",
            "Training batch number: 296 : loss --> 0.3419353663921356\n",
            "Training batch number: 297 : loss --> 9.584867477416992\n",
            "Training batch number: 298 : loss --> 1.8476231098175049\n",
            "Training batch number: 299 : loss --> 0.08128434419631958\n",
            "Training batch number: 300 : loss --> 2.1868128776550293\n",
            "Training batch number: 301 : loss --> 4.332737445831299\n",
            "Training batch number: 302 : loss --> 0.8365845084190369\n",
            "Training batch number: 303 : loss --> 0.396989643573761\n",
            "Training batch number: 304 : loss --> 0.127388134598732\n",
            "Training batch number: 305 : loss --> 0.0422508604824543\n",
            "Training batch number: 306 : loss --> 0.39262330532073975\n",
            "Training batch number: 307 : loss --> 2.941991090774536\n",
            "Training batch number: 308 : loss --> 0.26270854473114014\n",
            "Training batch number: 309 : loss --> 5.009436130523682\n",
            "Training batch number: 310 : loss --> 0.08710841089487076\n",
            "Training batch number: 311 : loss --> 4.306055545806885\n",
            "Training batch number: 312 : loss --> 0.06404602527618408\n",
            "Training batch number: 313 : loss --> 5.163297653198242\n",
            "Training batch number: 314 : loss --> 0.23825876414775848\n",
            "Training batch number: 315 : loss --> 1.7876371145248413\n",
            "Training batch number: 316 : loss --> 2.0531888008117676\n",
            "Training batch number: 317 : loss --> 0.42195025086402893\n",
            "Training batch number: 318 : loss --> 0.27090078592300415\n",
            "Training batch number: 319 : loss --> 1.8058040142059326\n",
            "Training batch number: 320 : loss --> 3.5142860412597656\n",
            "Training batch number: 321 : loss --> 0.0940762609243393\n",
            "Training batch number: 322 : loss --> 0.9323431253433228\n",
            "Training batch number: 323 : loss --> 3.6944239139556885\n",
            "Training batch number: 324 : loss --> 0.2574444115161896\n",
            "Training batch number: 325 : loss --> 5.279661655426025\n",
            "Training batch number: 326 : loss --> 1.9970351457595825\n",
            "Training batch number: 327 : loss --> 1.4859215021133423\n",
            "Training batch number: 328 : loss --> 0.11890872567892075\n",
            "Training batch number: 329 : loss --> 4.31389856338501\n",
            "Training batch number: 330 : loss --> 0.03847242891788483\n",
            "Training batch number: 331 : loss --> 4.922135829925537\n",
            "Training batch number: 332 : loss --> 0.7420547008514404\n",
            "Training batch number: 333 : loss --> 0.5907367467880249\n",
            "Training batch number: 334 : loss --> 0.8475973606109619\n",
            "Training batch number: 335 : loss --> 1.823136568069458\n",
            "Training batch number: 336 : loss --> 0.16625478863716125\n",
            "Training batch number: 337 : loss --> 2.0139119625091553\n",
            "Training batch number: 338 : loss --> 0.2451338768005371\n",
            "Training batch number: 339 : loss --> 1.8143231868743896\n",
            "Training batch number: 340 : loss --> 9.943909645080566\n",
            "Training batch number: 341 : loss --> 0.2747088074684143\n",
            "Training batch number: 342 : loss --> 4.53210973739624\n",
            "Training batch number: 343 : loss --> 0.1346713900566101\n",
            "Training batch number: 344 : loss --> 10.007911682128906\n",
            "Training batch number: 345 : loss --> 1.5079731941223145\n",
            "Training batch number: 346 : loss --> 0.4382835328578949\n",
            "Training batch number: 347 : loss --> 1.6306359767913818\n",
            "Training batch number: 348 : loss --> 0.885571300983429\n",
            "Training batch number: 349 : loss --> 0.8851556181907654\n",
            "Training batch number: 350 : loss --> 0.20209389925003052\n",
            "Training batch number: 351 : loss --> 0.3488186299800873\n",
            "Training batch number: 352 : loss --> 0.10440853983163834\n",
            "Training batch number: 353 : loss --> 1.9595814943313599\n",
            "Training batch number: 354 : loss --> 0.08173170685768127\n",
            "Training batch number: 355 : loss --> 0.5294244885444641\n",
            "Training batch number: 356 : loss --> 0.376956969499588\n",
            "Training batch number: 357 : loss --> 3.1994314193725586\n",
            "Training batch number: 358 : loss --> 3.3813118934631348\n",
            "Training batch number: 359 : loss --> 4.709015369415283\n",
            "Training batch number: 360 : loss --> 1.4888111352920532\n",
            "Training batch number: 361 : loss --> 3.230532169342041\n",
            "Training batch number: 362 : loss --> 3.2492260932922363\n",
            "Training batch number: 363 : loss --> 0.629901647567749\n",
            "Training batch number: 364 : loss --> 0.34114205837249756\n",
            "Training batch number: 365 : loss --> 1.175194263458252\n",
            "Training batch number: 366 : loss --> 5.682898998260498\n",
            "Training batch number: 367 : loss --> 1.208889365196228\n",
            "Training batch number: 368 : loss --> 0.045135438442230225\n",
            "Training batch number: 369 : loss --> 0.8996382355690002\n",
            "Training batch number: 370 : loss --> 0.08696445822715759\n",
            "Training batch number: 371 : loss --> 0.5222454071044922\n",
            "Training batch number: 372 : loss --> 5.3900532722473145\n",
            "Training batch number: 373 : loss --> 2.7917070388793945\n",
            "Training batch number: 374 : loss --> 0.164426788687706\n",
            "Training batch number: 375 : loss --> 0.5083152651786804\n",
            "Training batch number: 376 : loss --> 0.45744290947914124\n",
            "Training batch number: 377 : loss --> 4.4153547286987305\n",
            "Training batch number: 378 : loss --> 3.5627918243408203\n",
            "Training batch number: 379 : loss --> 0.31220579147338867\n",
            "Training batch number: 380 : loss --> 0.28140562772750854\n",
            "Training batch number: 381 : loss --> 24.179378509521484\n",
            "Training batch number: 382 : loss --> 0.7409615516662598\n",
            "Training batch number: 383 : loss --> 0.7613193392753601\n",
            "Training batch number: 384 : loss --> 2.616875648498535\n",
            "Training batch number: 385 : loss --> 0.4085099995136261\n",
            "Training batch number: 386 : loss --> 6.7248125076293945\n",
            "Training batch number: 387 : loss --> 1.901097059249878\n",
            "Training batch number: 388 : loss --> 6.645720958709717\n",
            "Training batch number: 389 : loss --> 1.36174476146698\n",
            "Training batch number: 390 : loss --> 0.09545763581991196\n",
            "Training batch number: 391 : loss --> 3.3436219692230225\n",
            "Training batch number: 392 : loss --> 5.549360752105713\n",
            "Training batch number: 393 : loss --> 7.79240083694458\n",
            "Training batch number: 394 : loss --> 2.3299169540405273\n",
            "Training batch number: 395 : loss --> 0.8237287402153015\n",
            "Training batch number: 396 : loss --> 10.694644927978516\n",
            "Training batch number: 397 : loss --> 4.491589546203613\n",
            "Training batch number: 398 : loss --> 0.7713171243667603\n",
            "Training batch number: 399 : loss --> 1.2459849119186401\n",
            "Training batch number: 400 : loss --> 1.8334009647369385\n",
            "Training batch number: 401 : loss --> 1.1437557935714722\n",
            "Training batch number: 402 : loss --> 0.911815345287323\n",
            "Training batch number: 403 : loss --> 0.9237964153289795\n",
            "Training batch number: 404 : loss --> 0.5723809599876404\n",
            "Training batch number: 405 : loss --> 2.174509286880493\n",
            "Training batch number: 406 : loss --> 3.1364450454711914\n",
            "Training batch number: 407 : loss --> 1.9494506120681763\n",
            "Training batch number: 408 : loss --> 4.80828857421875\n",
            "Training batch number: 409 : loss --> 1.1023409366607666\n",
            "Training batch number: 410 : loss --> 0.24482892453670502\n",
            "Training batch number: 411 : loss --> 2.747112274169922\n",
            "Training batch number: 412 : loss --> 6.757145881652832\n",
            "Training batch number: 413 : loss --> 2.3825137615203857\n",
            "Training batch number: 414 : loss --> 0.2928600311279297\n",
            "Training batch number: 415 : loss --> 6.243693828582764\n",
            "Training batch number: 416 : loss --> 0.9411055445671082\n",
            "Training batch number: 417 : loss --> 1.2316769361495972\n",
            "Training batch number: 418 : loss --> 0.7801239490509033\n",
            "Training batch number: 419 : loss --> 5.091206073760986\n",
            "Training batch number: 420 : loss --> 3.3453609943389893\n",
            "Training batch number: 421 : loss --> 0.9536489844322205\n",
            "Training batch number: 422 : loss --> 0.8689754605293274\n",
            "Training batch number: 423 : loss --> 0.2674421966075897\n",
            "Training batch number: 424 : loss --> 0.3000667989253998\n",
            "Training batch number: 425 : loss --> 4.427524089813232\n",
            "Training batch number: 426 : loss --> 0.2565135657787323\n",
            "Training batch number: 427 : loss --> 0.3221966624259949\n",
            "Training batch number: 428 : loss --> 0.1593400537967682\n",
            "Training batch number: 429 : loss --> 3.6405954360961914\n",
            "Training batch number: 430 : loss --> 0.45871931314468384\n",
            "Training batch number: 431 : loss --> 0.46057456731796265\n",
            "Training batch number: 432 : loss --> 0.17453762888908386\n",
            "Training batch number: 433 : loss --> 0.09586694836616516\n",
            "Training batch number: 434 : loss --> 0.20903022587299347\n",
            "Training batch number: 435 : loss --> 0.17012441158294678\n",
            "Training batch number: 436 : loss --> 2.503937005996704\n",
            "Training batch number: 437 : loss --> 0.2952263653278351\n",
            "Training batch number: 438 : loss --> 0.2967239320278168\n",
            "Training batch number: 439 : loss --> 8.054152488708496\n",
            "Training batch number: 440 : loss --> 0.25602608919143677\n",
            "Training batch number: 441 : loss --> 4.593039035797119\n",
            "Training batch number: 442 : loss --> 14.062131881713867\n",
            "Training batch number: 443 : loss --> 0.5087376236915588\n",
            "Training batch number: 444 : loss --> 0.32592135667800903\n",
            "Training batch number: 445 : loss --> 4.683997631072998\n",
            "Training batch number: 446 : loss --> 0.8798772692680359\n",
            "Training batch number: 447 : loss --> 0.3511614501476288\n",
            "Training batch number: 448 : loss --> 2.9156441688537598\n",
            "Training batch number: 449 : loss --> 1.6186680793762207\n",
            "Training batch number: 450 : loss --> 2.9397761821746826\n",
            "Training batch number: 451 : loss --> 0.3796621561050415\n",
            "Training batch number: 452 : loss --> 2.0120105743408203\n",
            "Training batch number: 453 : loss --> 0.6845167875289917\n",
            "Training batch number: 454 : loss --> 0.34913212060928345\n",
            "Training batch number: 455 : loss --> 1.0796501636505127\n",
            "Training batch number: 456 : loss --> 0.45474889874458313\n",
            "Training batch number: 457 : loss --> 1.9525519609451294\n",
            "Training batch number: 458 : loss --> 0.7640106678009033\n",
            "Training batch number: 459 : loss --> 0.463276743888855\n",
            "Training batch number: 460 : loss --> 0.40445199608802795\n",
            "Training batch number: 461 : loss --> 0.6599981188774109\n",
            "Training batch number: 462 : loss --> 0.5804342031478882\n",
            "Training batch number: 463 : loss --> 0.8669586181640625\n",
            "Training batch number: 464 : loss --> 0.7288153767585754\n",
            "Training batch number: 465 : loss --> 0.5513868927955627\n",
            "Training batch number: 466 : loss --> 9.615452766418457\n",
            "Training batch number: 467 : loss --> 0.27733567357063293\n",
            "Training batch number: 468 : loss --> 0.11271370947360992\n",
            "\n",
            "\n",
            "Training.. Loss: 0.0162 Acc: 0.9953\n",
            "Validating batch number: 0 : loss --> 0.19701673090457916\n",
            "Validating batch number: 1 : loss --> 0.8893774747848511\n",
            "Validating batch number: 2 : loss --> 0.449178546667099\n",
            "Validating batch number: 3 : loss --> 0.7322217226028442\n",
            "Validating batch number: 4 : loss --> 0.4234936237335205\n",
            "Validating batch number: 5 : loss --> 2.0130293369293213\n",
            "Validating batch number: 6 : loss --> 1.273837685585022\n",
            "Validating batch number: 7 : loss --> 0.04972134530544281\n",
            "Validating batch number: 8 : loss --> 0.1546134352684021\n",
            "Validating batch number: 9 : loss --> 0.4128633439540863\n",
            "Validating batch number: 10 : loss --> 0.23800115287303925\n",
            "Validating batch number: 11 : loss --> 0.4159066081047058\n",
            "Validating batch number: 12 : loss --> 7.729494094848633\n",
            "Validating batch number: 13 : loss --> 1.3196866512298584\n",
            "Validating batch number: 14 : loss --> 3.8460936546325684\n",
            "Validating batch number: 15 : loss --> 0.1728452444076538\n",
            "Validating batch number: 16 : loss --> 0.30914610624313354\n",
            "Validating batch number: 17 : loss --> 4.126054763793945\n",
            "Validating batch number: 18 : loss --> 6.245477199554443\n",
            "Validating batch number: 19 : loss --> 1.9481669664382935\n",
            "Validating batch number: 20 : loss --> 1.8637959957122803\n",
            "Validating batch number: 21 : loss --> 4.973559379577637\n",
            "Validating batch number: 22 : loss --> 1.245047688484192\n",
            "Validating batch number: 23 : loss --> 0.13540233671665192\n",
            "Validating batch number: 24 : loss --> 8.794780731201172\n",
            "Validating batch number: 25 : loss --> 1.940741777420044\n",
            "Validating batch number: 26 : loss --> 2.6980953216552734\n",
            "Validating batch number: 27 : loss --> 4.854737281799316\n",
            "Validating batch number: 28 : loss --> 1.606355905532837\n",
            "Validating batch number: 29 : loss --> 0.05837901309132576\n",
            "Validating batch number: 30 : loss --> 0.0794866681098938\n",
            "Validating batch number: 31 : loss --> 0.1409038007259369\n",
            "Validating batch number: 32 : loss --> 0.05695241317152977\n",
            "Validating batch number: 33 : loss --> 5.1071343421936035\n",
            "Validating batch number: 34 : loss --> 2.017991542816162\n",
            "Validating batch number: 35 : loss --> 1.8482009172439575\n",
            "Validating batch number: 36 : loss --> 0.8524779081344604\n",
            "Validating batch number: 37 : loss --> 2.8862533569335938\n",
            "Validating batch number: 38 : loss --> 5.79097318649292\n",
            "Validating batch number: 39 : loss --> 4.934886932373047\n",
            "Validating batch number: 40 : loss --> 1.8287473917007446\n",
            "Validating batch number: 41 : loss --> 5.394896984100342\n",
            "Validating batch number: 42 : loss --> 0.6174094080924988\n",
            "Validating batch number: 43 : loss --> 4.16558837890625\n",
            "Validating batch number: 44 : loss --> 6.175932884216309\n",
            "Validating batch number: 45 : loss --> 0.7188783884048462\n",
            "Validating batch number: 46 : loss --> 5.672581672668457\n",
            "Validating batch number: 47 : loss --> 0.05202819034457207\n",
            "Validating batch number: 48 : loss --> 0.9231442809104919\n",
            "Validating batch number: 49 : loss --> 0.5061629414558411\n",
            "Validating batch number: 50 : loss --> 18.891176223754883\n",
            "Validating batch number: 51 : loss --> 0.7027048468589783\n",
            "Validating batch number: 52 : loss --> 15.570976257324219\n",
            "Validating batch number: 53 : loss --> 0.18831731379032135\n",
            "Validating batch number: 54 : loss --> 0.13936518132686615\n",
            "Validating batch number: 55 : loss --> 6.5030670166015625\n",
            "Validating batch number: 56 : loss --> 9.925019264221191\n",
            "Validating batch number: 57 : loss --> 1.4881839752197266\n",
            "Validating batch number: 58 : loss --> 0.5789605975151062\n",
            "Validating batch number: 59 : loss --> 9.395527839660645\n",
            "Validating batch number: 60 : loss --> 6.043107986450195\n",
            "Validating batch number: 61 : loss --> 0.5814459919929504\n",
            "Validating batch number: 62 : loss --> 11.477479934692383\n",
            "Validating batch number: 63 : loss --> 0.7379536032676697\n",
            "Validating batch number: 64 : loss --> 8.448219299316406\n",
            "Validating batch number: 65 : loss --> 0.9268022775650024\n",
            "Validating batch number: 66 : loss --> 5.338122844696045\n",
            "Validating batch number: 67 : loss --> 0.056581493467092514\n",
            "Validating batch number: 68 : loss --> 1.8886739015579224\n",
            "Validating batch number: 69 : loss --> 4.8685808181762695\n",
            "Validating batch number: 70 : loss --> 0.09593617916107178\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 86%|████████▌ | 6/7 [02:51<00:28, 28.61s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validating batch number: 71 : loss --> 0.06507963687181473\n",
            "Validating batch number: 72 : loss --> 0.08361772447824478\n",
            "Validating batch number: 73 : loss --> 6.616591930389404\n",
            "Validating batch number: 74 : loss --> 1.9971903562545776\n",
            "Validating batch number: 75 : loss --> 0.04359527677297592\n",
            "Validating batch number: 76 : loss --> 1.4129408597946167\n",
            "Validating batch number: 77 : loss --> 1.9189790487289429\n",
            "Validating batch number: 78 : loss --> 0.000441500247688964\n",
            "Testing.. Loss: 0.0227 Acc: 0.9934\n",
            "\n",
            "Epoch 6/6\n",
            "----------\n",
            "Training batch number: 0 : loss --> 0.10448864102363586\n",
            "Training batch number: 1 : loss --> 1.9639939069747925\n",
            "Training batch number: 2 : loss --> 0.12282001227140427\n",
            "Training batch number: 3 : loss --> 0.06233997270464897\n",
            "Training batch number: 4 : loss --> 1.018123745918274\n",
            "Training batch number: 5 : loss --> 0.48233669996261597\n",
            "Training batch number: 6 : loss --> 0.056103453040122986\n",
            "Training batch number: 7 : loss --> 0.7025155425071716\n",
            "Training batch number: 8 : loss --> 2.964141845703125\n",
            "Training batch number: 9 : loss --> 0.5672515034675598\n",
            "Training batch number: 10 : loss --> 4.472421169281006\n",
            "Training batch number: 11 : loss --> 0.1522999256849289\n",
            "Training batch number: 12 : loss --> 0.19588682055473328\n",
            "Training batch number: 13 : loss --> 0.9904263615608215\n",
            "Training batch number: 14 : loss --> 0.19002437591552734\n",
            "Training batch number: 15 : loss --> 0.26523444056510925\n",
            "Training batch number: 16 : loss --> 0.47439444065093994\n",
            "Training batch number: 17 : loss --> 0.026024559512734413\n",
            "Training batch number: 18 : loss --> 0.12379015982151031\n",
            "Training batch number: 19 : loss --> 0.25823813676834106\n",
            "Training batch number: 20 : loss --> 0.5901980400085449\n",
            "Training batch number: 21 : loss --> 0.18024134635925293\n",
            "Training batch number: 22 : loss --> 0.21935778856277466\n",
            "Training batch number: 23 : loss --> 0.26822808384895325\n",
            "Training batch number: 24 : loss --> 0.030685652047395706\n",
            "Training batch number: 25 : loss --> 0.27829688787460327\n",
            "Training batch number: 26 : loss --> 0.3565235733985901\n",
            "Training batch number: 27 : loss --> 2.1371262073516846\n",
            "Training batch number: 28 : loss --> 2.4306530952453613\n",
            "Training batch number: 29 : loss --> 0.20107980072498322\n",
            "Training batch number: 30 : loss --> 0.0740334615111351\n",
            "Training batch number: 31 : loss --> 0.11691023409366608\n",
            "Training batch number: 32 : loss --> 1.3033572435379028\n",
            "Training batch number: 33 : loss --> 0.5885882377624512\n",
            "Training batch number: 34 : loss --> 0.4895380139350891\n",
            "Training batch number: 35 : loss --> 0.02159828692674637\n",
            "Training batch number: 36 : loss --> 0.05355747044086456\n",
            "Training batch number: 37 : loss --> 0.9190008640289307\n",
            "Training batch number: 38 : loss --> 0.07018985599279404\n",
            "Training batch number: 39 : loss --> 0.48323893547058105\n",
            "Training batch number: 40 : loss --> 0.4716348350048065\n",
            "Training batch number: 41 : loss --> 0.08044973760843277\n",
            "Training batch number: 42 : loss --> 3.5431313514709473\n",
            "Training batch number: 43 : loss --> 0.251653790473938\n",
            "Training batch number: 44 : loss --> 1.0668058395385742\n",
            "Training batch number: 45 : loss --> 0.2353605180978775\n",
            "Training batch number: 46 : loss --> 1.2289609909057617\n",
            "Training batch number: 47 : loss --> 0.11978884041309357\n",
            "Training batch number: 48 : loss --> 2.8623979091644287\n",
            "Training batch number: 49 : loss --> 1.2504416704177856\n",
            "Training batch number: 50 : loss --> 0.17377454042434692\n",
            "Training batch number: 51 : loss --> 2.7228753566741943\n",
            "Training batch number: 52 : loss --> 0.05391627922654152\n",
            "Training batch number: 53 : loss --> 0.16609501838684082\n",
            "Training batch number: 54 : loss --> 0.18330320715904236\n",
            "Training batch number: 55 : loss --> 3.313410758972168\n",
            "Training batch number: 56 : loss --> 0.39949584007263184\n",
            "Training batch number: 57 : loss --> 0.03541318699717522\n",
            "Training batch number: 58 : loss --> 0.13266855478286743\n",
            "Training batch number: 59 : loss --> 0.0348159559071064\n",
            "Training batch number: 60 : loss --> 0.16372999548912048\n",
            "Training batch number: 61 : loss --> 2.351691961288452\n",
            "Training batch number: 62 : loss --> 4.022754192352295\n",
            "Training batch number: 63 : loss --> 1.5996718406677246\n",
            "Training batch number: 64 : loss --> 0.41467607021331787\n",
            "Training batch number: 65 : loss --> 0.06893277168273926\n",
            "Training batch number: 66 : loss --> 0.21186046302318573\n",
            "Training batch number: 67 : loss --> 0.28820571303367615\n",
            "Training batch number: 68 : loss --> 0.04445667564868927\n",
            "Training batch number: 69 : loss --> 0.2819626033306122\n",
            "Training batch number: 70 : loss --> 0.33371293544769287\n",
            "Training batch number: 71 : loss --> 0.4599596858024597\n",
            "Training batch number: 72 : loss --> 0.03215145319700241\n",
            "Training batch number: 73 : loss --> 6.490523338317871\n",
            "Training batch number: 74 : loss --> 0.31357303261756897\n",
            "Training batch number: 75 : loss --> 3.406615734100342\n",
            "Training batch number: 76 : loss --> 0.8167566657066345\n",
            "Training batch number: 77 : loss --> 0.07601673901081085\n",
            "Training batch number: 78 : loss --> 0.7215108871459961\n",
            "Training batch number: 79 : loss --> 1.84311842918396\n",
            "Training batch number: 80 : loss --> 4.310787677764893\n",
            "Training batch number: 81 : loss --> 9.134404182434082\n",
            "Training batch number: 82 : loss --> 5.457388877868652\n",
            "Training batch number: 83 : loss --> 0.41866469383239746\n",
            "Training batch number: 84 : loss --> 1.6226506233215332\n",
            "Training batch number: 85 : loss --> 0.17210780084133148\n",
            "Training batch number: 86 : loss --> 0.07475844770669937\n",
            "Training batch number: 87 : loss --> 3.160386085510254\n",
            "Training batch number: 88 : loss --> 0.15899281203746796\n",
            "Training batch number: 89 : loss --> 0.31907176971435547\n",
            "Training batch number: 90 : loss --> 4.071084976196289\n",
            "Training batch number: 91 : loss --> 5.371453762054443\n",
            "Training batch number: 92 : loss --> 0.15125514566898346\n",
            "Training batch number: 93 : loss --> 0.10586237162351608\n",
            "Training batch number: 94 : loss --> 2.2798573970794678\n",
            "Training batch number: 95 : loss --> 1.8521150350570679\n",
            "Training batch number: 96 : loss --> 0.11964171379804611\n",
            "Training batch number: 97 : loss --> 0.6354358196258545\n",
            "Training batch number: 98 : loss --> 0.08747950196266174\n",
            "Training batch number: 99 : loss --> 0.15452848374843597\n",
            "Training batch number: 100 : loss --> 1.5198954343795776\n",
            "Training batch number: 101 : loss --> 0.08973610401153564\n",
            "Training batch number: 102 : loss --> 0.5052246451377869\n",
            "Training batch number: 103 : loss --> 2.1366429328918457\n",
            "Training batch number: 104 : loss --> 0.21794897317886353\n",
            "Training batch number: 105 : loss --> 0.5817426443099976\n",
            "Training batch number: 106 : loss --> 0.05591975525021553\n",
            "Training batch number: 107 : loss --> 0.3372134566307068\n",
            "Training batch number: 108 : loss --> 0.052017223089933395\n",
            "Training batch number: 109 : loss --> 1.8246454000473022\n",
            "Training batch number: 110 : loss --> 0.10358838737010956\n",
            "Training batch number: 111 : loss --> 0.06689957529306412\n",
            "Training batch number: 112 : loss --> 0.4456794559955597\n",
            "Training batch number: 113 : loss --> 0.03207243233919144\n",
            "Training batch number: 114 : loss --> 0.09688515216112137\n",
            "Training batch number: 115 : loss --> 2.7915942668914795\n",
            "Training batch number: 116 : loss --> 0.16375674307346344\n",
            "Training batch number: 117 : loss --> 0.18011435866355896\n",
            "Training batch number: 118 : loss --> 0.21301254630088806\n",
            "Training batch number: 119 : loss --> 8.171631813049316\n",
            "Training batch number: 120 : loss --> 0.015988970175385475\n",
            "Training batch number: 121 : loss --> 0.2593671381473541\n",
            "Training batch number: 122 : loss --> 0.340637743473053\n",
            "Training batch number: 123 : loss --> 1.2981071472167969\n",
            "Training batch number: 124 : loss --> 0.05552263930439949\n",
            "Training batch number: 125 : loss --> 3.1453142166137695\n",
            "Training batch number: 126 : loss --> 0.5032643675804138\n",
            "Training batch number: 127 : loss --> 0.1056152954697609\n",
            "Training batch number: 128 : loss --> 0.6863231658935547\n",
            "Training batch number: 129 : loss --> 0.4033581614494324\n",
            "Training batch number: 130 : loss --> 3.08620548248291\n",
            "Training batch number: 131 : loss --> 4.066802024841309\n",
            "Training batch number: 132 : loss --> 0.01674855872988701\n",
            "Training batch number: 133 : loss --> 3.109513998031616\n",
            "Training batch number: 134 : loss --> 3.9958131313323975\n",
            "Training batch number: 135 : loss --> 0.2791890799999237\n",
            "Training batch number: 136 : loss --> 0.28788885474205017\n",
            "Training batch number: 137 : loss --> 0.09044034034013748\n",
            "Training batch number: 138 : loss --> 5.79187536239624\n",
            "Training batch number: 139 : loss --> 0.081535704433918\n",
            "Training batch number: 140 : loss --> 0.22785694897174835\n",
            "Training batch number: 141 : loss --> 0.32182323932647705\n",
            "Training batch number: 142 : loss --> 1.3330703973770142\n",
            "Training batch number: 143 : loss --> 5.591836452484131\n",
            "Training batch number: 144 : loss --> 1.20314621925354\n",
            "Training batch number: 145 : loss --> 1.666533350944519\n",
            "Training batch number: 146 : loss --> 0.4904068112373352\n",
            "Training batch number: 147 : loss --> 0.10097916424274445\n",
            "Training batch number: 148 : loss --> 0.12909971177577972\n",
            "Training batch number: 149 : loss --> 1.6868369579315186\n",
            "Training batch number: 150 : loss --> 0.5734319686889648\n",
            "Training batch number: 151 : loss --> 0.2065006047487259\n",
            "Training batch number: 152 : loss --> 5.316068172454834\n",
            "Training batch number: 153 : loss --> 7.359192848205566\n",
            "Training batch number: 154 : loss --> 8.266048431396484\n",
            "Training batch number: 155 : loss --> 3.5206196308135986\n",
            "Training batch number: 156 : loss --> 5.380505084991455\n",
            "Training batch number: 157 : loss --> 1.3526846170425415\n",
            "Training batch number: 158 : loss --> 1.427252173423767\n",
            "Training batch number: 159 : loss --> 0.4936383068561554\n",
            "Training batch number: 160 : loss --> 2.430107593536377\n",
            "Training batch number: 161 : loss --> 0.3440694808959961\n",
            "Training batch number: 162 : loss --> 3.760221004486084\n",
            "Training batch number: 163 : loss --> 0.6774372458457947\n",
            "Training batch number: 164 : loss --> 0.3186015784740448\n",
            "Training batch number: 165 : loss --> 3.6912291049957275\n",
            "Training batch number: 166 : loss --> 0.3272183835506439\n",
            "Training batch number: 167 : loss --> 0.43507063388824463\n",
            "Training batch number: 168 : loss --> 2.38655686378479\n",
            "Training batch number: 169 : loss --> 0.2152940034866333\n",
            "Training batch number: 170 : loss --> 0.2087661325931549\n",
            "Training batch number: 171 : loss --> 0.30441200733184814\n",
            "Training batch number: 172 : loss --> 0.118224136531353\n",
            "Training batch number: 173 : loss --> 1.0543078184127808\n",
            "Training batch number: 174 : loss --> 0.11718425899744034\n",
            "Training batch number: 175 : loss --> 0.017135893926024437\n",
            "Training batch number: 176 : loss --> 0.2487889677286148\n",
            "Training batch number: 177 : loss --> 0.03364833444356918\n",
            "Training batch number: 178 : loss --> 3.353841543197632\n",
            "Training batch number: 179 : loss --> 2.415184736251831\n",
            "Training batch number: 180 : loss --> 0.4076158404350281\n",
            "Training batch number: 181 : loss --> 0.9806976318359375\n",
            "Training batch number: 182 : loss --> 0.09633642435073853\n",
            "Training batch number: 183 : loss --> 1.2554821968078613\n",
            "Training batch number: 184 : loss --> 0.5245370864868164\n",
            "Training batch number: 185 : loss --> 0.7352039813995361\n",
            "Training batch number: 186 : loss --> 0.027067847549915314\n",
            "Training batch number: 187 : loss --> 0.0955715999007225\n",
            "Training batch number: 188 : loss --> 0.8170214295387268\n",
            "Training batch number: 189 : loss --> 0.38987696170806885\n",
            "Training batch number: 190 : loss --> 2.704028367996216\n",
            "Training batch number: 191 : loss --> 0.42705658078193665\n",
            "Training batch number: 192 : loss --> 0.23135708272457123\n",
            "Training batch number: 193 : loss --> 1.8216530084609985\n",
            "Training batch number: 194 : loss --> 0.04407533258199692\n",
            "Training batch number: 195 : loss --> 0.9617831707000732\n",
            "Training batch number: 196 : loss --> 1.1230182647705078\n",
            "Training batch number: 197 : loss --> 8.03853702545166\n",
            "Training batch number: 198 : loss --> 0.08698917180299759\n",
            "Training batch number: 199 : loss --> 0.254440575838089\n",
            "Training batch number: 200 : loss --> 11.09616470336914\n",
            "Training batch number: 201 : loss --> 5.869132995605469\n",
            "Training batch number: 202 : loss --> 0.15788832306861877\n",
            "Training batch number: 203 : loss --> 4.180629253387451\n",
            "Training batch number: 204 : loss --> 0.3545473515987396\n",
            "Training batch number: 205 : loss --> 0.056911077350378036\n",
            "Training batch number: 206 : loss --> 2.8222029209136963\n",
            "Training batch number: 207 : loss --> 2.0098886489868164\n",
            "Training batch number: 208 : loss --> 8.606216430664062\n",
            "Training batch number: 209 : loss --> 1.6782437562942505\n",
            "Training batch number: 210 : loss --> 0.47978320717811584\n",
            "Training batch number: 211 : loss --> 0.36719000339508057\n",
            "Training batch number: 212 : loss --> 0.5361257791519165\n",
            "Training batch number: 213 : loss --> 4.489290714263916\n",
            "Training batch number: 214 : loss --> 1.9248840808868408\n",
            "Training batch number: 215 : loss --> 1.6621953248977661\n",
            "Training batch number: 216 : loss --> 0.39541977643966675\n",
            "Training batch number: 217 : loss --> 0.4262768626213074\n",
            "Training batch number: 218 : loss --> 0.050552789121866226\n",
            "Training batch number: 219 : loss --> 0.15293775498867035\n",
            "Training batch number: 220 : loss --> 0.9669299125671387\n",
            "Training batch number: 221 : loss --> 0.20199528336524963\n",
            "Training batch number: 222 : loss --> 0.02744811400771141\n",
            "Training batch number: 223 : loss --> 0.27535057067871094\n",
            "Training batch number: 224 : loss --> 4.836479187011719\n",
            "Training batch number: 225 : loss --> 5.631062030792236\n",
            "Training batch number: 226 : loss --> 0.16234618425369263\n",
            "Training batch number: 227 : loss --> 2.181093692779541\n",
            "Training batch number: 228 : loss --> 4.267778396606445\n",
            "Training batch number: 229 : loss --> 1.7842289209365845\n",
            "Training batch number: 230 : loss --> 0.49774855375289917\n",
            "Training batch number: 231 : loss --> 6.200699329376221\n",
            "Training batch number: 232 : loss --> 0.12174122035503387\n",
            "Training batch number: 233 : loss --> 0.5579960942268372\n",
            "Training batch number: 234 : loss --> 5.524940490722656\n",
            "Training batch number: 235 : loss --> 0.12823398411273956\n",
            "Training batch number: 236 : loss --> 0.14124202728271484\n",
            "Training batch number: 237 : loss --> 0.036578886210918427\n",
            "Training batch number: 238 : loss --> 0.024513378739356995\n",
            "Training batch number: 239 : loss --> 0.3734091818332672\n",
            "Training batch number: 240 : loss --> 0.2886200547218323\n",
            "Training batch number: 241 : loss --> 3.3689448833465576\n",
            "Training batch number: 242 : loss --> 0.1826145499944687\n",
            "Training batch number: 243 : loss --> 1.6036120653152466\n",
            "Training batch number: 244 : loss --> 0.7677381634712219\n",
            "Training batch number: 245 : loss --> 0.046939607709646225\n",
            "Training batch number: 246 : loss --> 0.28589993715286255\n",
            "Training batch number: 247 : loss --> 0.10127323865890503\n",
            "Training batch number: 248 : loss --> 1.1037776470184326\n",
            "Training batch number: 249 : loss --> 2.398124933242798\n",
            "Training batch number: 250 : loss --> 0.08844088017940521\n",
            "Training batch number: 251 : loss --> 0.33152416348457336\n",
            "Training batch number: 252 : loss --> 8.555318832397461\n",
            "Training batch number: 253 : loss --> 0.19206881523132324\n",
            "Training batch number: 254 : loss --> 1.5624971389770508\n",
            "Training batch number: 255 : loss --> 0.597277820110321\n",
            "Training batch number: 256 : loss --> 0.06426073610782623\n",
            "Training batch number: 257 : loss --> 6.348024368286133\n",
            "Training batch number: 258 : loss --> 0.10403448343276978\n",
            "Training batch number: 259 : loss --> 0.9545524716377258\n",
            "Training batch number: 260 : loss --> 0.3815179169178009\n",
            "Training batch number: 261 : loss --> 0.3862100839614868\n",
            "Training batch number: 262 : loss --> 0.20061713457107544\n",
            "Training batch number: 263 : loss --> 0.8887264728546143\n",
            "Training batch number: 264 : loss --> 0.09732618927955627\n",
            "Training batch number: 265 : loss --> 6.906221866607666\n",
            "Training batch number: 266 : loss --> 1.0576130151748657\n",
            "Training batch number: 267 : loss --> 0.26857319474220276\n",
            "Training batch number: 268 : loss --> 3.38110613822937\n",
            "Training batch number: 269 : loss --> 1.63517427444458\n",
            "Training batch number: 270 : loss --> 0.20201632380485535\n",
            "Training batch number: 271 : loss --> 10.445923805236816\n",
            "Training batch number: 272 : loss --> 0.9529007077217102\n",
            "Training batch number: 273 : loss --> 0.2105303406715393\n",
            "Training batch number: 274 : loss --> 2.113098382949829\n",
            "Training batch number: 275 : loss --> 3.1783130168914795\n",
            "Training batch number: 276 : loss --> 12.362814903259277\n",
            "Training batch number: 277 : loss --> 6.532106876373291\n",
            "Training batch number: 278 : loss --> 6.021492958068848\n",
            "Training batch number: 279 : loss --> 0.02135205641388893\n",
            "Training batch number: 280 : loss --> 0.19829238951206207\n",
            "Training batch number: 281 : loss --> 9.502370834350586\n",
            "Training batch number: 282 : loss --> 0.44352537393569946\n",
            "Training batch number: 283 : loss --> 0.15269987285137177\n",
            "Training batch number: 284 : loss --> 0.5239033102989197\n",
            "Training batch number: 285 : loss --> 0.030637823045253754\n",
            "Training batch number: 286 : loss --> 2.2584731578826904\n",
            "Training batch number: 287 : loss --> 0.20984375476837158\n",
            "Training batch number: 288 : loss --> 2.766162872314453\n",
            "Training batch number: 289 : loss --> 5.584924697875977\n",
            "Training batch number: 290 : loss --> 2.233510732650757\n",
            "Training batch number: 291 : loss --> 0.8916441798210144\n",
            "Training batch number: 292 : loss --> 3.7329230308532715\n",
            "Training batch number: 293 : loss --> 5.25840425491333\n",
            "Training batch number: 294 : loss --> 11.617244720458984\n",
            "Training batch number: 295 : loss --> 5.72376823425293\n",
            "Training batch number: 296 : loss --> 6.177681922912598\n",
            "Training batch number: 297 : loss --> 0.8652768135070801\n",
            "Training batch number: 298 : loss --> 0.00975946243852377\n",
            "Training batch number: 299 : loss --> 7.900752544403076\n",
            "Training batch number: 300 : loss --> 2.0953683853149414\n",
            "Training batch number: 301 : loss --> 2.100306510925293\n",
            "Training batch number: 302 : loss --> 0.3428136110305786\n",
            "Training batch number: 303 : loss --> 5.357848167419434\n",
            "Training batch number: 304 : loss --> 3.783557176589966\n",
            "Training batch number: 305 : loss --> 0.567940354347229\n",
            "Training batch number: 306 : loss --> 5.505494594573975\n",
            "Training batch number: 307 : loss --> 0.16832990944385529\n",
            "Training batch number: 308 : loss --> 1.3717738389968872\n",
            "Training batch number: 309 : loss --> 1.329901933670044\n",
            "Training batch number: 310 : loss --> 1.3364487886428833\n",
            "Training batch number: 311 : loss --> 1.2819784879684448\n",
            "Training batch number: 312 : loss --> 0.7936141490936279\n",
            "Training batch number: 313 : loss --> 0.20720624923706055\n",
            "Training batch number: 314 : loss --> 0.20635390281677246\n",
            "Training batch number: 315 : loss --> 0.46561768651008606\n",
            "Training batch number: 316 : loss --> 0.11654161661863327\n",
            "Training batch number: 317 : loss --> 0.8223244547843933\n",
            "Training batch number: 318 : loss --> 0.2677053213119507\n",
            "Training batch number: 319 : loss --> 0.0859893411397934\n",
            "Training batch number: 320 : loss --> 4.91738224029541\n",
            "Training batch number: 321 : loss --> 4.094753265380859\n",
            "Training batch number: 322 : loss --> 0.24534890055656433\n",
            "Training batch number: 323 : loss --> 0.8431931734085083\n",
            "Training batch number: 324 : loss --> 0.15932118892669678\n",
            "Training batch number: 325 : loss --> 0.424761563539505\n",
            "Training batch number: 326 : loss --> 0.40831655263900757\n",
            "Training batch number: 327 : loss --> 0.10731103271245956\n",
            "Training batch number: 328 : loss --> 3.3904213905334473\n",
            "Training batch number: 329 : loss --> 0.5548058748245239\n",
            "Training batch number: 330 : loss --> 0.8603490591049194\n",
            "Training batch number: 331 : loss --> 0.9816282987594604\n",
            "Training batch number: 332 : loss --> 0.4973924458026886\n",
            "Training batch number: 333 : loss --> 0.05912294238805771\n",
            "Training batch number: 334 : loss --> 6.520564556121826\n",
            "Training batch number: 335 : loss --> 0.042488690465688705\n",
            "Training batch number: 336 : loss --> 0.0694269984960556\n",
            "Training batch number: 337 : loss --> 1.028348445892334\n",
            "Training batch number: 338 : loss --> 3.9662563800811768\n",
            "Training batch number: 339 : loss --> 4.936304092407227\n",
            "Training batch number: 340 : loss --> 0.09368439763784409\n",
            "Training batch number: 341 : loss --> 0.9655145406723022\n",
            "Training batch number: 342 : loss --> 5.270116329193115\n",
            "Training batch number: 343 : loss --> 0.20163202285766602\n",
            "Training batch number: 344 : loss --> 0.3402257561683655\n",
            "Training batch number: 345 : loss --> 0.4264090061187744\n",
            "Training batch number: 346 : loss --> 0.5114375948905945\n",
            "Training batch number: 347 : loss --> 0.2313811331987381\n",
            "Training batch number: 348 : loss --> 0.3242682218551636\n",
            "Training batch number: 349 : loss --> 2.051255464553833\n",
            "Training batch number: 350 : loss --> 1.0029674768447876\n",
            "Training batch number: 351 : loss --> 1.9498882293701172\n",
            "Training batch number: 352 : loss --> 0.11824628710746765\n",
            "Training batch number: 353 : loss --> 0.42997607588768005\n",
            "Training batch number: 354 : loss --> 0.7018531560897827\n",
            "Training batch number: 355 : loss --> 2.3971714973449707\n",
            "Training batch number: 356 : loss --> 0.48421478271484375\n",
            "Training batch number: 357 : loss --> 0.8886904716491699\n",
            "Training batch number: 358 : loss --> 0.02347230352461338\n",
            "Training batch number: 359 : loss --> 4.699535369873047\n",
            "Training batch number: 360 : loss --> 2.374842882156372\n",
            "Training batch number: 361 : loss --> 0.06771237403154373\n",
            "Training batch number: 362 : loss --> 2.486210346221924\n",
            "Training batch number: 363 : loss --> 0.22285278141498566\n",
            "Training batch number: 364 : loss --> 1.239898920059204\n",
            "Training batch number: 365 : loss --> 1.0565440654754639\n",
            "Training batch number: 366 : loss --> 0.5632908344268799\n",
            "Training batch number: 367 : loss --> 2.9352545738220215\n",
            "Training batch number: 368 : loss --> 0.31069403886795044\n",
            "Training batch number: 369 : loss --> 0.5871893167495728\n",
            "Training batch number: 370 : loss --> 0.6173915863037109\n",
            "Training batch number: 371 : loss --> 1.8398195505142212\n",
            "Training batch number: 372 : loss --> 2.871847152709961\n",
            "Training batch number: 373 : loss --> 3.6197335720062256\n",
            "Training batch number: 374 : loss --> 0.09903073310852051\n",
            "Training batch number: 375 : loss --> 0.13931171596050262\n",
            "Training batch number: 376 : loss --> 0.053088195621967316\n",
            "Training batch number: 377 : loss --> 0.06034060940146446\n",
            "Training batch number: 378 : loss --> 0.7259158492088318\n",
            "Training batch number: 379 : loss --> 0.0742068886756897\n",
            "Training batch number: 380 : loss --> 1.1135261058807373\n",
            "Training batch number: 381 : loss --> 0.27490687370300293\n",
            "Training batch number: 382 : loss --> 1.8409688472747803\n",
            "Training batch number: 383 : loss --> 0.07485639303922653\n",
            "Training batch number: 384 : loss --> 0.15154233574867249\n",
            "Training batch number: 385 : loss --> 6.4527997970581055\n",
            "Training batch number: 386 : loss --> 0.08514177054166794\n",
            "Training batch number: 387 : loss --> 5.189113616943359\n",
            "Training batch number: 388 : loss --> 0.5904138684272766\n",
            "Training batch number: 389 : loss --> 2.937899112701416\n",
            "Training batch number: 390 : loss --> 1.260881781578064\n",
            "Training batch number: 391 : loss --> 0.1398237645626068\n",
            "Training batch number: 392 : loss --> 7.5863776206970215\n",
            "Training batch number: 393 : loss --> 0.26677653193473816\n",
            "Training batch number: 394 : loss --> 6.431353569030762\n",
            "Training batch number: 395 : loss --> 8.130924224853516\n",
            "Training batch number: 396 : loss --> 0.0752016082406044\n",
            "Training batch number: 397 : loss --> 1.1914368867874146\n",
            "Training batch number: 398 : loss --> 0.6747581362724304\n",
            "Training batch number: 399 : loss --> 0.0656747967004776\n",
            "Training batch number: 400 : loss --> 4.193973064422607\n",
            "Training batch number: 401 : loss --> 0.615811288356781\n",
            "Training batch number: 402 : loss --> 0.6079159379005432\n",
            "Training batch number: 403 : loss --> 0.03681747615337372\n",
            "Training batch number: 404 : loss --> 0.13273140788078308\n",
            "Training batch number: 405 : loss --> 0.4508041441440582\n",
            "Training batch number: 406 : loss --> 4.278463363647461\n",
            "Training batch number: 407 : loss --> 0.4702323079109192\n",
            "Training batch number: 408 : loss --> 0.7393890619277954\n",
            "Training batch number: 409 : loss --> 5.460003852844238\n",
            "Training batch number: 410 : loss --> 1.8068689107894897\n",
            "Training batch number: 411 : loss --> 0.0964103490114212\n",
            "Training batch number: 412 : loss --> 0.08408359438180923\n",
            "Training batch number: 413 : loss --> 0.8089778423309326\n",
            "Training batch number: 414 : loss --> 0.0655333548784256\n",
            "Training batch number: 415 : loss --> 0.42311564087867737\n",
            "Training batch number: 416 : loss --> 0.5297371745109558\n",
            "Training batch number: 417 : loss --> 1.8813146352767944\n",
            "Training batch number: 418 : loss --> 0.29358527064323425\n",
            "Training batch number: 419 : loss --> 1.7173272371292114\n",
            "Training batch number: 420 : loss --> 0.07609247416257858\n",
            "Training batch number: 421 : loss --> 1.1273845434188843\n",
            "Training batch number: 422 : loss --> 0.8285360932350159\n",
            "Training batch number: 423 : loss --> 0.5265403389930725\n",
            "Training batch number: 424 : loss --> 0.0752486065030098\n",
            "Training batch number: 425 : loss --> 0.05087701231241226\n",
            "Training batch number: 426 : loss --> 4.686453342437744\n",
            "Training batch number: 427 : loss --> 1.3687808513641357\n",
            "Training batch number: 428 : loss --> 1.1989938020706177\n",
            "Training batch number: 429 : loss --> 1.6971811056137085\n",
            "Training batch number: 430 : loss --> 0.8755512237548828\n",
            "Training batch number: 431 : loss --> 0.18896272778511047\n",
            "Training batch number: 432 : loss --> 0.35948100686073303\n",
            "Training batch number: 433 : loss --> 2.279993772506714\n",
            "Training batch number: 434 : loss --> 0.16507579386234283\n",
            "Training batch number: 435 : loss --> 0.2994228005409241\n",
            "Training batch number: 436 : loss --> 0.4285809099674225\n",
            "Training batch number: 437 : loss --> 0.5451463460922241\n",
            "Training batch number: 438 : loss --> 0.3129766285419464\n",
            "Training batch number: 439 : loss --> 4.214687347412109\n",
            "Training batch number: 440 : loss --> 0.4216803014278412\n",
            "Training batch number: 441 : loss --> 0.7048781514167786\n",
            "Training batch number: 442 : loss --> 0.9264393448829651\n",
            "Training batch number: 443 : loss --> 3.5874125957489014\n",
            "Training batch number: 444 : loss --> 0.5593653321266174\n",
            "Training batch number: 445 : loss --> 1.6393531560897827\n",
            "Training batch number: 446 : loss --> 0.14069315791130066\n",
            "Training batch number: 447 : loss --> 2.781438112258911\n",
            "Training batch number: 448 : loss --> 0.20839768648147583\n",
            "Training batch number: 449 : loss --> 1.7500685453414917\n",
            "Training batch number: 450 : loss --> 0.5626668930053711\n",
            "Training batch number: 451 : loss --> 2.9874515533447266\n",
            "Training batch number: 452 : loss --> 3.7821340560913086\n",
            "Training batch number: 453 : loss --> 5.94430685043335\n",
            "Training batch number: 454 : loss --> 0.23885314166545868\n",
            "Training batch number: 455 : loss --> 0.23860931396484375\n",
            "Training batch number: 456 : loss --> 2.072333574295044\n",
            "Training batch number: 457 : loss --> 0.7148365378379822\n",
            "Training batch number: 458 : loss --> 1.0981910228729248\n",
            "Training batch number: 459 : loss --> 0.17331242561340332\n",
            "Training batch number: 460 : loss --> 8.432765007019043\n",
            "Training batch number: 461 : loss --> 0.09979073703289032\n",
            "Training batch number: 462 : loss --> 0.33067378401756287\n",
            "Training batch number: 463 : loss --> 0.10326801240444183\n",
            "Training batch number: 464 : loss --> 0.12254490703344345\n",
            "Training batch number: 465 : loss --> 2.9696340560913086\n",
            "Training batch number: 466 : loss --> 9.193344116210938\n",
            "Training batch number: 467 : loss --> 10.63642692565918\n",
            "Training batch number: 468 : loss --> 0.9629335105419159\n",
            "\n",
            "\n",
            "Training.. Loss: 0.0123 Acc: 0.9964\n",
            "Validating batch number: 0 : loss --> 6.539777755737305\n",
            "Validating batch number: 1 : loss --> 9.721715927124023\n",
            "Validating batch number: 2 : loss --> 6.2279181480407715\n",
            "Validating batch number: 3 : loss --> 1.7475022077560425\n",
            "Validating batch number: 4 : loss --> 16.229331970214844\n",
            "Validating batch number: 5 : loss --> 0.8651338219642639\n",
            "Validating batch number: 6 : loss --> 0.5873016119003296\n",
            "Validating batch number: 7 : loss --> 12.176925659179688\n",
            "Validating batch number: 8 : loss --> 7.293326377868652\n",
            "Validating batch number: 9 : loss --> 14.915675163269043\n",
            "Validating batch number: 10 : loss --> 11.92366886138916\n",
            "Validating batch number: 11 : loss --> 14.647863388061523\n",
            "Validating batch number: 12 : loss --> 7.282641410827637\n",
            "Validating batch number: 13 : loss --> 1.3478306531906128\n",
            "Validating batch number: 14 : loss --> 4.853628158569336\n",
            "Validating batch number: 15 : loss --> 3.7727620601654053\n",
            "Validating batch number: 16 : loss --> 8.086971282958984\n",
            "Validating batch number: 17 : loss --> 6.850317478179932\n",
            "Validating batch number: 18 : loss --> 3.0339062213897705\n",
            "Validating batch number: 19 : loss --> 5.495301246643066\n",
            "Validating batch number: 20 : loss --> 8.268333435058594\n",
            "Validating batch number: 21 : loss --> 9.09778881072998\n",
            "Validating batch number: 22 : loss --> 8.688230514526367\n",
            "Validating batch number: 23 : loss --> 0.7051685452461243\n",
            "Validating batch number: 24 : loss --> 9.854635238647461\n",
            "Validating batch number: 25 : loss --> 8.508903503417969\n",
            "Validating batch number: 26 : loss --> 27.27764892578125\n",
            "Validating batch number: 27 : loss --> 5.856426239013672\n",
            "Validating batch number: 28 : loss --> 27.465362548828125\n",
            "Validating batch number: 29 : loss --> 1.8340092897415161\n",
            "Validating batch number: 30 : loss --> 14.409993171691895\n",
            "Validating batch number: 31 : loss --> 0.9770326018333435\n",
            "Validating batch number: 32 : loss --> 11.467846870422363\n",
            "Validating batch number: 33 : loss --> 13.20080852508545\n",
            "Validating batch number: 34 : loss --> 14.707576751708984\n",
            "Validating batch number: 35 : loss --> 6.324907302856445\n",
            "Validating batch number: 36 : loss --> 14.99438190460205\n",
            "Validating batch number: 37 : loss --> 2.7879574298858643\n",
            "Validating batch number: 38 : loss --> 12.82426643371582\n",
            "Validating batch number: 39 : loss --> 2.5702788829803467\n",
            "Validating batch number: 40 : loss --> 7.547941207885742\n",
            "Validating batch number: 41 : loss --> 6.512385845184326\n",
            "Validating batch number: 42 : loss --> 3.0201003551483154\n",
            "Validating batch number: 43 : loss --> 9.833943367004395\n",
            "Validating batch number: 44 : loss --> 11.340594291687012\n",
            "Validating batch number: 45 : loss --> 21.929906845092773\n",
            "Validating batch number: 46 : loss --> 1.7074906826019287\n",
            "Validating batch number: 47 : loss --> 7.5807647705078125\n",
            "Validating batch number: 48 : loss --> 7.894688129425049\n",
            "Validating batch number: 49 : loss --> 12.079008102416992\n",
            "Validating batch number: 50 : loss --> 9.134844779968262\n",
            "Validating batch number: 51 : loss --> 6.371107578277588\n",
            "Validating batch number: 52 : loss --> 13.511008262634277\n",
            "Validating batch number: 53 : loss --> 3.501143217086792\n",
            "Validating batch number: 54 : loss --> 3.7662405967712402\n",
            "Validating batch number: 55 : loss --> 4.411894798278809\n",
            "Validating batch number: 56 : loss --> 1.3997688293457031\n",
            "Validating batch number: 57 : loss --> 3.139026165008545\n",
            "Validating batch number: 58 : loss --> 15.45928955078125\n",
            "Validating batch number: 59 : loss --> 0.48703306913375854\n",
            "Validating batch number: 60 : loss --> 1.3879528045654297\n",
            "Validating batch number: 61 : loss --> 11.05479621887207\n",
            "Validating batch number: 62 : loss --> 12.529121398925781\n",
            "Validating batch number: 63 : loss --> 9.942809104919434\n",
            "Validating batch number: 64 : loss --> 5.05111837387085\n",
            "Validating batch number: 65 : loss --> 9.97286319732666\n",
            "Validating batch number: 66 : loss --> 9.383809089660645\n",
            "Validating batch number: 67 : loss --> 8.523395538330078\n",
            "Validating batch number: 68 : loss --> 2.178192138671875\n",
            "Validating batch number: 69 : loss --> 13.780893325805664\n",
            "Validating batch number: 70 : loss --> 11.427728652954102\n",
            "Validating batch number: 71 : loss --> 23.447721481323242\n",
            "Validating batch number: 72 : loss --> 5.218699932098389\n",
            "Validating batch number: 73 : loss --> 0.8266471028327942\n",
            "Validating batch number: 74 : loss --> 6.252050399780273\n",
            "Validating batch number: 75 : loss --> 10.902457237243652\n",
            "Validating batch number: 76 : loss --> 6.0703020095825195\n",
            "Validating batch number: 77 : loss --> 4.929781913757324\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 7/7 [03:20<00:00, 28.59s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validating batch number: 78 : loss --> 0.09782351553440094\n",
            "Testing.. Loss: 0.0649 Acc: 0.9825\n",
            "\n",
            "\n",
            " Training complete in {:.4f}seconds 200.14584755897522\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fz55DcQyRsIl"
      },
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/gdrive')\n",
        "\n",
        "model_save_name = 'mnist_resnet18.pth'\n",
        "path = F\"/content/gdrive/My Drive/disentangling-vae/classifers/{model_save_name}\" \n",
        "torch.save(trained_MResnet.state_dict(), path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "2qbhTOSq_0vM"
      },
      "source": [
        "#@title\n",
        "model_save_name = 'MResnet18.pt'\n",
        "path = F\"/content/gdrive/My Drive/Trained_models/{model_save_name}\" \n",
        "torch.save(Mresnet18.state_dict(), path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "tEnIVkoSDOJz"
      },
      "source": [
        "#@title\n",
        "Fresnet18.to(device)\n",
        "optimizer_ft = optim.Adadelta(Fresnet18.parameters())\n",
        "trained_FResnet,FRbatch_train_loss,FRacc_epoch_train,FRacc_epoch_val = train_model(Fresnet18,Fmnist_trainset,Fmnist_testset,optimizer_ft,Fmnist_epoch)\n",
        "\n",
        "plt.title(\"Loss per batch vs. Number of Batches\")\n",
        "plt.xlabel(\"Loss in Training batches\")\n",
        "plt.ylabel(\"Loss in Validation batches\")\n",
        "plt.plot(FRbatch_train_loss,label = \"Training\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.title(\"Accuracy per epoch vs Number of epochs\")\n",
        "plt.xlabel(\"Training Accuracy per epoch\")\n",
        "plt.ylabel(\"Testing Accuracy per epoch\")\n",
        "plt.plot(FRacc_epoch_train,label = \"Training\")\n",
        "plt.plot(FRacc_epoch_val,label = \"Testing\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "MfFE5Aa-RjVq"
      },
      "source": [
        "#@title\n",
        "model_save_name = 'FResnet18.pt'\n",
        "path = F\"/content/gdrive/My Drive/Trained_models/{model_save_name}\" \n",
        "torch.save(Fresnet18.state_dict(), path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBp1BcNADN_7"
      },
      "source": [
        "#@title\n",
        "Mvgg13.to(device)\n",
        "optimizer_ft = optim.Adadelta(Mvgg13.parameters())\n",
        "print(torch.cuda.memory_allocated(device))\n",
        "trained_Mvgg13,MVbatch_train_loss,MVacc_epoch_train,MVacc_epoch_val = train_model(Mvgg13,mnist_trainset,mnist_testset,optimizer_ft,mnist_epoch)\n",
        "\n",
        "plt.title(\"Loss per batch vs. Number of Batches\")\n",
        "plt.xlabel(\"Loss in Training batches\")\n",
        "plt.ylabel(\"Loss in Validation batches\")\n",
        "plt.plot(MVbatch_train_loss,label = \"Training\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.title(\"Accuracy per epoch vs Number of epochs\")\n",
        "plt.xlabel(\"Training Accuracy per epoch\")\n",
        "plt.ylabel(\"Testing Accuracy per epoch\")\n",
        "plt.plot(MVacc_epoch_train,label = \"Training\")\n",
        "plt.plot(MVacc_epoch_val,label = \"Testing\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2520Rc44RoPv",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "model_save_name = 'Mvgg13.pt'\n",
        "path = F\"/content/gdrive/My Drive/Trained_models/{model_save_name}\" \n",
        "torch.save(Mvgg13.state_dict(), path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2toWTx4DN0l"
      },
      "source": [
        "Fvgg13.to(device)\n",
        "optimizer_ft = optim.Adadelta(Fvgg13.parameters())\n",
        "trained_Fvgg13,FVbatch_train_loss,FVacc_epoch_train,FVacc_epoch_val = train_model(Fvgg13,Fmnist_trainset,Fmnist_testset,optimizer_ft,Fmnist_epoch)\n",
        "\n",
        "plt.title(\"Loss per batch vs. Number of Batches\")\n",
        "plt.xlabel(\"Loss in Training batches\")\n",
        "plt.ylabel(\"Loss in Validation batches\")\n",
        "plt.plot(FVbatch_train_loss,label = \"Training\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.title(\"Accuracy per epoch vs Number of epochs\")\n",
        "plt.xlabel(\"Training Accuracy per epoch\")\n",
        "plt.ylabel(\"Testing Accuracy per epoch\")\n",
        "plt.plot(FVacc_epoch_train,label = \"Training\")\n",
        "plt.plot(FVacc_epoch_val,label = \"Testing\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}